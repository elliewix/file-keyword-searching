Award Number: 1229067
Title: Sleep Restriction and Circadian Mismatch Effects on Differential Decision Processes
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 18, 2012
Latest Amendment Date: September 18, 2012
Award Instrument: Standard Grant
Program Manager: Anne Cleary
Start Date: September 01, 2012
Expires: August 31, 2015
Awarded Amount to Date: $405,628
ARRA Amount: $
Investigator(s): David Dickinson dickinsondl@appstate.edu (Principal Investigator) Todd McElroy (Co-Principal Investigator) 
Organization: Appalachian State University
P.O. Box 32174, Boone, NC 28608-2174, (828)262-2130
NSF Directorate: SBE
Program(s): DECISION RISK & MANAGEMENT SCI PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7956 8605 1321 7252
Program Element Code(s): 1321, 7252
Abstract: In our fast-paced world, people often find themselves making important decisions while sleepy. Current surveys indicate that tens of millions of Americans are considered sleep deprived and/or have occupations requiring some shift work. Fatigue can result from insufficient sleep or other factors. For example, some people are naturally more alert in the morning as opposed to the evening. The goal of this research is to examine how sleepiness or fatigue affects decision making. The decision-making process involves (at least) two systems: 1) an unconscious system that responds more automatically and without conscious thought, and 2) a deliberative system that makes "high-level" rational decisions. The investigators hypothesize that when individuals are sleepy, and therefore cognitive resources are compromised, the decisions a person makes will be more the result of automatic processes than deliberative ones, thereby saving mental effort. The investigators will conduct a controlled experimental study that manipulates individuals' sleep quantity as well as the time during the day that decisions are made. The project will test the hypothesis that decision rules that a person uses while sleepy are more automatic and less a product of high-level deliberation. This will also allow the investigators to evaluate whether decisions based on automatic processes are inherently worse than decisions based on more complex deliberative processes. In contrast, relying on automatic processes might lead to better decisions if more thoughtful deliberation makes people overthink the decision task.<br/><br/>This research has the potential to significantly improve our understanding of how fatigue affects decision making in areas such as simple social interactions, the incorporation of new information into decisions, or the importance of how a choice is framed. This is especially critical in our modern society where decisions are often made while one is tired. In occupations like truck driving, air traffic control, medical professionals, and emergency services, poor decision making can cost lives.<br/><br/>[Co-funded by Perception, Action and CognitionProgram and Decision, Risk and Management Sciences Program]

Award Number: 1353338
Title: Understanding Higher Order Color: Beyond the Cardinal Mechanisms
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: February 12, 2014
Latest Amendment Date: February 12, 2014
Award Instrument: Continuing grant
Program Manager: Anne Cleary
Start Date: February 15, 2014
Expires: January 31, 2017
Awarded Amount to Date: $161,533
ARRA Amount: $
Investigator(s): Rhea Eskew eskew@neu.edu (Principal Investigator) 
Organization: Northeastern University
360 HUNTINGTON AVE, BOSTON, MA 02115-5005, (617)373-2508
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7252 9251
Program Element Code(s): 7252
Abstract: Humans typically rely on vision as their primary sense in their interactions with the world. Understanding how higher-order cortical brain processes contribute to human visual perception is a critical issue in cognitive science. Color offers a unique opportunity for understanding the brain mechanisms of perception because the very first step in color processing, the absorption of light by cells in the retina (photoreceptors), is fully understood. This knowledge allows the color scientist to manipulate early signals in the nervous system and measure the perceptual result. Perceptual and neurophysiological experiments have shown that signals from the different photoreceptors in the eye are combined, approximately by addition and subtraction. Our ability to see colors and to discriminate one color from another are the result of these neural sums and differences. For about the past half-century, perceptual scientists have tried to understand the number and nature of these neural color mechanisms: Are there few color mechanisms or many? Do they just add and subtract photoreceptor signals or do they perform more complex calculations? The present work combines computational modeling with novel experimental techniques and strategies. In some cases, random visual flickering elements will appear over a test stimulus. If the noise and test are processed through the same color mechanisms, the noise should hinder the ability of a person to see the test color (by a process like camouflage). If the noise and test are processed in separate mechanisms, the noise should have no effect on the ability to see the test at all. The properties of the color mechanisms will be quantitatively studied by varying the color of the noise relative to the test.<br/><br/>Understanding how the brain processes color information is an important part of the more general understanding of how we perceive the world around us. There are practical applications of this research as well: Color is an important part of signaling systems and information displays. Having a quantitative model of color vision will help the designer of cockpit and automotive displays, medical information and imaging displays, and even digital television systems, to more efficiently and accurately convey information to users.

Award Number: 1353360
Title: Acquisition of hierarchical control in skilled action sequencing
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: February 10, 2014
Latest Amendment Date: February 10, 2014
Award Instrument: Continuing grant
Program Manager: Betty H. Tuller
Start Date: July 01, 2014
Expires: June 30, 2017
Awarded Amount to Date: $110,897
ARRA Amount: $
Investigator(s): Matthew Crump mcrump@brooklyn.cuny.edu (Principal Investigator) 
Organization: CUNY Brooklyn College
Office of Research & Sponsored P, Brooklyn, NY 11210-2889, (718)951-5622
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7252
Program Element Code(s): 7252
Abstract: A critical issue in cognitive science is understanding how behavior unfolds in time. Most human behaviors, from walking and talking, to driving, typing and playing instruments, consist of a series of ordered actions. Little is known concerning how people learn the serial ordering of actions and how this ability changes with expertise. This investigation uses the task of skilled typing to study how high- and low-level cognitive processes interact to allow typists to order their keystrokes rapidly and accurately. Typing is a useful model system for this purpose because experts are plentiful and typing performance can be measured precisely in terms of keystroke timing and errors. Prior studies have shown that typing skill is controlled hierarchically by two processing loops: an "outer loop" turns ideas into words and sentences and an "inner loop" translates words into motor movements for executing individual keystrokes. Three related projects test how these loops control serial ordering across levels of typing skill. Project 1 uses a large-scale web-based approach to quantify how novices and experts become sensitive to the statistics of natural keystrokes and learn to optimize typing performance for predictable letter patterns. Project 2 explores how the inner loop becomes gradually connected to the outer loop, allowing verbal codes used for language production to control serial ordering of fingers on the keyboard. Project 3 tests how the inner loop schedules the timing of individal keystrokes after it has received word-level instructions from the outer loop. <br/><br/>Typing, especially on computers and smartphones, has become an indispensable modern skill. This work can provide new statistical signatures of typing skill that can be used to promote rapid skill acquisition. The science of typing can also serve as a model for investigating other domains that rely on hierarchical control loops to accomplish complex serial ordering, such as playing a musical instrument, assembly work, and interactive technologies. The project also provides training opportunities for graduate and undergraduate students at an institution that serves several minority and underrepresented groups.

Award Number: 1057556
Title: Musical Experience in Older Adults: Impact on Hearing Speech in Noise
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: March 11, 2011
Latest Amendment Date: April 09, 2013
Award Instrument: Continuing grant
Program Manager: Betty H. Tuller
Start Date: April 01, 2011
Expires: March 31, 2015
Awarded Amount to Date: $401,021
ARRA Amount: $
Investigator(s): Nina Kraus nkraus@northwestern.edu (Principal Investigator) 
Organization: Northwestern University
1801 Maple Ave., Evanston, IL 60201-3149, (847)491-3003
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7969
Program Element Code(s): 7252
Abstract: Many cognitive and perceptual skills are negatively affected by aging. One critical concern for older adults is an age-related decrease in speech understanding, especially in noisy environments such as restaurants. Dr. Nina Kraus, at Northwestern University, will examine whether musical training can protect against age-related deterioration in the ability to perceive speech in noise. Dr. Kraus will record the subcortical processing of speech in noise in musicians and non-musicians who are 45 to 65 years of age to determine whether lifelong musical training results in enhanced brainstem tracking of speech in noise. In addition, the influences of musical training on a variety of behavioral measures will be examined, including perception of speech in noise, short-term memory, and attention.<br/><br/>As life circumstances change, including loss of job or spouse, maintaining social contacts becomes ever more important for social and emotional health. It would be of great benefit if speech understanding could be improved at the source of difficulty by increasing the brain's ability to represent the speech signal accurately and to separate speech from background noise. If musical experience can lessen or slow the negative effects of age on cognitive skills, perceptual acuity and neural encoding of speech in noise, it would have substantial scientific and societal impact, emphasizing the importance of music as a rehabilitation strategy.<br/> <br/> 

Award Number: 1127086
Title: Leadership Support for Core Activities of the Board on Behavioral, Cognitive and Sensory Sciences (BBCSS)
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 22, 2011
Latest Amendment Date: September 22, 2011
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: September 15, 2011
Expires: August 31, 2016
Awarded Amount to Date: $750,000
ARRA Amount: $
Investigator(s): Barbara Wanchisen bwanchisen@nas.edu (Principal Investigator) 
Organization: National Academy of Sciences
500 FIFTH STREET NW, WASHINGTON, DC 20001-2721, (202)334-2254
NSF Directorate: SBE
Program(s): LINGUISTICS SOCIAL PSYCHOLOGY LAW AND SOCIAL SCIENCES DEVELOP& LEARNING SCIENCES/CRI COGNEURO PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 
Program Element Code(s): 1311, 1332, 1372, 1698, 1699, 7252
Abstract: The Board on Behavioral, Cognitive, and Sensory Sciences (BBCSS), a standing body of the National Academies, was established in 1997 with five principal goals: (1) to engage academic and other researchers in these sciences in the consideration of major national policy issues; (2) to promote and advance cross-disciplinary inquiry into complex scientific and policy questions; (3) to provide a forum for objective, independent, and rigorous deliberation among researchers, the public, the media, Congress, professional associations, the NSF and other federal agencies; (4) to draw attention to the significance of the behavioral, cognitive, and sensory sciences to national policy; and (5) to link these sciences to the diverse policy issues addressed by the National Academies.<br/>The Board convenes meetings for planning and other purposes, develops workshops and consensus studies that are separately funded and conducted by committees of experts, suggests appropriate experts for other National Academies studies, participates in the review of study activities and reports, and widely disseminates study results. It also regularly grapples with broad issues such as future directions and emerging trends of the behavioral sciences, with an emphasis on policy implications. Members of the Board serve as volunteers.

Award Number: 1230311
Title: Collaborative Research: Understanding the Rules for Human Rhythmic Motor Coordination
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: August 13, 2012
Latest Amendment Date: July 27, 2013
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: August 15, 2012
Expires: July 31, 2015
Awarded Amount to Date: $393,685
ARRA Amount: $
Investigator(s): Tim Kiemel kiemel@umd.edu (Principal Investigator) Norman Wereley (Co-Principal Investigator) John Jeka (Former Co-Principal Investigator) 
Organization: University of Maryland College Park
3112 LEE BLDG, COLLEGE PARK, MD 20742-5141, (301)405-6269
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7252 7298 7956 8605
Program Element Code(s): 7252
Abstract: Walking for a healthy adult seems easy. However, underlying this apparent simplicity our nervous system is performing a task of astounding complexity. Using sensory information about body movement, the nervous system coordinates the activation of dozens of muscles so that we stably and efficiently move through our environment. For example, if our nervous system senses that our foot will strike the ground too soon, it will adjust muscle activations so we do not stumble and fall. In this project, an interdisciplinary team of investigators aims to uncover the rules the nervous system uses to make such adjustments. Using a general theoretical framework taken from engineering (used to understand, for example, the rhythmic control of the angle of attack of rotating helicopter blades), the method depends on gently perturbing a person's senses and body in various ways and observing how the nervous system adjusts muscle activations in response. The investigators will first test their methods on a simpler type of rhythmic movement, repetitive hitting of a virtual ball with a paddle, then extend the findings to coordination during walking. <br/><br/>By constructing a general approach to understanding the control of rhythmic movements, including swimming in fish, flying in insects and birds, and walking in people and robots, the investigators may provide a foundation for understanding how control breaks down for people with neurological conditions such as stroke and incomplete spinal cord injury. This has the potential to advance neuromuscular rehabilitation and the design of assistive devices. <br/><br/>[Co-funded by CISE and SBE]

Award Number: 1353809
Title: Dynamics of Posture and Reaching
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: January 31, 2014
Latest Amendment Date: January 31, 2014
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: February 15, 2014
Expires: January 31, 2017
Awarded Amount to Date: $449,061
ARRA Amount: $
Investigator(s): James Lackner lackner@brandeis.edu (Principal Investigator) Paul DiZio (Co-Principal Investigator) 
Organization: Brandeis University
415 SOUTH ST MAILSTOP 116, WALTHAM, MA 02454-9110, (781)736-2121
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7252
Program Element Code(s): 7252
Abstract: An essential issue in the movement sciences is to understand how people maintain upright posture and balance. In everyday life, people often simultaneously turn and reach to pick up objects or to place objects. Such movements are typically accurate and seem virtually effortless. However, when an arm movement is made during ongoing trunk rotation large inertial forces are generated on the reaching arm. These forces are so large that huge errors in reaching and destabilization of posture would occur if the central nervous system (CNS) did not take them into account. In this program of research, two series of experiments will examine the interaction of the control of self-rotation and of arm-torso dynamics. Series 1 will evaluate the kinematics of reaching and turning, postural sway, arm joint torques, and ground reaction forces at the feet. Series 2 will assess these variables in tasks involving active and passive torso rotation and reaching. The results will indicate whether the CNS control of trunk rotation and arm reaching are a) parallel and independent, b) hierarchical with information about intended trunk rotation contributing to separate control of torso and arm movements, or c) distributed with a single internal model for reaching and turning. <br/><br/>Falling in the elderly is a major health problem. Neurological diseases that affect torso and or limb movement control (e.g., Parkinson's disease) also increase the likelihood of falls. This research should provide a scientific basis for developing rehabilitation strategies that compensate for trunk-generated forces on the limbs and minimize the likelihood of falling.

Award Number: 1022313
Title: Speech Reduction across Languages and Dialects
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: August 30, 2010
Latest Amendment Date: September 14, 2013
Award Instrument: Standard Grant
Program Manager: Joan Maling
Start Date: September 01, 2010
Expires: February 28, 2015
Awarded Amount to Date: $271,481
ARRA Amount: $
Investigator(s): Natasha Warner nwarner@u.arizona.edu (Principal Investigator) Miguel Simonet (Co-Principal Investigator) 
Organization: University of Arizona
888 N Euclid Ave, TUCSON, AZ 85721-0001, (520)626-6000
NSF Directorate: SBE
Program(s): LINGUISTICS PERCEPTION, ACTION & COGNITION COLLABORATIVE RESEARCH 
Program Reference Code(s): 0000 1311 OTHR 5941 5978 7298 5948 5979 9179 SMET 9178 9251
Program Element Code(s): 1311, 7252, 7298
Abstract: The speech humans produce every day in casual conversation is incredibly varied, with sounds and whole syllables changed or missing. American English listeners notice nothing unusual when hearing such "reduced speech" in context; however, second-language speakers and even listeners from other English-speaking countries often find American English reduced speech difficult to understand. The current research centers on how speakers and listeners use reduced, spontaneous speech across languages and dialects, and on how such speech may hinder or even facilitate communication among speakers of different backgrounds. The project will test speakers of Dutch, Spanish, Japanese, and three dialects of English to determine 1) to what extent reduction is language-specific and part of the grammar rather than random or physically-determined variability, 2) whether the sound patterns of the native language influence phonetic variability at the level of spontaneous speech in the second language, 3) how strongly dialect affects understanding of reduced speech, and 4) how degree of proficiency, years of experience, strength of ethnic/national identity, etc. affect production and understanding of reduction. The overarching theoretical question is, what is part of the learned grammar and what is low-level variability. Furthermore, the project will provide data on theoretical questions about exemplar models of speech perception, mutual effects between speakers' first and second languages, and articulatory planning. <br/><br/>Through globalization, immigration, and telecommunications, humans in the modern world often interact across language backgrounds. Native English speakers interact with non-native speakers and speakers of different English dialects interact with each other. The current project addresses how humans handle the variability of conversational speech in communicative situations. Detailed knowledge of natural, reduced speech, gained through this project, will impact speech technology and how humans interact with computers by voice, benefiting speech synthesis and automatic recognition of spontaneous speech. Because the project includes extensive investigation of English proficiency and language background for the experiment participants, it will also shed light on what factors make it easier or harder for non-native listeners to understand conversational speech in their second language. This project forms a synergistic international collaborative group to answer these questions.

Award Number: 1026943
Title: RUI - Investigating the Design of Human Communication Systems in the Laboratory: The Effects of Mimesis
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 10, 2010
Latest Amendment Date: September 08, 2011
Award Instrument: Continuing grant
Program Manager: Betty H. Tuller
Start Date: September 15, 2010
Expires: August 31, 2014
Awarded Amount to Date: $349,686
ARRA Amount: $
Investigator(s): Bruno Galantucci galantuc@yu.edu (Principal Investigator) 
Organization: Yeshiva University
500 West 185th Street, New York, NY 10033-3201, (718)430-3642
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 9229
Program Element Code(s): 7252
Abstract: When deaf children are not exposed to sign language they spontaneously develop novel forms of communication. In the laboratory, adults interacting in the absence of conventional means of communication also have a remarkable talent for developing novel forms of communication. Bruno Galantucci and his students at Yeshiva University use these observations to test a simple hypothesis about the core design principles of natural languages. At the most fundamental level, spoken languages rely on a few dozen basic forms. Sign languages, however, rely on a much larger number of basic forms. Signed and spoken languages also differ in another way: The forms of sign language can more easily mimic aspects of the world to which they refer than can the forms of speech. It may be that the two differences are intimately connected: The more the forms of a communication system mimic the world, the more likely it is that the system relies on a large set of basic forms. If correct, this provides a simple and elegant explanation for a core difference in design between spoken and signed languages. <br/><br/>This research will be carried out at Yeshiva University, a primarily undergraduate institution, and will provide excellent research experience for undergraduate students. The project also develops an innovative methodology which will be made readily available to other researchers through a dedicated website. Such methodology can be used for further investigations of the core mechanisms supporting human communication. Knowledge about these mechanisms has a number of potential applications. For example, it can be used (a) to design languages which facilitate universal communication over the internet; (b) to develop new technologies to help people who suffer from communication disorders; and (c) to improve the methods used for teaching foreign languages.

Award Number: 1344279
Title: INSPIRE Track 1: Selection as an organizing process: from molecules to languages
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 10, 2013
Latest Amendment Date: September 10, 2013
Award Instrument: Continuing grant
Program Manager: Betty H. Tuller
Start Date: September 15, 2013
Expires: August 31, 2016
Awarded Amount to Date: $683,998
ARRA Amount: $
Investigator(s): Gary Lupyan lupyan@wisc.edu (Principal Investigator) Rick Dale (Co-Principal Investigator) David Ardell (Co-Principal Investigator) Suzanne Sindi (Co-Principal Investigator) Russell Gray (Co-Principal Investigator) 
Organization: University of Wisconsin-Madison
21 North Park Street, MADISON, WI 53715-1218, (608)262-3822
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION MATHEMATICAL BIOLOGY INSPIRE 
Program Reference Code(s): 7252 8653
Program Element Code(s): 7252, 7334, 8078
Abstract: This INSPIRE award is partially funded by the Perception, Action, and Cognition Program in the Division of Behavioral and Cognitive Sciences in the Directorate for Social, Behavioral, and Economic Sciences and the Mathematical Biology Program in the Division of Mathematical Sciences in the Directorate for Mathematical and Physical Sciences. <br/><br/>This work explores the role of selection and adaptation in two radically different domains, 1) molecules and 2) languages. Consider, for example, human languages. The 6,000-7,000 languages spoken by people display a dazzling variety of sounds, words, and grammatical forms. This diversity is typically explained as a product of random drift: As a single population splits and drifts apart, the accumulation of small random changes eventually produces mutually unintelligible languages. An alternative is that some of the variation we see among human languages is due to selection. In this account, languages adapt to some extent to the different social and ecological environments in which they are spoken. Similarly, researchers have only recently considered the role of selection and adaptation in the study of prions (self-replicating proteins)and how they propagate within and across generations of cells. <br/><br/>Though biological structures and human languages are radically different domains, they share properties that suggest they may be described by a common mathematical framework. Specifically, (i) they are both epigenetically inherited, (ii) they both capitalize upon a pre-existing biological substrate, and (iii) they both propagate in a system of agents (cells, people). A highly interdisciplinary team of cognitive scientists, linguists, biologists, and mathematicians seek to connect and inform these domains by using mathematical models and large-scale behavioral experiments to understand the selective processes. They will assess convergent tests of the idea that selection acts as an organizing principle of systems at different scales. This work has important implications for issues such as the role of environmental context in the spread of structures such as prions or linguistic elements. For example, results could help explain how a structure newly infects populations, such as when a new word "invades" a linguistic environment, or when a prion structure successfully propagates and infects cells in its environment. The interdisciplinary nature of this award will provide a unique training experience for graduate students and will include outreach efforts to local schools.

Award Number: 1153034
Title: Collaborative Research: Sensory Integration and Sensorimotor Transformations for Dexterous Manipulation
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: February 13, 2012
Latest Amendment Date: July 30, 2013
Award Instrument: Continuing grant
Program Manager: Betty H. Tuller
Start Date: March 15, 2012
Expires: February 28, 2015
Awarded Amount to Date: $320,001
ARRA Amount: $
Investigator(s): Marco Santello marco.santello@asu.edu (Principal Investigator) 
Organization: Arizona State University
ORSPA, TEMPE, AZ 85281-6011, (480)965-5479
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7298 7252
Program Element Code(s): 7252
Abstract: The ability to grasp and manipulate objects is an extremely complex motor behavior. When grasping an object such as a cup, people often vary their finger placements and the forces exerted by the fingers in order to ensure that the goal is attained (in this case, lifting the cup without spilling its contents). How humans do this so efficiently is not known, in part because previous research constrained subjects to place their fingers at pre-specified locations on the objects. This has led to a major gap in our understanding of how these complex motor behaviors are learned, planned, and executed. To address this gap, the proposed studies will investigate how subjects grasp and manipulate objects in tasks that allow them to choose their finger placements and applied forces and, importantly, to adjust the interaction between the two. Another function of grasping is to develop a representation of an object's weight and the distribution of weight within the object. How subjects develop this representation and refine their actions will be examined by studying the interaction between information extracted by grasping and memories of past manipulations of the object. The hypothesis that fingertip placement and forces are learned independently from each other will be tested by manipulating visual feedback of fingertip placement, varying visual shape and density cues, and through object rotation tasks that create a discrepancy between visual cues and memories of the same object from prior manipulations.<br/><br/>The proposed studies represent a major paradigm shift in the research on grasping by opening new and fundamental questions about how the brain learns to control the hand. Thus, the results of this work may inform the design of more dexterous robotic manipulators, brain-machine interfaces, and neuroprosthetic hands.

Award Number: 1059523
Title: UNCERTAINTY REDUCTION: The Guiding Principle of Attentional Allocation
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: February 18, 2011
Latest Amendment Date: July 30, 2012
Award Instrument: Continuing grant
Program Manager: Betty H. Tuller
Start Date: March 15, 2011
Expires: February 28, 2015
Awarded Amount to Date: $407,905
ARRA Amount: $
Investigator(s): Sarah Shomstein shom@gwu.edu (Principal Investigator) 
Organization: George Washington University
2121 Eye Street NW, Washington, DC 20052-2000, (202)994-6255
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7969
Program Element Code(s): 7252
Abstract: Attention is a central process in cognition. When people search through the environment for information relevant to their current goal, they are selectively processing perceptual information. Crucial to our understanding of attentional selection is determining what guides this selection. Until recently it has been observed that spatial locations are important for attentional guidance. However, recent evidence suggests that under some circumstances objects, not spatial locations, guide attentional selection. The full understanding of why under some circumstances such guidance is possible has remained elusive. This program of research introduces and tests a new unifying framework, uncertainty reduction, within which to examine factors that influence object-based attentional guidance. The generality of this framework is investigated, in several different paradigms, by systematically testing its predictive powers in two domains of uncertainty: internal uncertainty reduction as manipulated by reward and external uncertainty reduction as manipulated by varying the scope of attentional set. In order to understand uncertainty reduction and its effects on object-based attentional guidance, behavioral profiles as well as the neural underpinnings of this mechanism are examined.<br/> Whether reading a book in the peace and quiet of your living room or driving a car in traffic, the ability to pay attention to important aspects of the environment is an integral part of our lives and, ultimately, is crucial to our success and survival. A better understanding of critical factors that determine how attentional selection is deployed is important to diverse fields such as the design of interactive and ergonomic panels and enhanced training programs across multiple industries, ranging from training drivers and machine operators to security personnel (e.g., airport baggage screeners) to neurologists reading X-rays for evidence of cancer.

Award Number: 1058937
Title: Sequential learning from a scale-invariant representation of remembered time
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: December 28, 2011
Latest Amendment Date: December 28, 2011
Award Instrument: Standard Grant
Program Manager: Anne Cleary
Start Date: January 15, 2012
Expires: December 31, 2014
Awarded Amount to Date: $366,567
ARRA Amount: $
Investigator(s): Marc Howard mahoward@syr.edu (Principal Investigator) 
Organization: Trustees of Boston University
881 COMMONWEALTH AVE, BOSTON, MA 02215-1300, (617)353-4365
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 
Program Element Code(s): 7252
Abstract: It can be argued that the primary adaptive function of memory is not to remember the past, but to predict the future. Knowledge of past experiences combined with an understanding of the present state gives organisms the ability to anticipate future rewards or avoid impending danger. The goal of the proposed research is to develop a mathematical theory describing how people use past history and a representation of the present to predict the future. The theory will be built on a mathematical model of how the history leading up to the present moment can be efficiently compressed into a representation that could be maintained by the brain. The investigators pursue the development of the theory using three techniques. First, in order to see if human learners behave in a way consistent with the hypothesis, the investigators will conduct a series of behavioral experiments using undergraduate students as research subjects. The experiments present the subjects with a series of symbols chosen according to a hidden sequence and ask them to predict the symbols that will follow at various stages. Second, the investigators will conduct computer simulations to train the equations on a large body of naturally-occuring language. Language has a rich temporal structure defined by the way words, and combinations of words, follow one another. Third, the investigators will work to extend the mathematics of the hypothesis to enable it to describe a wide range of phenomena in learning and memory. The large scale goal is to reorient several subfields of cognitive psychology---episodic memory, semantic memory, conditioning, and interval timing---around an understanding of how temporal history is represented and utilized by the human brain.<br/><br/>If successful, the proposed research could have far-reaching practical impacts. It could provide insight into how children and adults learn, leading to better instructional tools. If successful, it would also represent a large step forward in completely automated natural language processing. The rise of electronic communication has led to vast quantities of text---more than could ever be read by even a small army of human readers. Algorithms that can extract knowledge from large quantities of text currently find use in applications as far-ranging as essay grading and other educational applications to intelligence uses. The investigators anticipate that the equations will be much better at extracting knowledge from natural text than several widely-used algorithms. Finally, there may be useful technologies that exploit the ability to predict the future from the past and the present with the level of efficiency that humans can.

Award Number: 1247971
Title: INSPIRE: The Hunting of the Spark: A Systematic Study of Natural Creativity in Human Networks
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: July 17, 2012
Latest Amendment Date: July 17, 2012
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: September 15, 2012
Expires: August 31, 2015
Awarded Amount to Date: $999,762
ARRA Amount: $
Investigator(s): Ali Minai ali.minai@uc.edu (Principal Investigator) 
Organization: University of Cincinnati Main Campus
University Hall, Suite 530, Cincinnati, OH 45221-0222, (513)556-4358
NSF Directorate: SBE
Program(s): SOCIAL PSYCHOLOGY INFORMATION TECHNOLOGY RESEARC PERCEPTION, ACTION & COGNITION Cyber-Human Systems INSPIRE 
Program Reference Code(s): 8653
Program Element Code(s): 1332, 1640, 7252, 7367, 8078
Abstract: This INSPIRE award is partially funded by the Perception, Action, and Cognition Program and the Social Psychology Program in the Division of Behavioral and Cognitive Sciences in the Directorate for Social, Behavioral and Economic Sciences, and the Human-Centered Computing Program in the Division of Information and Intelligent Systems in the Directorate for Computer and Information Science and Engineering. <br/><br/>The notion of collective wisdom - that many interacting individuals can produce better ideas, insights, solutions and decisions than a single individual can produce - is a foundational principle for many institutions of modern societies, including elections, parliaments, governing boards, free markets and the free press. Until recently, such collective wisdom could only be harnessed through massive unstructured mechanisms such as elections and markets, or through highly structured and interactive but smaller-scale mechanisms such as legislatures, committees, boards and panels. With the advent of the Internet, the World-Wide Web and social network technology, the paradigm has changed. It is now possible for large numbers of diverse, geographically distant individuals to interact in structured ways and at almost no cost. This has opened up vast possibilities for innovation and creativity, but these possibilities are still largely unrealized. A major reason for this is the lack of a systematic scientific understanding of collective innovation in human networks. The current project addresses this through a combination of methods from several disciplines, bringing together a unique group of researchers with expertise in social psychology, cognitive science, computer science, engineering and network theory.<br/><br/>The fundamental principle underlying the research is that an understanding of natural human innovation requires a coordinated combination of laboratory and field studies and that neither is sufficient on its own. Field studies of innovative human networks (communities of research scholars and engineers) will be used to identify natural patterns of innovation through data mining and intelligent analysis methods. Laboratory experiments will then verify and validate these patterns under controlled conditions. Thus, through a combination of large-scale fieldwork and careful experiments, the investigators will develop better metrics for measuring innovation and discover ways to help groups and individuals be more innovative. Most importantly, this project will clarify how the connectivity of individuals in a network acts to boost or suppress innovation, leading to recommendations for making human networks from corporations to social networks more innovative.<br/><br/>Recent studies suggest that the rate of innovation must increase exponentially to sustain a growing and urbanizing global society. While the proposed research will focus on specific areas, the methods it generates will inform organizations and governments broadly in developing policies conducive to innovation. In today's fast-moving, highly competitive world, such policies are likely to be a major determinant of scientific, technological and geopolitical leadership.

Award Number: 1247365
Title: Workshop on Cognitive Science: the Computational Paradigm
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 17, 2012
Latest Amendment Date: September 17, 2012
Award Instrument: Standard Grant
Program Manager: Anne Cleary
Start Date: October 01, 2012
Expires: September 30, 2014
Awarded Amount to Date: $20,000
ARRA Amount: $
Investigator(s): Peter Erdi perdi@kzoo.edu (Principal Investigator) 
Organization: Kalamazoo College
1200 Academy Street, Kalamazoo, MI 49006-3291, (269)337-7162
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION Cyber-Human Systems ROBUST INTELLIGENCE 
Program Reference Code(s): 7252 7367 7495 7556
Program Element Code(s): 7252, 7367, 7495
Abstract: This award will provide support for a satellite workshop to the International Joint Conference on Neural Networks (IJCNN), to be held in Dallas, TX on August 4-9, 2013. The goal of the workshop is to explore subfields in cognitive science that hold the most promise for increasing our understanding of neural networks and computational intelligence.<br/><br/>The IJCNN explores the theoretical and computational understanding of the brain in order to develop new and more effective forms of machine intelligence. Cognitive science is the interdisciplinary, scientific study of the mind and mental processes. The workshop is intended to foster more effective integration between the two communities. The workshop will provide a venue in which neural network researchers and students can learn more about the state of the art in cognitive science and its interface with computational intelligence. The broader impacts of the workshop include fostering new collaborations between neural network researchers and those working in other areas of cognitive science. In addition, it provides for reduced registration for women and other scientists underrepresented in the field.

Award Number: 1052765
Title: RUI: Effects of hand gesture on auditory and vocabulary learning of a second language
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: March 08, 2011
Latest Amendment Date: July 30, 2012
Award Instrument: Continuing grant
Program Manager: Betty H. Tuller
Start Date: March 15, 2011
Expires: February 28, 2015
Awarded Amount to Date: $316,238
ARRA Amount: $
Investigator(s): Yukari Hirata yhirata@mail.colgate.edu (Principal Investigator) Spencer Kelly (Co-Principal Investigator) 
Organization: Colgate University
13 Oak Drive, Hamilton, NY 13346-1398, (315)228-7451
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION OTHER GLOBAL LEARNING & TRNING 
Program Reference Code(s): 9229 9251 5937 5979
Program Element Code(s): 7252, 7731
Abstract: Learning to hear novel speech sounds is one of the major challenges of mastering a second language (L2). Previous work using intensive auditory training has helped learners perceive challenging and unfamiliar speech sounds but no method to date has consistently brought learners to native levels. This research project by Yukari Hirata and Spencer Kelly at Colgate University attempts to improve upon previous methods by coupling auditory instruction with visual and motor information conveyed though hand gestures. Specifically, Hirata and Kelly will explore how observing and producing different types of gestures helps native English speakers learn novel speech sounds in Japanese. Hirata and Kelly will investigate this question using behavioral and brain measures and will explore whether mastering these novel speech sounds also leads to better L2 vocabulary learning.<br/><br/>Given the increasing contact among people all over the world, developing effective and efficient strategies for L2 teaching is more important than ever. The project also allows for integration of research into the educational experience of undergraduate students at a primarily undergraduate institution, exposing them to the excitement of scientific inquiry. Finally, strengthening on-campus laboratory facilities (that can be used by several disciplines) for the behavioral and neural investigation of foreign language learning will enrich and diversify undergraduate science education.

Award Number: 1057969
Title: Collaborative Research: Multisensory Perceptual Learning
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: March 31, 2011
Latest Amendment Date: December 03, 2012
Award Instrument: Continuing grant
Program Manager: Anne Cleary
Start Date: April 01, 2011
Expires: March 31, 2015
Awarded Amount to Date: $299,527
ARRA Amount: $
Investigator(s): Ladan Shams ladan@psych.ucla.edu (Principal Investigator) 
Organization: University of California-Los Angeles
11000 Kinross Avenue, Suite 211, LOS ANGELES, CA 90095-2000, (310)794-0102
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7252 CL10
Program Element Code(s): 7252
Abstract: Ladan Shams, University of California at Los Angeles<br/>Aaron Seitz, University of California at Riverside<br/><br/>COLLABORATIVE RESEARCH: MULTISENSORY PERCEPTUAL LEARNING<br/><br/>ABSTRACT<br/><br/>In daily life, we frequently experience correlated sensations across our different sensory modalities. For example, as we climb up the stairs, we receive sensations to our auditory, visual, tactile, and vestibular systems that are all related to the experience of stair-climbing. These types of multisensory experiences are a key aspect of how we interact with, and learn about, the world around us. Through our experience with the world, our sensory abilities undergo refinements that allow us to optimize our performance in the tasks that we perform. These sensory refinements involve fine-tuning of processing in each of our sensory modalities, but equally importantly, in how we merge information across modalities. While much research has focused on how learning can take place within each individual sensory system, the learning of how information is combined across the senses has been largely neglected. The PIs will conduct a series of experiments in which they can track visual, auditory, and auditory-visual multisensory learning in parallel, and discriminate among different theories of multisensory processing and learning. Behavioral and neuroimaging methods will be combined to shed light on the roles that different brain areas, and the interactions between brain areas, play in the process of multisensory learning. Altogether these studies will provide fundamental insights into how our sensory systems work together and refine their interactions to best operate in the tasks that we perform.<br/><br/>This project will be the first systematic investigation of multisensory perceptual learning. It will also be the first study of changes in interaction between brain areas that may occur as a result of sensory learning. Altogether, this study promises to provide foundational knowledge regarding the brain mechanisms involved in multisensory learning as well as the mechanisms of learning in general. Understanding multisensory learning can contribute to the development of more effective strategies for learning. These strategies can be utilized to enhance learning for typically-developed children and adults, as well as to facilitate learning and communication for individuals with deprivation in one sense (e.g., individuals with low-vision or low-hearing, patients with cochlear implants or undergoing macular degeneration or cataract surgeries). They can also contribute to devising remedial programs for dyslexia, which appears to involve deficits in combining information across the senses.

Award Number: 1344269
Title: INSPIRE Track 1: Gradient Symbolic Computation
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 12, 2013
Latest Amendment Date: September 12, 2013
Award Instrument: Continuing grant
Program Manager: Joan Maling
Start Date: September 15, 2013
Expires: February 28, 2018
Awarded Amount to Date: $833,997
ARRA Amount: $
Investigator(s): Paul Smolensky smolensky@jhu.edu (Principal Investigator) Benjamin Van Durme (Co-Principal Investigator) Akira Omaki (Co-Principal Investigator) Kyle Rawlins (Co-Principal Investigator) Geraldine Legendre (Co-Principal Investigator) 
Organization: Johns Hopkins University
3400 N CHARLES ST, Baltimore, MD 21218-2608, (410)516-8668
NSF Directorate: SBE
Program(s): LINGUISTICS INSPIRE PERCEPTION, ACTION & COGNITION ROBUST INTELLIGENCE ALGORITHMS IIS SPECIAL PROJECTS 
Program Reference Code(s): 1311 7252 8089 8653
Program Element Code(s): 1311, 8078, 7252, 7495, 7926, 7484
Abstract: This INSPIRE award is partially funded by the Linguistics Program and the Perception, Action & Cognition Program in the Division of Behavioral and Cognitive Sciences in the Directorate for Social, Behavioral & Economic Sciences; by the Robust Intelligence Program in the Division of Information & Intelligent Systems in the Directorate for Computer & Information Science & Engineering; and by the Algorithmic Foundations Program in the Division of Computer and Network Systems in the Directorate for Computer & Information Science & Engineering.<br/><br/>Discrete, combinatorial systems of structured symbols permeate human cognition in domains such as language, motor control, complex action planning, learning, and higher-level vision. Nonetheless, the computational apparatus that the brain exploits is based on continuous, activation-based propagation of information through complex networks of neurons. A fundamental problem of the cognitive sciences is how to integrate gradient, continuous neural computation with the discrete combinatorial dimension of cognition. The solution to this puzzle will provide a deeper understanding of the mind and may also serve as the basis of a new generation of computing systems capable of authentically brain-like behavior.<br/><br/>Under the direction of Dr. Smolensky, the research team will develop an approach to this puzzle by exploring and testing the predictions of their theory of Gradient Symbolic Computation (GSC) in the domain of language. Their efforts will include the development of the formal, mathematical foundations of GSC. In parallel, the PIs will develop a framework for modeling Gradient Symbolic Processing. To that end, the PIs will use computational modeling and experimental psycholinguistic studies of phenomena that typify the morpho-phonological, syntactic, and semantic characteristics of language and language processing. <br/><br/>The broader impacts of the work include the potential to transform general computing for future approaches to computer design, to provide innovations in computer language processing, and to empower major advances in our understanding of human language, its impairment in disease, and its learning and remediation. The project also strongly engages STEM education. Undergraduate, graduate, and post-doctoral researchers will all play key roles in highly interdisciplinary STEM research integrating experimental, theoretical, and computational methods. The new type of computation created will provide an integrative framework for developing courses bridging computation theory, psychology, and linguistics. Pedagogical materials developed in these courses will be made publicly available to facilitate undergraduate and graduate program development at other institutions.

Award Number: 1360663
Title: Experimental Pragmatics: Advancing Theory and Method Special Session at the 27th Annual CUNY Conference on Human Sentence Processing
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: January 07, 2014
Latest Amendment Date: January 07, 2014
Award Instrument: Standard Grant
Program Manager: William J. Badecker
Start Date: February 15, 2014
Expires: July 31, 2015
Awarded Amount to Date: $34,608
ARRA Amount: $
Investigator(s): Shari Speer speer@ling.ohio-state.edu (Principal Investigator) Lauren Squires (Co-Principal Investigator) Laura Wagner (Co-Principal Investigator) 
Organization: Ohio State University
Office of Sponsored Programs, Columbus, OH 43210-1016, (614)292-3805
NSF Directorate: SBE
Program(s): LINGUISTICS PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 1311 7252 7556
Program Element Code(s): 1311, 7252
Abstract: Understanding how humans employ pragmatic principles to facilitate communication is an important element in understanding human interaction. Research in experimental pragmatics has shown marked recent theoretical progress. Current work successfully models not only contextual influences from linguistic structure and visually available context, but also factors such as the interlocutors involved, the world knowledge available to them as individuals and as they interact, and their mutually sustained beliefs and intentions. Recent progress in the field can be linked to researchers' ability to leverage novel methods, such as mining multi-interlocutor text interactions in social media, web-crawling language produced and understood via electronic means such as texting, internet blogs and other web-based platforms, and collecting data via Mechanical Turk. These approaches have expanded the available testable hypotheses as well as the diversity and representativeness of the language data under study. Pragmatics theorists have also been particularly open to incorporating insights from experimentally-generated results into developing theories of language interpretation. The 27th Annual CUNY Human Sentence Processing Conference, to be held in March 2014 at The Ohio State University in Columbus, Ohio, will include a Special Session on Experimental Pragmatics that aims to take stock of recent progress in this burgeoning sub-field and help to set the agenda for future research in this domain.<br/><br/>The core of the special session is a series of invited talks by six prominent researchers, diverse with respect to gender and academic seniority, with relevant expertise. Together, the speakers represent a wide range of theoretical and methodological approaches (corpus linguistics as well as experimental studies; language acquisition as well as adult processing; speech perception as well as the processing of quantifiers and anaphora). The special session promises to provide a fuller understanding of the operation of pragmatic influences and their interaction with other aspects of linguistic competence and performance that we believe will lead to a more realistic and representative picture of how the human mind represents and processes language.

Award Number: 1151805
Title: Body Movement and Action Perception
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: June 27, 2012
Latest Amendment Date: January 07, 2014
Award Instrument: Continuing grant
Program Manager: Betty H. Tuller
Start Date: July 01, 2012
Expires: June 30, 2017
Awarded Amount to Date: $439,923
ARRA Amount: $
Investigator(s): Ayse Saygin apsaygin@gmail.com (Principal Investigator) 
Organization: University of California-San Diego
Office of Contract & Grant Admin, La Jolla, CA 92093-0934, (858)534-4896
NSF Directorate: SBE
Program(s): CROSS-DIRECTORATE ACTIV PROGR PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 1045 7252 CL10
Program Element Code(s): 1397, 7252
Abstract: Understanding the movements of others is critical for a wide range of functions such as detecting prey and predators, inferring the goals and intentions of others, and engendering emotional responses, such as feeling compassion and empathy toward others. The investigators will use a range of complementary methods to understand how the human brain detects and interprets the body language of others. For example, body movements can be characterized by very minimalist visual representations, consisting of a dozen points of light attached to the joints on the body seen in darkness (a method used in many motion capture systems). In addition, the investigators will use high-resolution videos to link the work to the field of social cognition by determining how processing is affected when the movements are produced by an artificial agent like a robot, rather than by a human. <br/><br/>Understanding the perceptual and neural basis for body movement processing is essential to an account of how humans negotiate objects and events in the world and can inform fields as wide-ranging as cognitive science, neuroscience, robotics, brain-computer interfaces, social cognition, technology design, visual arts, and computer vision. In addition, social artificial agents such as humanoid robots and virtual animated characters are becoming increasingly common in a range of domains such as education, defense, healthcare and entertainment. This research can help guide the design of these interactive technologies. The investigators also aim to further science and education through recruitment and retention of young people, especially minorities underrepresented in science. Since early engagement is critical for this goal, the focus will be on the development of hands-on research and computation skills for high school and undergraduate students.

Award Number: 1059088
Title: Human Preferences for Color and Spatial Composition.
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: May 05, 2011
Latest Amendment Date: August 16, 2012
Award Instrument: Continuing grant
Program Manager: Anne Cleary
Start Date: May 01, 2011
Expires: April 30, 2014
Awarded Amount to Date: $450,000
ARRA Amount: $
Investigator(s): Stephen Palmer sepalmer@gmail.com (Principal Investigator) 
Organization: University of California-Berkeley
Sponsored Projects Office, BERKELEY, CA 94704-5940, (510)642-8109
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7252
Program Element Code(s): 7252
Abstract: Abstract <br/><br/>People show strong preferences for certain colors and spatial compositions in visually displayed images, preferences that pervasively influence a multitude of decisions, such as buying clothes, taking vacations, decorating homes, and landscaping yards. Yet, surprisingly little is known scientifically about what color combinations and spatial arrangements people like or why they like them. Many previous treatments have come from philosophers, artists, and business consultants. Dr. Stephen Palmer and his research team at University of California -- Berkeley take a scientific, descriptive approach by measuring what people prefer and determining what factors determine these preferences. Further, they are also discovering why people have those preferences. <br/><br/>An early discovery of this line of research, for example, is that people's average preference for a given color is largely determined by how much they like the objects that characteristically have that color. For instance, people generally like blues because they generally like clear skies, clean water, and most other similarly colored objects. In addition, people show strong and systematic tendencies to prefer objects that are positioned at or near the center of the frame, but offset horizontally so that they face into (rather than out of) the frame, and offset vertically so that their position within the frame reflects their typical position relative to a viewer (e.g., a flying eagle toward the top of the frame and a swimming stingray toward the bottom). <br/><br/>The investigators propose to extend these projects in important ways. In the domain of color preferences, they will test their "ecological valence theory" of color preference by determining whether it can account for cross-cultural differences, using the same methods they have developed to test the theory previously. They plan to use similar methods to find out whether the theory can also account for individual differences in color preferences. In the domain of spatial composition, they plan to test a theory of compositional symmetries and asymmetries based on what they call "affordance spaces" around objects. <br/><br/>The proposed research project provides promising bridges among science, art, and business, using rigorous scientific methods to understand human preferences that are distinct from well-studied cognitive (knowledge-based) processes. It also defines new research methods aimed at understanding different patterns of preference between individuals across different domains. Do people who prefer unusual, highly contrasting color combinations, for example, also prefer nonstandard spatial compositions and less conventional music? The answers to such questions have important implications for business decisions and consumer marketing techniques. Given the pioneering nature of the proposed research on an under-studied topic, many new discoveries are being made that provide links between existing scientific knowledge in vision and cognitive science on the one hand and practical issues in the business world on the other.

Award Number: 1320410
Title: Integrating low-level speech features into a model of speech perception
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: July 27, 2013
Latest Amendment Date: September 16, 2013
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: September 01, 2013
Expires: August 31, 2015
Awarded Amount to Date: $179,724
ARRA Amount: $
Investigator(s): Naomi Feldman nhf@umd.edu (Principal Investigator) 
Organization: University of Maryland College Park
3112 LEE BLDG, COLLEGE PARK, MD 20742-5141, (301)405-6269
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION ROBUST INTELLIGENCE 
Program Reference Code(s): 7252 7495 9251 9178
Program Element Code(s): 7252, 7495
Abstract: Why are humans so much better than machines at recognizing speech? This research aims to measure differences between humans and machines in how they compute similarities between sounds. A computational model of speech perception will be trained on speech production data, using several types of features that are typically used in speech recognitions systems. It will then be tested on its ability to predict human listeners' responses in speech sound discrimination tasks. Results are expected to provide information about how the speech that listeners hear shapes their perception of sounds, as well as how well the information used by automatic speech recognition systems matches the information used by human listeners.<br/><br/>By allowing us to compare how humans and speech recognition systems use information when perceiving speech, this research will provide a tool that can help make speech recognition systems more human-like. Reverse engineering human perception can improve the way these systems generalize to new dialects, talkers, and noise conditions. This has the potential to facilitate the construction of systems for low-resource languages, broadening the impact of speech recognition technologies.<br/><br/>[Supported by SBE/BCS/PAC and CISE/IIS/RI]

Award Number: 1148638
Title: Can Behavioral Data Underlying Receiver Operating Characteristic (ROC) Analysis Support Complex Theories of Perception and Memory?
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 12, 2012
Latest Amendment Date: September 11, 2013
Award Instrument: Continuing grant
Program Manager: Betty H. Tuller
Start Date: September 01, 2012
Expires: August 31, 2015
Awarded Amount to Date: $235,765
ARRA Amount: $
Investigator(s): Jeffrey Rouder rouderj@missouri.edu (Principal Investigator) Paul Speckman (Co-Principal Investigator) 
Organization: University of Missouri-Columbia
310 JESSE HALL, COLUMBIA, MO 65211-1230, (573)882-7560
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION METHOD, MEASURE & STATS  
Program Reference Code(s): 7252 1333
Program Element Code(s): 7252, 1333, L598
Abstract: One of the primary tools in the investigation of perception and memory is the Receiver Operating Characteristic (ROC), which is a plot of hits ('yes' responses on target trials) vs. false alarms ('yes' responses on non-target trials). Researchers commonly claim that the details of these curves provide insight into cognitive phenomena of interest. It is not clear, however, that ROCs are sufficiently rich so as to allow differentiation of processes. The basic idea to be explored is whether ROCs collected across disparate domains have the same pattern, that is, can all be explained by common processing, or have different patterns, that is, can support rich theories of processing. Based on our observations, we will develop flexible, nonparametric, model-free tests of ROC pattern equivalence. These tests will allow us to assess whether ROCs can be used to make detailed inferences about processing.<br/><br/>Understanding human memory has many practical consequences in education, professional training, health delivery, and even in legal settings. Memory researchers have posited many two-process models to account for their results, including 'remember-know', 'automatic-controlled', and 'unconscious-conscious'. Unfortunately, direct tests of these models have often lacked rigor, because they have used statistical tests such as ROCs in a confirmatory mode. This project will allow perception and memory researchers to understand how to use ROCs in a more principled way.

Award Number: 1256959
Title: Classifying Categorization Using State Trace Analysis and Hierarchical Bayesian Modeling
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: February 25, 2013
Latest Amendment Date: February 25, 2013
Award Instrument: Standard Grant
Program Manager: Anne Cleary
Start Date: March 15, 2013
Expires: February 29, 2016
Awarded Amount to Date: $521,947
ARRA Amount: $
Investigator(s): Michael Kalish mlkalish@syr.edu (Principal Investigator) Michael Lee (Co-Principal Investigator) 
Organization: University of Louisiana at Lafayette
104 University Circle, Lafayette, LA 70503-2701, (337)482-6203
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION EXP PROG TO STIM COMP RES 
Program Reference Code(s): 9150
Program Element Code(s): 7252, 9150
Abstract: Humans must categorize the world in order to understand it. Categories like healthy versus poisonous, valuable versus overpriced, or even apples versus oranges coordinate our thinking and guide our actions in the world. In line with their centrality in cognition, current theories of categorization and category learning are numerous and diverse. The goal of this research project is to gather together these diverse theories and subject them to direct comparison using new and innovative statistical theories, computer implementations, and experimental designs. This project aims to serve Socrate's goal of carving nature at its joints by finding out why different people, at different times, learn different kinds of categories in different ways. <br/><br/>This project has many potential scientific and practical ramifications. In particular, the techniques to be developed are directly relevant to research in the psychology of memory, judgment, and decision-making. More broadly, knowing if different category types are reliably different in the way they are learned, or if people are reliably different in the way they learn categories, will allow development of programs to help people learn difficult categories (like cancerous vs benign radiographs) more quickly and accurately.

Award Number: 1052784
Title: Collaborative Research: A Bayesian model of phonetic and phonotactic effects in cross-language speech production
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: August 15, 2011
Latest Amendment Date: August 15, 2011
Award Instrument: Standard Grant
Program Manager: William J. Badecker
Start Date: September 01, 2011
Expires: February 28, 2015
Awarded Amount to Date: $163,817
ARRA Amount: $
Investigator(s): Colin Wilson wilson@cogsci.jhu.edu (Principal Investigator) 
Organization: Johns Hopkins University
3400 N CHARLES ST, Baltimore, MD 21218-2608, (410)516-8668
NSF Directorate: SBE
Program(s): LINGUISTICS PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 1311 7298
Program Element Code(s): 1311, 7252
Abstract: A fundamental aspect of human cognition is the capacity to perceive and produce language. Studies of non-native speech processing provide some of the most striking evidence bearing on this capacity: when humans attempt to perceive or produce words containing foreign sounds or sound sequences, they show systematic patterns of correct and incorrect performance. Prior research has established that different non-native structures elicit different rates and types of error; it has been hypothesized that these differences can be explained by a combination of grammatical, perceptual, and articulatory factors. The main goals of this project are to provide carefully controlled experimental evaluations of these factors, and to develop an explicit, probabilistic model of how they interact in human performance. Particular experimental issues to be investigated are: (1) what phonetic characteristics humans are most sensitive to when processing non-native sounds; (2) how the quality of the input and ambient acoustics affect non-native perception and production; and (3) whether learning word meaning can modulate sensitivity to detailed properties of non-native sounds. The computational model builds on a growing body of work suggesting that human perception and action reflect optimal Bayesian inference conditioned on prior expectations and noisy sensory measurements. The relevant prior reflects knowledge of the native language; the model predicts that non-native structures that are more similar to those in the native language should be processed with greater accuracy. The model also predicts that non-native sounds with robust perceptual properties should be processed more accurately, even if their prior probabilities are low. The development of this model, which will be made available to other researchers, will promote the role of phonology and phonetics within the broader context of cognitive science research. Because our experiments examine the impact of classroom acoustics and talker variation on non-native sound processing, this project also has ramifications for foreign language pedagogy.

Award Number: 1126957
Title: Implications of Fractal Scaling in Sensorimotor Systems
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: August 10, 2011
Latest Amendment Date: August 10, 2011
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: August 15, 2011
Expires: July 31, 2014
Awarded Amount to Date: $299,255
ARRA Amount: $
Investigator(s): Mark Shelhamer mjs@dizzy.med.jhu.edu (Principal Investigator) Steven Lowen (Co-Principal Investigator) 
Organization: Johns Hopkins University
3400 N CHARLES ST, Baltimore, MD 21218-2608, (410)516-8668
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 
Program Element Code(s): 7252
Abstract: In this project, the investigators will develop a set of computational tools to investigate long sequences of continuous data from physiological systems. One interesting property exhibited by these data sets is called "fractal scaling." This means that the data are "self-similar" on different time scales, so that short sections of data, when rescaled properly, resemble larger sections of data and this scaling holds true over a large range of time intervals. This is a defining feature of a fractal. Another fractal property is that the data exhibit fluctuations on many different time scales. Not all physiological data sets show this attribute. Some show instead a particular time scale at which the variability is most prominent. Many analytical and computational tools to study fractals have been developed over the last two decades. However, the large number of these tools makes it difficult for many investigators to make the proper choice when analyzing any given data set. Furthermore, different computational tools are susceptible to different types of artifacts and different tools are valid for different data types. Erroneous or misleading results can be obtained when the tools are improperly applied. It is very difficult for a non-expert in fractal mathematics to understand and keep track of all of these issues, which can inhibit progress in the investigation of many physiological and behavioral systems. In this project, the research team will assemble a set of established computational tools for fractal time-series analysis, apply them to a variety of data sets with known and unknown properties (i.e., artificially generated data and real physiological data), and determine when different tools break down, when they are most effective, how to detect when erroneous results are being generated, and how to interpret the results. This will provide increased confidence in the application of these tools and encourage their widespread use.<br/><br/>Fractal fluctuations are common in the behavior of biological systems. They have been found in heartbeat intervals, stride intervals, firing rates of neurons, human reaction times, and many other cases. However, the source and significance of these characteristics are unclear. They could arise from fractal properties of ion channels in neurons and have no special role or relevance. On the other hand, they could be deliberately produced at the behavioral level and confer a physiological benefit. By developing and making freely available a standard set of computational tools that implement the most modern algorithms for assessing data for fractal properties, the current project can further our understanding of how sensorimotor systems organize, how behaviors are produced, how sensory information is processed and how perceptions emerge.

Award Number: 1362417
Title: Neurodynamics of Tonality
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: December 02, 2013
Latest Amendment Date: December 02, 2013
Award Instrument: Continuing grant
Program Manager: Anne Cleary
Start Date: August 17, 2013
Expires: July 31, 2014
Awarded Amount to Date: $18,291
ARRA Amount: $
Investigator(s): Edward Large edward.large@uconn.edu (Principal Investigator) 
Organization: University of Connecticut
438 Whitney Road Ext., Storrs, CT 06269-1133, (860)486-3622
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7252
Program Element Code(s): 7252
Abstract: Music is a high-level cognitive capacity, a form of communication that relies on highly structured temporal sequences comparable in complexity to language. Music is found among all human cultures, and musical "languages" vary among cultures and depend upon learning. For example, European melodies use different kinds of note combinations than Indian melodies, making it difficult for Westerners to understand Indian music, and vice versa. Unlike language, however, music rarely refers to the external world. It consists of self-contained patterns of sound, aspects of which are found universally among musical cultures. Therefore, while an understanding of the brain processes underlying language is still a distant goal, discovering the general principles of neural dynamics that underlie music may now be possible. Tonality refers to the stability relationships that are perceived among notes in a musical language. Although there are different kinds of tonality, tonality itself is a universal feature of music, found in virtually every musical language. The hypothesis of this research is that neural oscillation underlies tonal cognition and perception. Neural oscillation is periodic neural activity that, in the auditory system, becomes time-locked to incoming sounds. Neural oscillations can be complex, but there are now powerful mathematical tools for analyzing them. Mathematical analyses of time-locking auditory dynamics suggest constraints on what sorts of tonal relationships should be possible. They predict that fundamental principles of neural dynamics combined with fundamental principles of neural plasticity constrain what musical languages can be learned.<br/><br/>To make detailed predictions, a sophisticated computer model of the auditory system will be built, based on the organization of the auditory system and general neurodynamic principles. Two simulations will be trained through passive exposure to European and North Indian melodies. These two are chosen because they represent two very different musical languages that are each relatively well-studied. The computer model will be used to predict neurophysiological and perceptual observations about music perception that have been collected over the past thirty-five years or so. Success of this model would imply the existence of a musical universal grammar. Universals predicted by intrinsic neurodynamics would provide a direct link to neurophysiology, and explain how brain changes during learning can establish different musical languages. This could lead to fundamental paradigm shifts in music theory, music cognition and related fields. The success of this model would be equally influential in cognitive neuroscience. It would imply that high-level cognition and perception can arise from the interaction of acoustic signals with the physics of the auditory system. No neurodynamic approach has ever successfully captured such a high-level cognitive capacity. Researchers are currently struggling with the question of how to reconcile cognitive theories with neurodynamic principles and observations, and success in the musical domain could lead to new insights. This research will elucidate fundamental mechanisms of hearing and communication, and holds significant promise for understanding auditory system development. Identification of innate constraints shaping human communication behavior may have further implications for language learning. This research has implications for understanding a wide range of hearing and communication disorders. It has potential applicability to improving the design of neural prostheses, enhancing the perception of music and other sounds in cochlear implant patients.

Award Number: 1057625
Title: Collaborative Research: Multisensory Perceptual Learning
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: March 31, 2011
Latest Amendment Date: August 16, 2012
Award Instrument: Continuing grant
Program Manager: Anne Cleary
Start Date: April 01, 2011
Expires: March 31, 2014
Awarded Amount to Date: $238,675
ARRA Amount: $
Investigator(s): Aaron Seitz aseitz@ucr.edu (Principal Investigator) 
Organization: University of California-Riverside
Office of Research, RIVERSIDE, CA 92521-1000, (951)827-5535
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7252
Program Element Code(s): 7252
Abstract: Ladan Shams, University of California at Los Angeles<br/>Aaron Seitz, University of California at Riverside<br/><br/>COLLABORATIVE RESEARCH: MULTISENSORY PERCEPTUAL LEARNING<br/><br/>ABSTRACT<br/><br/>In daily life, we frequently experience correlated sensations across our different sensory modalities. For example, as we climb up the stairs, we receive sensations to our auditory, visual, tactile, and vestibular systems that are all related to the experience of stair-climbing. These types of multisensory experiences are a key aspect of how we interact with, and learn about, the world around us. Through our experience with the world, our sensory abilities undergo refinements that allow us to optimize our performance in the tasks that we perform. These sensory refinements involve fine-tuning of processing in each of our sensory modalities, but equally importantly, in how we merge information across modalities. While much research has focused on how learning can take place within each individual sensory system, the learning of how information is combined across the senses has been largely neglected. The PIs will conduct a series of experiments in which they can track visual, auditory, and auditory-visual multisensory learning in parallel, and discriminate among different theories of multisensory processing and learning. Behavioral and neuroimaging methods will be combined to shed light on the roles that different brain areas, and the interactions between brain areas, play in the process of multisensory learning. Altogether these studies will provide fundamental insights into how our sensory systems work together and refine their interactions to best operate in the tasks that we perform.<br/><br/>This project will be the first systematic investigation of multisensory perceptual learning. It will also be the first study of changes in interaction between brain areas that may occur as a result of sensory learning. Altogether, this study promises to provide foundational knowledge regarding the brain mechanisms involved in multisensory learning as well as the mechanisms of learning in general. Understanding multisensory learning can contribute to the development of more effective strategies for learning. These strategies can be utilized to enhance learning for typically-developed children and adults, as well as to facilitate learning and communication for individuals with deprivation in one sense (e.g., individuals with low-vision or low-hearing, patients with cochlear implants or undergoing macular degeneration or cataract surgeries). They can also contribute to devising remedial programs for dyslexia, which appears to involve deficits in combining information across the senses.

Award Number: 1255538
Title: CAREER: The Role of Self-Directed Learning in Facilitating Concept Acquisition: Advancing Research and Training in the Cognitive Science of Learning
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: July 10, 2013
Latest Amendment Date: July 10, 2013
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: July 15, 2013
Expires: June 30, 2018
Awarded Amount to Date: $721,540
ARRA Amount: $
Investigator(s): Todd Gureckis tg35@nyu.edu (Principal Investigator) 
Organization: New York University
70 WASHINGTON SQUARE S, NEW YORK, NY 10012-1019, (212)998-2121
NSF Directorate: SBE
Program(s): REAL PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 1045 7625
Program Element Code(s): 7625, 7252
Abstract: NSF Summary Statement<br/><br/>Humans, particularly children, are often likened to sponges, soaking up patterns and regularities in their environment as they learn new concepts and skills. However, unlike a passive sponge, human learners are able to alter their experience in important ways by interacting with the world. For example, when studying for a test, learners can decide how to allocate study effort across materials. Any complete account of human learning must explain not only what is learned from the information we experience, but also the capacity for our actions and choices to expose that information. Many laboratory studies of learning used in cognitive psychology are silent on this issue because they limit participants' control over the flow information (emphasizing instead a type of "passive" learning where the learner does not have control over which information is experienced). This tends to create a blind spot in our theories for how people gather information, when they decide to stop, and for how these ongoing decisions impact learning.<br/><br/>The research in this proposal seeks to address this gap by extending theories of human learning to account for self-directed learning behaviors (i.e., situations where learners can control of the flow information they experience by way of their ongoing decisions). The proposed research addresses two specific questions relevant to the cognitive science of learning. First, how do people decide what information will be useful to know when learning? Second, what impact does self-directed learning have on the acquisition of novel concepts? In answer these questions, a computational theory of human learning is tested which attempts to explain how people make decisions to gather more information. A novel aspect of the theory is that it can account for the differences in learning resulting from either self-directed or passive learning. Understanding self-directed learning is a core issue in cognitive psychology which has potential implications for education policy. In addition, the principles identified in this research may translate into assistive learning technologies which may shorten the time required to develop expertise in a domain by tailoring training to the capabilities of individual learners. Alongside this research objective, an overarching goal of this proposal is to integrate research and training activities in computational cognitive science. An innovative training plan will prepare the next generation of psychological scientists for a field that is increasingly organized around computational approaches to the study of the mind.

Award Number: 1059560
Title: The Role of Visual Attention in Multiple Object Tracking
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: March 31, 2011
Latest Amendment Date: March 31, 2011
Award Instrument: Standard Grant
Program Manager: Anne Cleary
Start Date: April 01, 2011
Expires: September 30, 2014
Awarded Amount to Date: $370,602
ARRA Amount: $
Investigator(s): James Hoffman hoffman@udel.edu (Principal Investigator) 
Organization: University of Delaware
210 Hullihen Hall, Newark, DE 19716-2553, (302)831-2136
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 
Program Element Code(s): 7252
Abstract: Many critical tasks such as driving and piloting a plane depend on tracking multiple moving objects with continuously changing positions, speeds, and trajectories. Laboratory studies of multiple object tracking (MOT) require observers to keep track of a small number of moving target objects embedded in a set of identical moving distractors. Surprisingly, these studies show that people can only keep track of about four objects before they begin to make errors. How do people divide their attention between tracked objects while ignoring distractors and why is divided attention limited to four objects? The investigator proposes to study these questions by presenting probe flashes on targets and distractors during MOT and measuring visual attention by recording brain activity using EEG-based measures. Preliminary studies using this approach show that successful tracking performance depends on both enhancement of tracked targets and suppression of distractors. The proposed studies should further reveal why tracking is limited to a small number of objects as well as illuminating why errors occur when attentional capacity is exceeded. <br/><br/>The multiple object tracking (MOT) task captures an important aspect of many critical, real-world tasks such as air-traffic control where errors can have catastrophic consequences. In 2009, there were 1,869 "operational errors" recorded for air traffic controllers including 333 in the serious category. These numbers are considered to be a small fraction of the actual number of errors as the majority go unreported. At least part of the reason for these errors may lie in the limited ability to track more than about four objects. Are there interventions that might improve this critical ability? Importantly, several studies show that the number of tracked objects can be increased through training. For example, it is higher in people who play video games. Conversely, recent reports indicate substantial deficits in the number of objects than can be tracked due to both psychopathology (schizophrenia) and normal aging. Understanding the basic brain mechanisms responsible for these effects is the best approach to ultimately being able to design training methods for improving tracking ability as well as reducing environmental contributions to errors. Finally, from a basic science perspective, understanding MOT should help us understand the mechanisms of visual attention more broadly. Many current theories of attention are based on studies requiring observers to briefly attend to a single stationary location or object. These theories do a poor job of explaining the ability to sustain attention on multiple moving objects for extended periods of time, and this kind of attention may be commonplace in dealing with the dynamic visual world outside of the laboratory.

Award Number: 0842446
Title: Improving Fluid Intelligence by Training Working Memory
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 17, 2009
Latest Amendment Date: May 20, 2013
Award Instrument: Standard Grant
Program Manager: Anne Cleary
Start Date: October 01, 2009
Expires: September 30, 2014
Awarded Amount to Date: $312,317
ARRA Amount: $
Investigator(s): John Jonides jjonides@umich.edu (Principal Investigator) 
Organization: University of Michigan Ann Arbor
3003 South State St., Ann Arbor, MI 48109-1274, (734)763-6438
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 0000 OTHR
Program Element Code(s): 7252
Abstract: Improving Fluid Intelligence by Training Working Memory<br/><br/>Principal Investigator: John Jonides<br/>University of Michigan<br/><br/>Fluid intelligence (an important component of what is usually termed "IQ") is the ability to reason and to solve new problems independently of previously acquired knowledge, and so it is considered one of the most important factors in learning. Moreover, fluid intelligence is closely related to professional and educational success. There is considerable agreement that fluid intelligence is highly heritable, but this does not mean that it cannot be influenced by education and socialization. Recently, the research group of Dr. John Jonides at the University of Michigan published a widely acclaimed article in which they report evidence of transfer from training on a demanding working memory task to measures of fluid intelligence. Working memory is the system that is responsible for the short-term storage of information to be used in the service of higher-level cognitive processing. Individuals were trained on a working memory task that required simultaneously holding in mind spatial and verbal information that constantly had to be updated as the task continued. This working memory training effect is a surprising finding both because previous efforts to train IQ have thus far not been successful, and because it is all too rare to find transfer effects from any training task to another task that differs from the trained task in content. The transfer in question resulted even though the trained task was entirely different from the intelligence test itself. Furthermore, the results indicated that the amount of gain in IQ critically depended on the amount of training: the more training, the more improvement in fluid intelligence. The present NSF-funded project seeks to examine the brain basis of this training effect. It is based on the rationale that successful transfer from working memory to measures of intelligence must come about because there are brain mechanisms in common between the two. The project involves testing participants' fluid intelligence, training them for 5 weeks on a working memory task that has been shown to influence fluid intelligence, and then testing their fluid intelligence after training. Crucially, participants will be scanned using functional magnetic resonance imaging (fMRI) while they take intelligence tests, early and late in training on working memory, and then again when they take intelligence tests after training. The data should yield important insights about the brain regions that underlie working memory and fluid intelligence and whether these regions overlap between the two sets of tasks as expected. <br/><br/>This research project is significant in several ways. First, understanding the relationship between working memory and fluid intelligence leads the way to developing training schemes to improve fluid intelligence. There can be little doubt that facilitating the intelligence of individuals can only yield benefits for society in its productivity and its intellectual approach to society's problems. Second, understanding the brain mechanisms that are shared in common between working memory and intelligence permits prediction about what cognitive skills will be lost after certain brain traumas and what remediations may be effective. Third, undergraduates, graduate students, and postdoctoral researchers who are engaged in the project will learn the fundamentals of scanning in the functional magnetic resonance imaging environment as well as experimental skills, thereby strengthening their scientific training.

Award Number: 1031899
Title: Modeling memory to enhance motor learning
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: August 08, 2010
Latest Amendment Date: July 15, 2012
Award Instrument: Continuing grant
Program Manager: Betty H. Tuller
Start Date: September 01, 2010
Expires: August 31, 2014
Awarded Amount to Date: $344,536
ARRA Amount: $
Investigator(s): Nicolas Schweighofer schweigh@usc.edu (Principal Investigator) 
Organization: University of Southern California
University Park, Los Angeles, CA 90089-0701, (213)740-7762
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7969
Program Element Code(s): 7252
Abstract: This proposal takes an innovative stand in proposing that computational neuroscience can guide the design of effective and motivating adaptive training schedules for motor tasks. Although the goals of learning are generalization and long-term retention, current performance is a poor predictor of these learning goals. In this proposal, the general hypothesis is that adaptive scheduling of multiple motor tasks, based on long-term memory predictions, can enhance learning and that these long-term predictions are most effective when derived from neurally-based computational models of the motor memory system. The two specific research objectives of the proposed work are 1) to determine the mechanisms of multiple motor adaptation in humans, using a combined computational and behavioral approach, and 2) to investigate methods for tailoring training schedules to individual learners using multiple motor adaptation tasks.<br/><br/>The proposed research is in line with two of the 14 grand challenges for the 21st century, identified by the U.S. National Academy of Engineering (NAE): 'reverse-engineering the brain' and 'advancing personalized learning.' The work proposed considers these challenges as related and that 'advancing personalized learning' must be based on an understanding of the learning and motivational systems of the brain. Although there have been a few attempts to generate learning programs along these lines, this type of research is still in its infancy and is mostly based on descriptive models of learning and memory. Here, the PI will reverse-engineer the motor memory system with computational models that are both neurally and behaviorally valid and relevant. This has the potential to be useful in a large number of applications, including rehabilitation of movement-impaired patients (e.g., stroke patients), sport and exercise education, dance instruction, and special needs education.

Award Number: 1152819
Title: Language Processing as Boundedly Optimal Control of Memory, Perception, and Action
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: April 24, 2012
Latest Amendment Date: July 30, 2013
Award Instrument: Continuing grant
Program Manager: Anne Cleary
Start Date: May 01, 2012
Expires: April 30, 2015
Awarded Amount to Date: $401,267
ARRA Amount: $
Investigator(s): Richard Lewis rickl@umich.edu (Principal Investigator) Satinder Baveja (Co-Principal Investigator) 
Organization: University of Michigan Ann Arbor
3003 South State St., Ann Arbor, MI 48109-1274, (734)763-6438
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION ROBUST INTELLIGENCE 
Program Reference Code(s): 7252 7495
Program Element Code(s): 7252, 7495
Abstract: Language comprehension, whether listening or reading, is a rapid skill that requires the coordination of perception (e.g., recognizing printed words on a page), action (e.g., controlling the movement of the eyes), memory (e.g. remembering what came in earlier parts of a sentence to relate them to later parts), and linguistic knowledge (e.g., using the order of words to constrain possible relations among them). Understanding how these processes are combined on a moment-to-moment basis has been a major challenge for cognitive science. This project will develop and test a new theory of language comprehension from an adaptive perspective. The hypothesis is that language processing may be understood as the coordinated, adaptive control of both internal (and limited) cognitive processes and external perceptual-motor actions so as to maximize specific task goals. To apply this idea to language, the investigators will formalize and implement optimal control models of limited Bayesian perception and memory for different reading tasks. These Bounded Optimal Control models will generate predictions that will be tested by monitoring the eye-movements and responses of humans.<br/><br/>Bounded Optimal Control is a new approach to language, thought, and action, with possibly major long-term implications for how we understand and address individual differences and deficits in reading ability. It represents a shift in scientific focus from seeking to describe the perceptual and cognitive strategies that the mind and brain employ during reading, to specifying clearly the problems for which these strategies are the solutions. There is no canonical strategy because there is no canonical problem. Thus, the best cognitive and perceptual strategies are determined by specific task goals and individual constraints. This has the potential to change our approach to reading deficits: rather than asking how an individual's reading strategies deviate from normal, it is possible to ask instead whether those strategies are the best possible strategies, taking into account both task goals and idiosyncratic cognitive constraints. Revealing the true nature of the gaps between actual behavior and "bounded optimal" behavior is an important step in understanding how to go about closing them.

Award Number: 1232639
Title: Collaborative Research: Emotional Sophistication - Studies of Facial Expressions in Decision Making
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: August 27, 2012
Latest Amendment Date: August 27, 2012
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: September 01, 2012
Expires: August 31, 2015
Awarded Amount to Date: $166,977
ARRA Amount: $
Investigator(s): Clayton Morrison clayton@sista.arizona.edu (Principal Investigator) Alan Sanfey (Co-Principal Investigator) 
Organization: University of Arizona
888 N Euclid Ave, TUCSON, AZ 85721-0001, (520)626-6000
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION DECISION RISK & MANAGEMENT SCI 
Program Reference Code(s): 6867 7298
Program Element Code(s): 7252, 1321
Abstract: Social and economic decisions cannot be fully explained by "rational" attempts to maximize monetary gain, even in very simple game-theoretic scenarios. Complex emotional processes such as anger, guilt or generosity act as hidden forces that lead to observable actions. Such "non-rational" motivations can drive our own decisions and they affect our beliefs about what motivates others' decisions as well. The goal of this project is to use automatic measurements of dynamic facial expressions, in combination with other measurements such as functional MRI (fMRI) and eye-tracking, to investigate the role of non-rational motivations in social decision making. The core of the approach is to use state-of-the-art computer vision techniques to extract facial actions from video in real-time while participants interact with a computer or with each other, in some cases viewing live video of each others' faces. The investigators will use powerful statistical machine learning techniques to make inferences about the participants' internal emotional states during the interactions. The goal is to use the inferences concerning emotional state (a) to predict participants' behavior; (b) to explain why a decision is made in terms of the hidden forces driving it; and (c) to build autonomous agents that can use this information to drive their interactions with humans. <br/><br/>This multidisciplinary project contributes to several fields such as psychology, neuroscience, and economics. First, it develops new methodologies to study decision processes. Second, it uses these methods to test hypotheses about social decision-making and to bridge the gap between observable actions and the internal states that generated them. Third, the investigators intend to make available a dataset and toolset that should be an extremely useful for other investigators analyzing facial expression in multiple contexts. Additionally, automatic and on-line decoding of internal motivational states lays the groundwork for "affectively-aware" interactive computers, or artificial systems that can make inferences about the emotions and intentions of their users. Through the development of these systems, this project will make a significant contribution to the growing field of human-machine interaction.<br/><br/>[Supported by Perception, Action and Cognition, Decision, Risk and Management Sciences, and Robust Intelligence]

Award Number: 0960529
Title: Iterative Models in Figure-Ground Perception: Tests and Challenges
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: June 12, 2010
Latest Amendment Date: June 12, 2010
Award Instrument: Standard Grant
Program Manager: Anne Cleary
Start Date: June 15, 2010
Expires: May 31, 2014
Awarded Amount to Date: $444,193
ARRA Amount: $
Investigator(s): Mary Peterson mapeters@u.arizona.edu (Principal Investigator) 
Organization: University of Arizona
888 N Euclid Ave, TUCSON, AZ 85721-0001, (520)626-6000
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 
Program Element Code(s): 7252
Abstract: Visual perception seems trivially easy. Nevertheless, it has long withstood the attempts of both computer scientists and vision scientists to crack its code, most likely because past theories were based predominantly on conscious vision. In order for perceivers to experience a coherent visual world, perceptual input must be organized into separate objects, or "figures." As part of this process, the brain decides where a figure lies with respect to every border between two contiguous regions of space. Consider a vertical border, for instance. The brain decides whether that border is a boundary for a figure lying on the left or the right. When the border is perceived as a boundary of a figure lying on the left, the region on the right seems simply to continue behind the figure at the border; no shape is perceived there. These "figure-ground" decisions are made outside of conscious awareness. Because of this, it is extremely difficult to investigate the mechanisms that produce figure-ground perception. An important, unanswered, question is whether figure-ground perception is accomplished via fast, feed-forward mechanisms or whether iterative mechanisms involving feedback from higher processing levels are involved. In feed-forward models, the input is processed in successive stages until a coherent percept emerges. In iterative models, feed-forward processing is not sufficient; feedback from higher to lower levels is necessary to create the percept.<br/><br/>Mary Peterson and her colleagues at the University of Arizona have designed visual displays that allow them to investigate this question. One display type is a small symmetric, bounded silhouette lying on a larger ground. Portions of familiar shapes are hidden along the groundside of the silhouette's border; these shapes are not perceived consciously, only the shape of the enclosed silhouette is perceived consciously. (The classic "face-vase illusion" is an example of this.) Previous experiments have shown that the shape of the hidden object is suppressed when it is not perceived, supporting the view that figure-ground perception results from inhibitory competition between shapes that might be seen on opposite sides of a border; the winner is perceived as the shaped figure, whereas the loser is suppressed and the portion of space where it might have been seen is perceived as a shapeless ground. These displays are designed to isolate competition at the shape processing stage. Dr. Peterson and colleagues will also conduct a series of experiments which test whether suppression can be observed at lower levels where individual parts are represented and at higher levels where shape descriptions ("semantics") are represented. According to a feed-forward account, suppression of the losing shape would prevent access to its semantics; hence, no effects should be evident at higher levels. Without feedback, no effects should be evident at lower, part-processing, levels either. An iterative view could account for suppression at lower and/or higher levels by assuming that the outcome of the between-shape competition was relayed to higher and lower levels. Thus the proposed experiments will adjudicate between these competing views of how perception occurs. The researchers will also attempt to identify the processes involved in segregating figures from grounds in crowded real-world scenes. The perception of these more complex displays constitutes a challenge to current iterative models of figure-ground perception. The planned research will provide a foundation for neurophysiological experiments and formal computational models of vision and will contribute to our understanding of the temporal and spatial dynamics of shape perception.

Award Number: 1151358
Title: Gaze Control during Scene Viewing: Behavioral and Computational Approaches
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: January 31, 2012
Latest Amendment Date: January 31, 2012
Award Instrument: Standard Grant
Program Manager: Anne Cleary
Start Date: February 01, 2012
Expires: January 31, 2015
Awarded Amount to Date: $497,523
ARRA Amount: $
Investigator(s): John Henderson john.henderson@sc.edu (Principal Investigator) 
Organization: University South Carolina Research Foundation
901 Sumter Street, COLUMBIA, SC 29208-0001, (803)777-7093
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION EXP PROG TO STIM COMP RES 
Program Reference Code(s): 9150
Program Element Code(s): 7252, 9150
Abstract: When we view the visual world, our eyes flit from one location to another about three times per second, in movements called saccades. Useful visual information is acquired only during fixations, brief periods of time when gaze rests on an object or scene feature. The cognitive and neural processes that direct saccades and fixations through a scene in real time fall under the term 'gaze control'. This project focuses on unraveling how human gaze control operates during active real-world scene perception.<br/><br/>This project approaches human gaze control by starting with the insight that understanding eye movement timing will provide key insight into the underlying cognitive and neural systems that control gaze. The research combines innovative eye-tracking methods with a working computational model that simulates eye movement control. In this research program, the empirical and computational threads are complementary and synergistic. On the one hand, we can test our understanding of gaze control by determining whether the model can produce eye movements that look like those produced by people. On the other hand, insights from the model can be used as a tool to enhance our theoretical understanding of gaze control, and these insights can be further tested with new experiments.<br/><br/>The results from this project will enhance basic scientific understanding of how humans perceive and understand the visual world. The project also has wide-ranging implications for the creation of new display technologies and machine interfaces that can be controlled by eye movements. And the results are relevant for the design of new artificial vision systems that actively track and focus on relevant information in the environment.

Award Number: 1151209
Title: Embodied Attention: Attentional Guidance by Body Position
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: February 13, 2012
Latest Amendment Date: September 25, 2013
Award Instrument: Standard Grant
Program Manager: Anne Cleary
Start Date: March 01, 2012
Expires: February 28, 2015
Awarded Amount to Date: $479,315
ARRA Amount: $
Investigator(s): Shaun Vecera shaun-vecera@uiowa.edu (Principal Investigator) Catherine Reed (Co-Principal Investigator) J. Toby Mordkoff (Co-Principal Investigator) 
Organization: University of Iowa
2 GILMORE HALL, IOWA CITY, IA 52242-1320, (319)335-2123
NSF Directorate: SBE
Program(s): EXP PROG TO STIM COMP RES PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 9150 9251 7252
Program Element Code(s): 9150, 7252
Abstract: Finding the ketchup in the refrigerator is an everyday activity that requires the use of visual attention. When faced with a cluttered visual environment (the refrigerator), the human visual system can isolate a single, behaviorally relevant item (the ketchup). Visual attention reduces the complexity of the visual world, enabling us to successfully navigate and function without being overwhelmed with the sheer amount of visual information. But how does attention know what to select? Some theoretical accounts propose that attention is directed toward salient (conspicuous) objects, whereas other accounts state that attention is directed toward goal-relevant objects. One limitation of both of these accounts is that they fail to acknowledge that attention is typically used in conjunction with action, which involves coordinating your body with objects in the environment. The goal of this research project is to understand how the body and its position (such as an outstretched arm reaching into the refrigerator) controls visual attention. The research team will investigate this notion of 'embodied attention' using both behavioral and brain imaging (event-related potential) methods.<br/><br/>Research on embodied attention is important in providing a fuller account of how the mind and body interact to function successfully in the world. The research also offers the possibility to improve the lives of older adults and those with neuropsychological deficits, because these people often have impairments in both motor function and the use of attention. It may be that understanding the connection between body position and attention will lead to treatments that increase the ability of these target populations to function in the world.

Award Number: 1127711
Title: Processing of Chromatic Information in the Peripheral Retina
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: July 20, 2011
Latest Amendment Date: July 20, 2011
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: August 01, 2011
Expires: July 31, 2014
Awarded Amount to Date: $335,582
ARRA Amount: $
Investigator(s): Vicki Volbrecht Vicki.Volbrecht@ColoState.edu (Principal Investigator) Janice Nerger (Co-Principal Investigator) 
Organization: Colorado State University
601 S Howes St, Fort Collins, CO 80523-2002, (970)491-6355
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 
Program Element Code(s): 7252
Abstract: Processing of Chromatic Information in the Peripheral Retina<br/><br/>There are two types of light detectors in the human retina, rods and cones. Cones perform best in daylight conditions, whereas rods are specialized for night vision. In the daylight, humans are able to perceive color. This is because there are three different types of cones, each picking up on different spectrums of color (blue, green, and red). At night, human color perception is relatively impaired.<br/><br/>This study focuses on how the cones and rods operate under less than ideal conditions; conditions where both rods and cones are detecting light at the same time but neither detector is operating at its best. Such lighting conditions would be comparable to those experienced at dawn or dusk. The reason for this study is that for many years it has been assumed that rods do not contribute to our ability to perceive color, even under conditions where both rods and cones are detecting the presence of light. It is now known that rods alter color perception, so the color of an object seen under daylight conditions may not be the same as the color seen at dawn or dusk. When color is used to convey information, but lighting conditions under which the color is viewed are changing, the information provided by color may not be accurate. The goal of this study is determine under what conditions failures in color perception arise and how the visual system may compensate for these failures. The results from this project may be applied to the design of color monitors, the efficacy of color information dissemination, and the design of cockpit and/or video displays.

Award Number: 1059662
Title: Dynamical Analysis of Language Structure and Learning
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: March 29, 2011
Latest Amendment Date: September 16, 2013
Award Instrument: Continuing grant
Program Manager: Betty H. Tuller
Start Date: April 01, 2011
Expires: March 31, 2014
Awarded Amount to Date: $273,729
ARRA Amount: $
Investigator(s): Whitney Tabor whitney.tabor@uconn.edu (Principal Investigator) 
Organization: University of Connecticut
438 Whitney Road Ext., Storrs, CT 06269-1133, (860)486-3622
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 6868 7298 9179
Program Element Code(s): 7252
Abstract: Rules play a central role in human behavior. Social rules (e.g., do unto others as you would have them do unto you; don't judge a book by its cover) support the harmonious coexistence of people in a society. Rules of individual choice-making (e.g., exercise daily, beware of strangers bearing gifts) support the well-being of individuals. Although we are usually not consciously aware of using them, there are many rule-like behaviors associated with our use of language (e.g. add -s to form the plural of a noun). An important property of linguistic rules, like social rules and individual rules, is that they admit exceptions (e.g., not all nouns form a plural with -s). Knowing when to apply the rules and when to make exceptions is an important facet of being an effective member of a language community. This project investigates neural models of the formation of linguistic rules to understand how humans achieve an appropriate balance between rigid rule-following and flexibility. It explores human learning of simple invented languages in laboratory experiments and compares the results with mathematical models that approximate important features of neural systems. The goals are to identify two extremes---situations where people adopt rigid, universal rules and situations where they treat every instance as a special case---and then to examine how a balance between these can be achieved. By studying the neural underpinnings of such learning processes, we can gain insight into what kinds of human-environment relationships sustain appropriately systematic but flexible behavior. <br/><br/>This project has implications for the science of learning and for understanding the neural connectivity patterns that underlie complex thinking. Regarding learning, people sometimes fail to discover systematic principles, e.g. learning the spelling-sound correspondences that support reading. This project may shed light on why this happens. Regarding societal well-being, the tendency of people to form stereotypes appears to be a case of learning a rule "too well," that is failing to strike the right balance between generalization and exception making. By examining what brain/environment circumstances lead to such inaccurate learning, the research may help societies avoid the formation of rigid stereotypes.

Award Number: 1127216
Title: Influence of Head and Eye Movements on Visual Input Statistics and Early Neural Representations
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: August 15, 2011
Latest Amendment Date: September 13, 2013
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: September 01, 2011
Expires: August 31, 2014
Awarded Amount to Date: $482,813
ARRA Amount: $
Investigator(s): Michele Rucci mrucci@bu.edu (Principal Investigator) 
Organization: Trustees of Boston University
881 COMMONWEALTH AVE, BOSTON, MA 02215-1300, (617)353-4365
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7252
Program Element Code(s): 7252
Abstract: We are normally not aware that our eyes and head are always in motion. Yet small, involuntary head and eye movements continually occur, even when we attempt to keep our gaze fixed on a point in the scene. It has long been known that the temporal modulations resulting from these movements are essential for seeing: the world becomes progressively fainter and will eventually fade altogether when the head and eyes are immobilized. However, the specific mechanisms by which fixational movements contribute to vision remain unknown. With funding from the National Science Foundation, Dr. Rucci and his team at Boston University will measure microscopic head and eye movements in human observers and reconstruct the input signals entering their eyes. The investigators will then use these data to quantify the impact of the observer's actions on the sensory flow of information and examine how this flow changes during the execution of different visual activities.<br/><br/>This research will investigate the mechanisms by which fixational movements contribute to the processing of visual information in the brain and the establishment of a stable visual percept. Elucidating the impact of fixational movements is critical to advancing our knowledge of how the visual system functions and how to build machines which replicate human visual capabilities. Furthermore, abnormal fixational eye movements<br/>occur in various pathological conditions. A thorough understanding of the functions of fixational eye movements may lead to treatment of the visual impairments associated with such conditions.

Award Number: 1344285
Title: INSPIRE Track 1: Crowd-sourcing neuroscience: Neural oscillations and human social dynamics
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 12, 2013
Latest Amendment Date: September 12, 2013
Award Instrument: Continuing grant
Program Manager: Betty H. Tuller
Start Date: September 15, 2013
Expires: August 31, 2016
Awarded Amount to Date: $833,777
ARRA Amount: $
Investigator(s): David Poeppel david.poeppel@nyu.edu (Principal Investigator) Mingzhou Ding (Co-Principal Investigator) 
Organization: New York University
70 WASHINGTON SQUARE S, NEW YORK, NY 10012-1019, (212)998-2121
NSF Directorate: SBE
Program(s): SOCIAL PSYCHOLOGY PERCEPTION, ACTION & COGNITION COGNEURO INSPIRE CONTROL SYSTEMS 
Program Reference Code(s): 8089 8653 7252 1699 7625 030E 031E 034E
Program Element Code(s): 1332, 7252, 1699, 8078, 1632
Abstract: This INSPIRE award is partially funded by the Perception, Action, and Cognition Program, the Cognitive Neuroscience Program, and the Social Psychology Program in the Division of Behavioral and Cognitive Sciences in the Directorate for Social, Behavioral, and Economic Sciences, the Research and Evaluation on Education in Science and Engineering Program in the Division of Research on Learning in Formal and Informal Settings in the Directorate for Education and Human Resources, and the Control Systems Program in the Division of Civil, Mechanical, and Manufacturing Innovation in the Directorate for Engineering. <br/><br/>The goal of the project is to understand naturalistic human social interaction, specifically in group contexts. While neuroscientists are increasingly recording from two participants concurrently, the neural basis of group dynamics remains uninvestigated. Capitalizing on the growing body of knowledge about the role of brain rhythms, the project builds on the hypothesis that one can characterize coupled neural oscillations between individuals as one candidate mechanism that tracks successful social communication in a dynamic context. This aim is pursued by using novel portable EEG technology to record brain activity from a large number of participants concurrently (between 10-20) in ecological situations, specifically a classroom. This will address the significant hardware and software challenges associated with recording data sets from groups. Moreover, the new type and amount of data will also require a novel analytic toolbox, which will form the basis of modeling multiple brains engaged in socially relevant situations.<br/><br/>The research will impact education and technology, and provide significant outreach opportunities. First, the key experiments will be performed in a high school classroom, in collaboration with the science teachers. As such, the project provides a new type of platform to provide hands-on STEM training. Second, the successful implementation of the wearable mobile brain EEG recording system will have significant impact on future neuroscience research, providing a valuable tool for research outside of the lab (e.g., in a crowd: theatres, schools), with populations that are otherwise difficult to reach (e.g., children, patients, the elderly). Finally, by comparing communication between people in the same room to people at a distance (e.g., MOOCs), this project contributes to issues surrounding the relevance of real-life behavioral cues to successful communication and teaching.

Award Number: 1343544
Title: INSPIRE Track 1: Action, Vision and Language, and their Brain Mechanisms in Evolutionary Relationship
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 10, 2013
Latest Amendment Date: September 10, 2013
Award Instrument: Continuing grant
Program Manager: Betty H. Tuller
Start Date: September 15, 2013
Expires: August 31, 2016
Awarded Amount to Date: $667,000
ARRA Amount: $
Investigator(s): Michael Arbib arbib@usc.edu (Principal Investigator) 
Organization: University of Southern California
University Park, Los Angeles, CA 90089-0701, (213)740-7762
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION ROBUST INTELLIGENCE IIS SPECIAL PROJECTS INSPIRE 
Program Reference Code(s): 7252 8089 8653
Program Element Code(s): 7252, 7495, 7484, 8078
Abstract: This INSPIRE award is partially funded by the Perception, Action, and Cognition Program in the Division of Behavioral and Cognitive Sciences in the Directorate for Social, Behavioral, and Economic Sciences and the Robust Intelligence Program in the Division of Information and Intelligent Systems in the Directorate of Computer and Information Science and Engineering.<br/><br/>This research will address and bridge two grand challenges: (1) To understand how action, perception, and social interaction were supported by the brain of the last common ancestor of macaque and human, complementing modeling elsewhere on great apes, and (2) To build on evolutionary insights to better understand how different parts of the human brain work together when we use language. Key entry points will be signed and spoken languages and the use of hand gestures (e.g., novel hand gestures by apes) to convey meaning. Going further, a particular focus will be on systems that link the brain's capacities to generate as well as recognize actions, and their interactions with other brain systems. <br/><br/>An international group of scientists in linguistics, primatology, neuroanatomy, neurophysiology, and neurocomputational modeling of motor, cognitive and language processes will pool data on the anatomy, physiology, behavior and communication of the various primate species. To support this extended collaboration, the researchers will build a novel online collaborative environment ("Collaboratory Workspaces") to test, make predictions, and challenge both the modeling and experimentation. This infrastructure may catalyze a new style of collaboration between modelers, experimentalists, and clinicians. <br/><br/>The research also has the potential to support modeling of the damage that results in the clinical disorders of apraxia and aphasia. Integration of models of vision, action and language is also important for creating robots that can flexibly and usefully interact with individual people and for "neuromorphic architecture," in which a building's sensors and action systems adaptively adjust to the human inhabitants.

Award Number: 0951612
Title: CAREER: Cued Recall: Theory and Data
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: August 27, 2010
Latest Amendment Date: September 10, 2013
Award Instrument: Continuing grant
Program Manager: Anne Cleary
Start Date: September 01, 2010
Expires: August 31, 2015
Awarded Amount to Date: $484,247
ARRA Amount: $
Investigator(s): Amy Criss acriss@syr.edu (Principal Investigator) 
Organization: Syracuse University
OFFICE OF SPONSORED PROGRAMS, SYRACUSE, NY 13244-1200, (315)443-2807
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 1045 1187 7252
Program Element Code(s): 7252
Abstract: Despite the importance of memory to everyday functioning and the functioning of society, the basic processes underlying the healthy function of the human memory system are not yet fully understood. "Episodic memory" is the ability to remember the components of a particular event, such as the food one ate for lunch yesterday (in contrast to the "semantic memory" for the fact that lunch is defined as a meal eaten in the middle of the day). It can be difficult to remember the specific details of a single event especially when the same components appear in multiple different events (e.g., bread may appear in many different meals) and events often repeat (e.g., lunch is eaten everyday). The NSF-funded research project conducted by Dr. Amy Criss at Syracuse University aims to evaluate three critical components of memory: first, the properties that contribute to the ability of a cue to successfully elicit a memory, independent of the content of the memory; second, the nature of the content of the memory that is successfully retrieved and reported, independent of the cue; finally, how cues and content can interact so that a cue is particularly effective for particular content, but not for other content (for example, sometimes a smell serves as a cue to elicit a very strong memory for a particular life event, such as a high school dance). To study these components of episodic memory, Dr. Criss will create laboratory events to be remembered by adult participants. Later, memory for those events will be measured. This research will advance understanding of episodic memory by using behavioral measures of memory in adults and by building a computer model that mimics the human memory system. <br/><br/>This research will contribute to understanding the fundamental processes that underlie human episodic memory. Memory is the essence of a person. It dictates behavior, preferences, fears, and abilities. Everyday failures of memory are common and costs range from spending a few extra minutes searching for car keys to falsely incriminating an innocent person. Critical components of society rely on memory. The judicial system relies on testimony of memory for events as a primary basis for determining innocence or guilt. Educators judge mastery of knowledge based on the ability to remember key facts during an examination. This research has the potential to inform the criminal justice system and educational testing on the properties of effective memory cues and also has the potential to contribute to treatments of memory disorders. This project also aims to train graduate and undergraduate students to be critical and effective consumers of science both inside and outside of the laboratory.

Award Number: 1056730
Title: CAREER: Individuation in Visual Cognition
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: February 25, 2011
Latest Amendment Date: September 10, 2013
Award Instrument: Continuing grant
Program Manager: Betty H. Tuller
Start Date: March 15, 2011
Expires: February 29, 2016
Awarded Amount to Date: $446,328
ARRA Amount: $
Investigator(s): Steven Franconeri franconeri@northwestern.edu (Principal Investigator) 
Organization: Northwestern University
1801 Maple Ave., Evanston, IL 60201-3149, (847)491-3003
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 1045 1187 7252 9251
Program Element Code(s): 7252
Abstract: Many visual tasks require that we divide attention across multiple locations at once. We need this ability for everyday tasks, from counting a set of chairs before a dinner party to navigating complex traffic during rush hour. We also need this ability at school or at work, when following the complex layout of a diagram or finding differences among values within graphs. In these kinds of tasks there is a surprising limit to our ability to split our attention and we can typically deal with only a few locations at once. The ubiquity of this limitation has led to its acceptance as a fundamental limit on visual processing, yet we have little understanding of why it happens. With the support of an NSF CAREER Award, Dr. Franconeri, Northwestern University, will test the possibility that these limits stem from a bottleneck within a cognitive 'map' of attended locations in the world. <br/><br/>Understanding the limits of divided attention could lead to important changes in the ways that we organize information in graphs and diagrams and may offer critical insight into our understanding of how children learn to count. It may also lead to better understanding of visual processing differences in autistic populations, who have greater difficulty dividing their attention. This research will be integrated with an education plan that includes (1) outreach to the general public through the design of a "Brain Week" series as well as other outreach talks in the Chicago area, (2) curriculum development, (3) advising of students and researchers from high school to postdoctoral levels, (4) outreach to underrepresented groups via recruitment to both Northwestern and the investigator's laboratory, including a summer fellowship for a local student from an under-represented background, and (5) collaborative outreach to related disciplines including education and information visualization.

Award Number: 1230793
Title: Current problems in lightness theory
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 18, 2012
Latest Amendment Date: September 18, 2012
Award Instrument: Standard Grant
Program Manager: Anne Cleary
Start Date: September 15, 2012
Expires: August 31, 2015
Awarded Amount to Date: $358,481
ARRA Amount: $
Investigator(s): Alan Gilchrist alan@psychology.rutgers.edu (Principal Investigator) 
Organization: Rutgers University Newark
Blumenthal Hall, Suite 206, Newark, NJ 07102-1896, (973)353-1538
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 
Program Element Code(s): 7252
Abstract: The two leading theories of how the white/gray/black shade of a surface is computed in your brain will be tested against each other in a series of 10 experiments. Human observers will be presented with three-dimensional displays carefully constructed so that, under controlled viewing conditions, the two theories make different predictions regarding the gray shade that will be perceived for a test surface embedded within the display. All of the displays will incorporate a serious challenge for vision, namely, multiple areas of light and shadow. These displays will vary in terms of (a) whether the change of illumination is projected onto a surface or occurs at a corner (or bend) in the surface, (b) the number of different gray shades on each surface, and (c) the range of gray shades on a given surface. The observers will be asked to match critical parts of each display for gray level and level of illumination. The strengths and weaknesses of each of the theories will be evaluated based on the observer matches. Ideally these results will suggest how the respective strengths of the two theories might be integrated.<br/><br/>How humans see the white, gray, and black shade of objects has not yet been explained scientifically. The basic problem is that the light that an object reflects to the eye does not reveal the shade of the object because it depends so heavily on the intensity of illumination on the object. Thus any intensity of light can be reflected from any shade of gray. The problem can only be solved by analyzing the surrounding context. Advances in our understanding of the nature of this analysis are absolutely necessary for guiding brain research in this area. The work will impact computer vision as well. At the moment, no robot can simply look at an object and report its shade of gray. Work with human vision provides the best hope for making better computer programs. Ironically, the proposed work will be conducted, not using computer images, as is almost universally done in human vision research, but with actual three-dimensional displays that are far more realistic.

Award Number: 1232654
Title: Prioritizing saccades according to their relative costs and benefits
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 21, 2012
Latest Amendment Date: September 21, 2012
Award Instrument: Standard Grant
Program Manager: Anne Cleary
Start Date: September 15, 2012
Expires: August 31, 2015
Awarded Amount to Date: $452,302
ARRA Amount: $
Investigator(s): Mark Harwood mharwood@sci.ccny.cuny.edu (Principal Investigator) 
Organization: CUNY City College
Convent Ave at 138th St, New York, NY 10031-9101, (212)650-5418
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 8089
Program Element Code(s): 7252
Abstract: Should I stay or should I go now? The clash between changing and staying the course is a very familiar choice in our everyday lives be it a trivial choice such as leaving a social event, or a 'big life choice' such as leaving one's job. This project hypothesizes that this type of decision underpins our most frequent choice: where to look next. On average, we move our eyes in discrete jumps (saccades) twice as often as we beat our hearts. Although this feels very automatic, each movement involves a mostly unconscious decision of when to move. Should I continue to exploit my maximum resolution vision on my current focal point, or is now the time to explore new regions of space? There are also costs as well as benefits to moving, since the visual disturbance during the movement (think major camera shake) is perceptually suppressed, meaning we are mostly blinded during the 10% of our waking lives spent making these movements. This project tests the novel hypothesis that saccade choices are governed by an evaluation of their relative costs and benefits, allowing us to prioritize better choices over worse ones. The same framework will be applied to test if the problems in initiating saccades seen in Parkinson's disease are due to a breakdown in a cost/benefit evaluation.<br/><br/>By understanding how such a simple, but pervasive, decision process is driven, the investigators expect to reveal more fundamental and general principles of decision making in the brain. Understanding how the brain achieves optimal behavior in dynamic environments, more broadly, is a critical issue in neuroscience and for intervening when this process goes awry. Clarifying these processes in Parkinson's disease could lead to better diagnostic tests of disease onset and progression.

Award Number: 1128769
Title: Collaborative Research: Implicit and Explicit Processes of Category-Based Induction
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 07, 2011
Latest Amendment Date: September 07, 2011
Award Instrument: Standard Grant
Program Manager: Anne Cleary
Start Date: September 15, 2011
Expires: August 31, 2014
Awarded Amount to Date: $356,795
ARRA Amount: $
Investigator(s): Gregory Murphy gregory.murphy@nyu.edu (Principal Investigator) 
Organization: New York University
70 WASHINGTON SQUARE S, NEW YORK, NY 10012-1019, (212)998-2121
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 
Program Element Code(s): 7252
Abstract: A fundamental component of intelligent behavior is how people make inferences about novel objects, people, and situations. In category-based induction, people use their knowledge of categories to make such inferences about new items from those categories (Will that dog attack? Which treatment will help this patient?). The present research investigates how people make such predictions in a common situation, when the categories are uncertain. For example, if medical treatment is required before a final diagnosis is possible, how do people take account of the various possible diagnoses (categories)? Past research has shown people often make inductions poorly in such situations, focusing on the most likely category and ignoring other possibilities. <br/><br/>The proposed research examines the basis for these errors in explicit, conscious predictions and whether reasoning can be improved by including information from implicit, fast, unconscious predictions. Pilot work suggests that implicit predictions can be more accurate than explicit ones. The proposed research attempts to confirm this finding and explores the psychological mechanisms that underlie the improved reasoning.<br/><br/>This research will have two important contributions. First, it will help in understanding the differences and similarities between conscious and unconscious processes. This distinction is crucial in much research on mind and behavior, and the present research will help us understand why these differences exist. Second, the results may lead to specific proposals for ways to improve predictions in medical, personal, and economic decisions. When categories are uncertain, people often do not make good predictions, but decision aids that help include implicit, unconscious processes may improve performance.

Award Number: 1331073
Title: Integrating Spatial and Temporal Models of Visual Attention
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: July 27, 2013
Latest Amendment Date: July 27, 2013
Award Instrument: Standard Grant
Program Manager: Anne Cleary
Start Date: August 01, 2013
Expires: July 31, 2016
Awarded Amount to Date: $335,197
ARRA Amount: $
Investigator(s): Bradley Wyble bpw10@psu.edu (Principal Investigator) 
Organization: Pennsylvania State Univ University Park
110 Technology Center Building, UNIVERSITY PARK, PA 16802-7000, (814)865-1372
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 
Program Element Code(s): 7252
Abstract: The human visual system uses attention to enhance the processing of information that is important for survival. For example, while driving on a highway, the sudden appearance of debris in the road will cause the visual system to rapidly orient towards the obstacle in order to allow avoidance. Attention orients in both space and time; it locks on to the location of the object, and also to the moment the object becomes visible. Currently, there is no well-accepted theory to describe how spatial and temporal factors interact in the control of attention. In order to create such a theory, Dr. Wyble will create a computational model of the spatial and temporal control of attention based on behavioral and EEG studies. This model will simulate the neural mechanisms that control attention and will thereby lead to a tighter coupling of theory and neuroscience. <br/><br/>The knowledge to be gained from this research will provide a simpler model of attention by combining existing theories from the spatial and temporal domains into a single framework. This framework will help us to understand why attention works in the way that it does, and also to understand why attention sometimes fails. The work may ultimately have applications in many areas, including transportation safety and visual prostheses for the blind. In addition, the work will include the development of a computer-programming tutorial aimed at increasing STEM training for students outside of traditional STEM fields.

Award Number: 1059166
Title: Surface color perception and illuminant cues in dynamic three-dimensional scenes
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: March 11, 2011
Latest Amendment Date: April 12, 2012
Award Instrument: Continuing grant
Program Manager: Anne Cleary
Start Date: April 01, 2011
Expires: March 31, 2014
Awarded Amount to Date: $324,062
ARRA Amount: $
Investigator(s): Laurence Maloney ltm1@nyu.edu (Principal Investigator) 
Organization: New York University
70 WASHINGTON SQUARE S, NEW YORK, NY 10012-1019, (212)998-2121
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 
Program Element Code(s): 7252
Abstract: One of the most impressive aspects of human vision is the stability of surface colors under very different lighting conditions. The visual system is remarkably good at separating the color of the illumination from the actual colors of surfaces and engineers are not currently able to build a camera that can match human performance. Most previous research has focused on very simple stimuli consisting of a small number of flat colored surfaces arranged on a flat panel under diffuse (uniform) lighting. The proposed research makes use of recent advances in computer graphics to create physically-accurate virtual scenes filled with objects at many different depths and orientations illuminated by realistic daylight illuminants mimicking the effects of sun, sky and cloud cover. The major challenge for the visual system in such scenes is that illumination is rarely uniform. In the research proposed here, the investigator will analyze how the visual system uses simple "cues" about illumination conditions such as surface shading and highlights to estimate stable surface colors. The use of eye tracking technology allows the experimenter to monitor where the observer looks in gathering information about the conditions of scene illumination. The proposed research represents the first use of eye-tracking technology to study surface color perception.<br/><br/>The human ability to assign stable surface colors and stable shapes to objects is an extraordinary achievement. This research will lead to a better understanding of how the brain adaptively makes use of sources of information about depth and color in scenes which, in turn, will provide a better understanding of how the brain reacts to injury, disease and even normal aging. Moreover, research on how the brain uses cues to scene layout and lighting can inform the production of artificial visual systems that duplicate biological function. Subtle changes in surface color accompany many disease states in plant and animal and such systems would have evident applications in remote sensing, from monitoring crops to early detection of illness.

Award Number: 0953730
Title: CAREER: Flexible Resource Allocation and Efficient Coding in Human Vision
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: July 07, 2010
Latest Amendment Date: July 30, 2013
Award Instrument: Continuing grant
Program Manager: Anne Cleary
Start Date: July 01, 2010
Expires: June 30, 2015
Awarded Amount to Date: $591,793
ARRA Amount: $
Investigator(s): George Alvarez alvarez@wjh.harvard.edu (Principal Investigator) 
Organization: Harvard University
1350 MASSACHUSETTS AVE, Cambridge, MA 02138-3846, (617)495-5501
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 1045 1187
Program Element Code(s): 7252
Abstract: Imagine you are driving in the middle lane of a busy three-lane highway, approaching your exit, with the radio on so you can hear the weather report, and with your children in the backseat calling for you. How are you going to manage this situation? Will you ignore all but one thing, and pay full attention to the road signs to determine when to switch lanes, or to cars in the rearview mirror as you change lanes, or to the radio, or to your children? Or will you somehow divide your attention between all of these incoming streams of information? What is the optimal thing to do under these circumstances, and can you do it? With the support of an NSF CAREER award, Dr. George Alvarez at Harvard University will examine whether people can optimize the way they divide their attention, and how this affects their ability to see and accurately report information in their field of view. We are limited in our ability to attend to multiple things at once, and given the limits on our attention, there is an optimal strategy for how to pay attention in any given situation. This optimal strategy could involve flexibly distributing attention (e.g., paying 40% to the road signs, 40% to cars in other lanes, and 20% to the children in the backseat), or using clever strategies for combing information (e.g., you can get the gist of whether the chatter in the backseat requires your immediate attention without paying attention to each individual voice). The proposed research will examine whether, and how, these different strategies are employed to optimize the use of our limited attentional resources.<br/><br/>A basic level understanding of how people optimize the use of their limited attentional resources opens the door to studying the extent to which attention deficits in humans are due to sub-optimal use of available resources. Thus, the results of the research will have implications for disorders such as Attention Deficit Disorder, Obsessive Compulsive Disorder, Autism, Schizophrenia, Dementia, or disorders due to stroke or brain damage, such as Visual Neglect or Executive Dysfunction. In addition, Dr. Alvarez will use the research project as an opportunity to mentor a cadre of undergraduate students from underrepresented groups. Dr. Alvarez helps coordinate a summer research program that places students in labs, providing them with hands on training in conducting behavioral research. As a professor of Hispanic background, Dr. Alvarez is in a unique position to increase minority representation and visibility in the brain and cognitive sciences.

Award Number: 0954749
Title: CAREER: Novel approaches to integrating color perception and color memory
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: August 31, 2010
Latest Amendment Date: July 30, 2013
Award Instrument: Continuing grant
Program Manager: Anne Cleary
Start Date: September 01, 2010
Expires: August 31, 2015
Awarded Amount to Date: $487,155
ARRA Amount: $
Investigator(s): Sarah Allred srallred@camden.rutgers.edu (Principal Investigator) 
Organization: Rutgers University Camden
311 N. 5th Street, Camden, NJ 08102-1400, (856)225-2949
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 1045 1187
Program Element Code(s): 7252
Abstract: Visual perception gives us information about objects in the world. For that information to be useful, however, it must be combined with memory. For example, the yellow color of a banana indicates ripeness only if it can be compared to a memory that distinguishes the yellow of ripe bananas from the green of unripe bananas. Everyday experience suggests that although sometimes color memory is very good (e.g. recalling the color of stop signs and bananas), it can also be very poor (e.g. picking out paint to match the walls at home). This everyday experience of color memory reflects a deep division between cognitive and perceptual scientists about the fidelity of color memory. How can these differences be reconciled? NSF-funded research conducted by Dr. Sarah Allred at Rutgers University aims to provide a single framework that unifies color perception and the disparate views of color memory. Human observers will make perceptual and memory judgments about the same colorful stimuli in progressively more complex environments. These data will then drive the development of a computational model that aims to describe color memory as a combination of color perception and independent noise. This view is in contrast to a prevailing account of cultural or linguistic relativity in color memory. <br/><br/>The relationship between perception and memory is important to fields as varied as forensic psychology (e.g. the reliability of eyewitness testimony), military reconnaissance, cognitive psychology (e.g. linguistic effects on memory) and medical diagnosis (e.g. does a dark cloud on an MRI image match memory templates of tumors?). The computational approach here represents an important advance to the study of perception and memory, because it will provide a baseline comparison for future research. This study will allow researchers to quantify how much of what is remembered is predicted by what was perceived. If memory is a predictable function of perception, there are two important implications: first, basic perceptual information will allow more sensible bounds to be placed on the reliability of memory; second, it suggests the possibility that querying memory in the appropriate perceptual environment will increase the reliability of memory. Although color is a relatively simple stimulus with which to probe the relationship between perception and memory, it is a good first approximation for more complicated stimuli like those involved in eyewitness testimony or medical diagnosis. This is because color is simple enough that its physical dimensions are well-characterized, thus making color amenable to computational modeling; in addition, unlike other simple physical stimuli such as oriented gratings, color is linked to higher order variables like emotion and language. Thus color provides a good bridge between simple physical stimuli and more complex stimuli with real-world applicability.

Award Number: 1331089
Title: Collaborative Research: Building a Unified Theory-Driven Methodology for Identification of Elementary Cognitive Systems
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 06, 2013
Latest Amendment Date: September 06, 2013
Award Instrument: Standard Grant
Program Manager: Anne Cleary
Start Date: September 15, 2013
Expires: August 31, 2016
Awarded Amount to Date: $363,973
ARRA Amount: $
Investigator(s): Michael Wenger michael.j.wenger@ou.edu (Principal Investigator) 
Organization: University of Oklahoma Norman Campus
201 David L. Boren Blvd., NORMAN, OK 73019-5300, (405)325-4757
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION EXP PROG TO STIM COMP RES 
Program Reference Code(s): 9150
Program Element Code(s): 7252, 9150
Abstract: Instant by instant, we take in all of the information that we can gather with our senses, and we organize that information into objects and events. This is the foundation for our ability to function in the real world. While we can talk in general terms about how and why we function with such effectiveness and efficiency, a satisfactory scientific explanation requires much more. In particular, it requires a way of relating the things we can measure---specifically, the frequency with which we choose certain to choose certain actions and the time it takes to make those choices---to each other and to our hypotheses for how and why we do these things. Currently, there is no general way of doing this: the work to be accomplished with this support will fundamentally change this state of affairs.<br/><br/>The creation of a unified theoretical approach to characterizing the ways in which we organize our perception of the world will open up numerous avenues of research linking behavior and neurobiology. With the aid of the theoretical language that this project will produce, there will be systematic and well-defined ways of testing competing hypotheses for both what we do with perceptual information and how we do it. Critically, because this language will be extremely general and will not be tied to a particular set of ideas or theory, scientists from competing perspectives will be able to frame their ideas using a common vocabulary, something that is not currently possible. In addition, since this language will be mathematical, it will possess an exceptional level of rigor and internal consistency. Finally, this language will demonstrate its utility and power by being applied to a set of thorny issues in the contemporary study of perceptual organization.

Award Number: 1331047
Title: Collaborative Research: Building a Unified Theory-Driven Methodology for Identification of Elementary Cognitive Systems
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 06, 2013
Latest Amendment Date: September 06, 2013
Award Instrument: Standard Grant
Program Manager: Anne Cleary
Start Date: September 15, 2013
Expires: August 31, 2016
Awarded Amount to Date: $353,292
ARRA Amount: $
Investigator(s): James Townsend jtownsen@ucs.indiana.edu (Principal Investigator) 
Organization: Indiana University
509 E 3RD ST, Bloomington, IN 47401-3654, (812)855-0516
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION ROBUST INTELLIGENCE 
Program Reference Code(s): 
Program Element Code(s): 7252, 7495
Abstract: Instant by instant, we take in all of the information that we can gather with our senses, and we organize that information into objects and events. This is the foundation for our ability to function in the real world. While we can talk in general terms about how and why we function with such effectiveness and efficiency, a satisfactory scientific explanation requires much more. In particular, it requires a way of relating the things we can measure---specifically, the frequency with which we choose certain to choose certain actions and the time it takes to make those choices---to each other and to our hypotheses for how and why we do these things. Currently, there is no general way of doing this: the work to be accomplished with this support will fundamentally change this state of affairs.<br/><br/>The creation of a unified theoretical approach to characterizing the ways in which we organize our perception of the world will open up numerous avenues of research linking behavior and neurobiology. With the aid of the theoretical language that this project will produce, there will be systematic and well-defined ways of testing competing hypotheses for both what we do with perceptual information and how we do it. Critically, because this language will be extremely general and will not be tied to a particular set of ideas or theory, scientists from competing perspectives will be able to frame their ideas using a common vocabulary, something that is not currently possible. In addition, since this language will be mathematical, it will possess an exceptional level of rigor and internal consistency. Finally, this language will demonstrate its utility and power by being applied to a set of thorny issues in the contemporary study of perceptual organization.

Award Number: 1257272
Title: Hierarchical Control of Cognitive Processes
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: February 14, 2013
Latest Amendment Date: February 14, 2013
Award Instrument: Standard Grant
Program Manager: Anne Cleary
Start Date: July 01, 2013
Expires: June 30, 2016
Awarded Amount to Date: $493,914
ARRA Amount: $
Investigator(s): Gordon Logan gordon.logan@vanderbilt.edu (Principal Investigator) 
Organization: Vanderbilt University
Office of Sponsored Programs, NASHVILLE, TN 37240-7830, (615)875-6070
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 
Program Element Code(s): 7252
Abstract: Many studies have shown that novice performers in demanding tasks (e.g., driving, carpentry, typing) are conscious of each step, attending to the details one by one. On the other hand, skilled performers are only conscious of high-level goals, leaving the details to lower-level processes (hierarchical control). These studies leave an important question unanswered: How does this hierarchical control emerge with practice? How do experts develop lower-level control processes that take care of the details? The proposed research focuses on skilled typewriting, where associative learning causes all the keystrokes in a word to be activated in the motor system at the same time, so the motor system must decide when to make each keystroke. Novice typists activate one keystroke at a time, using higher-level processes to control serial order. As typists acquire skill, words are associated with each of their constituent keystrokes, so the presentation of a word activates all of its keystrokes at once, forcing the motor system to control serial order. This creates a lower-level control process that produces keystrokes, which is distinct from the higher-level control process that produces words to be typed, resulting in hierarchical control. The proposed research will test this proposition in four projects in which the investigators will present typists with novel types of keyboards and tasks, and examine learning over many sessions. They will measure the emergence of hierarchical control in terms of parallel activation and the conscious awareness of the details of the task. <br/><br/>The project will have an important impact on basic and applied research. Typing has now become a ubiquitous activity in modern life, and understanding the science behind the skill will have a broad impact on education as our workforce ramps up for the 21st century. The proposed research may also provide new criteria for evaluating successful training in other areas requiring expertise, going beyond simple tests of speed and accuracy to more sophisticated tests for the emergence of hierarchical control. More generally, the projects will promote new connections between research on cognitive control and research on motor control, since both are required in typewriting.

Award Number: 1257096
Title: Disparity and Correspondence in Human Stereo Vision
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: February 12, 2013
Latest Amendment Date: February 12, 2013
Award Instrument: Standard Grant
Program Manager: Anne Cleary
Start Date: March 01, 2013
Expires: February 29, 2016
Awarded Amount to Date: $427,622
ARRA Amount: $
Investigator(s): Bart Farell bfarell@syr.edu (Principal Investigator) 
Organization: Syracuse University
OFFICE OF SPONSORED PROGRAMS, SYRACUSE, NY 13244-1200, (315)443-2807
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 
Program Element Code(s): 7252
Abstract: Due to their different locations, our left and right eyes have slightly different views of the world. It has been known since the 1830's that a horizontal disparity (shift) between the position of an image on the retinas of the left and right eyes is sufficient for humans to see that image in depth. Combining the two retinal images allows us to see stereoscopically the full three-dimensional volume of scenes (and even some movies and comic books). Stereoscopic vision helps people to navigate, to locate, identify and grasp objects, to judge distances, and to drive vehicles. The goal of this research project is to understand how the brain interprets retinal disparities as cues to stereoscopic depth. Horizontal disparities are relative easy to interpret; this is because of the separation of the eyes, when we are in upright posture, is horizontal. However, in naturalistic scenes disparities can have any direction, not just horizontal, and interpreting them is complicated by the variety of spatial arrangements among objects can influence the disparity directions we encounter. Therefore, understanding how humans see depth requires that we understand how the brain analyzes this two-dimensional (horizontal and vertical) disparity signal. The investigator will measure the depth that people perceive as they view displays containing several patterns, each with its own disparity direction. <br/><br/>The knowledge gained from these studies will help scientists understand how humans and other animals combine images from the two eyes in order to recover information not available from either eye alone and how perception of 3-D space can be distorted when this information is combined incorrectly. Learning about the brain's strategies in achieving normal 3-D vision will help explain why visual areas of the brain are organized the way they are and aid in the design of treatments for impaired stereoscopic vision (from, for example, amblyopia and strabismus, or "lazy eye"). It will also help in the design of more effective artificial depth-sensing systems and improve machine-vision algorithms for object recognition, robotics, and visual prosthetic devices.

Award Number: 1330318
Title: Common Principles in Reading and Skill Learning
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 04, 2013
Latest Amendment Date: September 04, 2013
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: September 15, 2013
Expires: August 31, 2016
Awarded Amount to Date: $493,139
ARRA Amount: $
Investigator(s): Richard Hazeltine eliot-hazeltine@uiowa.edu (Principal Investigator) Robert McMurray (Co-Principal Investigator) Gerald Zimmermann (Co-Principal Investigator) Carolyn Brown (Co-Principal Investigator) 
Organization: University of Iowa
2 GILMORE HALL, IOWA CITY, IA 52242-1320, (319)335-2123
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION EXP PROG TO STIM COMP RES 
Program Reference Code(s): 7252 7956 8605 9150
Program Element Code(s): 7252, 9150
Abstract: The research will investigate how factors that affect skill learning can be harnessed to improve reading and reading education. Beginning readers must decode letters on the page into the sounds of language. These decoded sounds can then be linked to meaningful ideas and concepts, allowing children to leverage what they know (spoken language) to learn a new skill (reading). Traditionally, decoding was taught as a process of memorizing a large set of rules that translate letters to sounds (e.g., "A" with a silent "E" makes the "long A," as in LATE). However, while rules are clearly an important educational tool, many psychologists now view learning to read as a process of encoding probabilistic relationships between the written letters and the sounds rather than explicit rules. As a result, learning to read may be more like acquiring a skill, like shooting a basketball, than like memorizing a list of rules. It is therefore potentially significant for the instruction of reading that basic research has uncovered several principles that improve skill acquisition. <br/><br/>The proposed studies test these principles in first-graders learning to read by partnering with a private-sector reading-technology company to develop short-term studies in which students learn a handful of decoding skills. To examine the correspondence between reading and skill acquisition, the investigators have also devised a motor task that captures the same kind of probabilistic input-output relationships that are required for decoding in word reading. Thus, the experiments will be run in pairs, and each pair will include a motor experiment and a reading experiment. Within each pair, the investigators will manipulate the ways in which the tasks are presented and observe whether these manipulations produce similar effects on performance across the two experiments. <br/><br/>The study aims to have a broad impact on our understanding of how children learn to read: First, the experiments will test how best to structure learning experiences to teach decoding skills. The findings will have immediate implications for reading instruction, and the project includes opportunities for outreach to educators and policy makers. Reading is a foundational skill, laying the groundwork for the entire school curriculum. Thus, improvements in children's reading ability have the potential for widespread and lasting impact throughout the educational system and into society in general. Second, because reading is an elaborate and nearly universal behavior, it provides an excellent model task for studying human learning generally in a much more complex domain than is typically studied in the laboratory. Uncovering principles common to reading and motor skill acquisition will have wide-spread implications for understanding how the brain encodes complex behaviors. Third, classroom-based research is expensive and difficult. By developing an analogue of reading in a motor task that can be run efficiently in the laboratory, this project may enable psychologists and educators to understand the fundamental learning principles that underlie reading in a much more cost-effective way, allowing for more targeted interventions in the classroom.

Award Number: 1257098
Title: Perceptual Categorization in Real-World Expertise
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: June 03, 2013
Latest Amendment Date: June 03, 2013
Award Instrument: Standard Grant
Program Manager: Anne Cleary
Start Date: July 01, 2013
Expires: June 30, 2016
Awarded Amount to Date: $400,000
ARRA Amount: $
Investigator(s): Thomas Palmeri thomas.j.palmeri@vanderbilt.edu (Principal Investigator) 
Organization: Vanderbilt University
Office of Sponsored Programs, NASHVILLE, TN 37240-7830, (615)875-6070
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7252 9150
Program Element Code(s): 7252
Abstract: People with perceptual expertise are skilled at making rapid identifications of specialized objects at a glance, often in poor light and camouflage. Forensic experts can accurately match exemplars to latent fingerprints that may be small, distorted, or smudged. Expert radiologists can quickly categorize medical images as normal or cancerous. This project examines perception, categorization, and identification along the continuum from novice to expert performance in two real-world perceptual domains. The overall aim is to understand how fundamental perceptual and cognitive mechanisms are tuned and modified by experience and expertise. The models arising from this project will enable us to understand the development of real-world perceptual expertise and to validate theoretically-grounded measures of expert performance. <br/><br/>Why study perceptual expertise? Just as gifted athletes push the limits of their bodies, or prize-winning mathematicians push the limits of their minds, perceptual experts push the limits of their perceptual systems. Perhaps with better markers of perceptual expertise and a better understanding of how people become perceptual experts, we could identify potential perceptual experts more effectively, train new perceptual experts more efficiently, and evaluate existing perceptual experts more thoroughly. Studying perceptual expertise can also help inform our understanding of the kinds of everyday expertise that we all have, such as recognizing faces or reading words. This can yield new insights into education and workforce training along with new insights into how the ravages of brain damage or disease might lead to perceptual and learning deficits and potentially inform future breakthroughs in evaluation, intervention, or treatment.

Award Number: 0843880
Title: CAREER: A Computational Investigation into Biological Motion Perception
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: July 23, 2009
Latest Amendment Date: July 23, 2009
Award Instrument: Standard Grant
Program Manager: Anne Cleary
Start Date: August 01, 2009
Expires: July 31, 2014
Awarded Amount to Date: $556,647
ARRA Amount: $556,647
Investigator(s): Hongjing Lu hongjing@ucla.edu (Principal Investigator) 
Organization: University of California-Los Angeles
11000 Kinross Avenue, Suite 211, LOS ANGELES, CA 90095-2000, (310)794-0102
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 0000 1045 OTHR 6890
Program Element Code(s): 7252
Abstract: CAREER: A Computational Investigation into Biological Motion Perception<br/>Hongjing Lu, Principal Investigator<br/><br/>This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>In everyday activities ranging from running in a proper trajectory to avoid hitting other pedestrians, recognizing a friend based upon his walking style from a far distance, to practicing boxing with a partner in the gym, action perception plays a critical role in interpreting the intentions of other people and interacting with the world. Despite the importance of action perception, there are major gaps in our understanding of three basic computational issues: (1) how efficiently human observers can process visual information to make action identifications in different contexts; (2) how humans learn and represent action categories, such as walking, boxing, and dancing; and (3) how humans acquire the ability to understand social interactions by bridging perception and reasoning. With the support of an NSF CAREER award, Dr. Hongjing Lu will integrate computer modeling approaches with behavioral experiments to answer the above three questions. This research will develop greater understanding of how visual information is used to achieve apparently effortless recognition of human actions at different processing levels. The work will significantly extend conventional computational approaches in order to render them applicable to the study of visual processes more complex than those to which they have previously been applied. <br/><br/>Understanding the computational basis of action perception is essential to achieving a scientific account of our ability to understand the external world and to conduct social interactions. Furthermore, understanding how the human visual system perceives actions will guide development of artificial vision systems to recognize and interpret complex biological movements. This research will improve a range of applications, including action visualization (e.g., deciding what visual information is important or could be ignored in 3D animation), security surveillance systems (e.g., detection and recognition of suspicious actions in airports or train stations), robotics (e.g., the ability of machines to interact effectively with people), blind assistance system and driver assistance systems (e.g., blind spot detection systems for drivers when backing the car). <br/><br/>In addition, the integration of research and education activities in the project will provide students with training opportunities in interdisciplinary research, encompassing psychology, statistics, computer science and mathematics. A new Ph.D. Major in Computational Cognition will be established in the Psychology Department at UCLA. Mathematical training will be promoted at both the graduate and undergraduate levels in Psychology; at the same time, training in experimental research will be introduced to students with mathematical and computer science backgrounds.

Award Number: 1153786
Title: Towards a unified theory of microsaccadic and saccadic function: determining the significance of microsaccades for perception, cognition, and oculomotor control
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 21, 2012
Latest Amendment Date: September 21, 2012
Award Instrument: Standard Grant
Program Manager: Anne Cleary
Start Date: September 15, 2012
Expires: August 31, 2015
Awarded Amount to Date: $200,760
ARRA Amount: $
Investigator(s): Susana Martinez-Conde smart@neuralcorrelate.com (Principal Investigator) 
Organization: St Joseph's Hospital and Medical Center
185 Berry St, Ste 300, San Francisco, CA 94107-1773, (602)406-6015
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7252
Program Element Code(s): 7252
Abstract: Saccadic eye movements -- the eye movements that shift our gaze during exploration of a visual scene --play multiple roles that are critical to visual perception. They bring high-interest targets to the center of our vision, they search and integrate general information about the environment to stitch together each visual scene, they correct gaze errors, and they are a primary mechanism to direct our attention. When we fixate our gaze between saccades, our eyes nevertheless move constantly, producing so called fixational eye movements. There is disagreement about the impact of fixational eye movements on visibility during fixation, their accuracy in correcting fixation errors, and their relationship with attention and other cognitive processes. Most of these controversies focus on the main roles historically attributed to microsaccades -the largest and fastest fixational eye movement-, such as the prevention and restoration of vision loss during fixation and the correction of fixation errors. The investigators will conduct eye movement experiments and analyses in human observers so as to determine the effects of microsaccades on the visibility of a large range of stimuli and on the correction of oculomotor error during fixation.<br/><br/>We fixate our gaze about 80% of the time during visual exploration, and so understanding the perceptual effects of fixational eye movements is critical to understanding vision. The project aims to widen the scope of microsaccades from having a single or primary function to playing multiple non-exclusive roles in vision, in the same fashion as large saccades do. As such, the investigators will set out to determine whether microsaccades restore the visibility of all faded stimuli -or only that of a narrow range of stimuli- and to elucidate the role of microsaccades in the correction of fixation error. Understanding how microsaccades affect visibility as a function of stimulus characteristics will moreover improve the interpretation of visual studies, most of which are conducted under fixation conditions. Finally, determining that microsaccades re-target fixation after gaze position errors may help engineers to account for these effects in the design of displays and procedures to improve accuracy when operating equipment.

Award Number: 1154313
Title: Processing Orthographic Structure: Associations Between Print and Fingerspelling
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: May 30, 2012
Latest Amendment Date: August 02, 2013
Award Instrument: Continuing grant
Program Manager: William J. Badecker
Start Date: June 01, 2012
Expires: November 30, 2015
Awarded Amount to Date: $507,222
ARRA Amount: $
Investigator(s): Karen Emmorey kemmorey@mail.sdsu.edu (Principal Investigator) Jennifer Petrich (Co-Principal Investigator) 
Organization: San Diego State University Foundation
5250 Campanile Drive, San Diego, CA 92182-2190, (619)594-5731
NSF Directorate: SBE
Program(s): LINGUISTICS PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 1311 7252 7487 7715
Program Element Code(s): 1311, 7252
Abstract: For over a century educators and researchers have been trying to determine how profoundly deaf children learn to read. Deaf children are in the unique position of learning to read and write a language that they do not speak and cannot hear. Unlike people who can hear, though, deaf people experience English orthography in two forms: as printed text and as fingerspelling, in which each alphabetic letter is represented by a distinct hand configuration. This project investigates whether fingerspelling can be used as an additional code for retaining English print in memory and whether printed or fingerspelled words are linked to phonology (speech-based representations). By identifying the similarities and differences between reading print and "reading" fingerspelling, one can identify how fingerspelling might be most effectively used in reading instruction. Another aim of the project is to use data from functional Magnetic Resonance Imaging (fMRI) to identify the brain areas that support the reading of print and fingerspelling for deaf readers. Specifically, the project will a) examine whether ASL signs are activated when deaf adults read English words, b) assess functional connectivity within the reading circuit for deaf signers, and c) identify the effects of deafness, acquisition of a sign language, and/or reading skill on brain anatomy. Finally, a parallel aim of the project is to increase the representation of deaf people in science by including deaf researchers on the project and by providing an accessible environment for deaf students to gain research experience.

Award Number: 1128029
Title: Collaborative Research: Implicit and Explicit Processes of Category-Based Induction
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 07, 2011
Latest Amendment Date: September 07, 2011
Award Instrument: Standard Grant
Program Manager: Anne Cleary
Start Date: September 15, 2011
Expires: August 31, 2014
Awarded Amount to Date: $93,201
ARRA Amount: $
Investigator(s): Brian Ross bross@s.psych.uiuc.edu (Principal Investigator) 
Organization: University of Illinois at Urbana-Champaign
SUITE A, CHAMPAIGN, IL 61820-7473, (217)333-2187
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 
Program Element Code(s): 7252
Abstract: A fundamental component of intelligent behavior is how people make inferences about novel objects, people, and situations. In category-based induction, people use their knowledge of categories to make such inferences about new items from those categories (Will that dog attack? Which treatment will help this patient?). The present research investigates how people make such predictions in a common situation, when the categories are uncertain. For example, if medical treatment is required before a final diagnosis is possible, how do people take account of the various possible diagnoses (categories)? Past research has shown people often make inductions poorly in such situations, focusing on the most likely category and ignoring other possibilities. <br/><br/>The proposed research examines the basis for these errors in explicit, conscious predictions and whether reasoning can be improved by including information from implicit, fast, unconscious predictions. Pilot work suggests that implicit predictions can be more accurate than explicit ones. The proposed research attempts to confirm this finding and explores the psychological mechanisms that underlie the improved reasoning.<br/><br/>This research will have two important contributions. First, it will help in understanding the differences and similarities between conscious and unconscious processes. This distinction is crucial in much research on mind and behavior, and the present research will help us understand why these differences exist. Second, the results may lead to specific proposals for ways to improve predictions in medical, personal, and economic decisions. When categories are uncertain, people often do not make good predictions, but decision aids that help include implicit, unconscious processes may improve performance.

Award Number: 1030831
Title: Decision and Memory: The Effect of Stopping Rules on Memory Use
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 26, 2010
Latest Amendment Date: September 12, 2011
Award Instrument: Continuing grant
Program Manager: Anne Cleary
Start Date: October 01, 2010
Expires: September 30, 2014
Awarded Amount to Date: $324,999
ARRA Amount: $
Investigator(s): Michael Dougherty mdougher@umd.edu (Principal Investigator) J. Isaiah Harbison (Co-Principal Investigator) 
Organization: University of Maryland College Park
3112 LEE BLDG, COLLEGE PARK, MD 20742-5141, (301)405-6269
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): OTHR
Program Element Code(s): 7252
Abstract: Recalling information from memory can be thought of as a search process. Thus, an important aspect of recall is deciding when to stop searching. This is true whether remembering things that are fairly mundane, such as recalling the titles of one's favorite books, to remembering things that are crucial, such as a physician generating potential diagnostic hypotheses. Although the importance of criteria for search termination is recognized in most contemporary models of memory and decision making, very little is known about the processes that underlie these decisions. The research conducted by Dr. Dougherty and colleagues tests the applicability of a cost-benefit framework for describing both memory search and search termination decisions. Within this framework, costs are incurred by the act of searching (which takes time and effort), and gains are realized through successful outcomes. The notion that search termination decisions flow from a system that regulates costs and benefits raises two broad questions. First, in what ways do the costs and benefits of retrieval influence both search termination decisions and the time course of retrieval? Second, what role does monitoring the search process (a form of "metacognition") play? The working hypothesis is that appropriate search termination decisions enable one to maximize the utility of any particular recall attempt and that metacognitive monitoring plays an important role in this process. <br/><br/>Understanding the fundamental nature of search termination decisions has important implications for understanding the basic functioning of human memory and the role of memory in decision making. This is important for a variety of real-world contexts. For example, in time-critical triage situations, medical personnel must quickly retrieve potential diagnostic hypotheses from memory to render a treatment decision. Premature search termination may lead to an incorrect diagnosis whereas protracted search may compromise the care of other time-critical patients. This research may help improve information search efficiency and decision making quality.

Award Number: 1023890
Title: RUI: Guided Imagery And Memory Errors: Identifying Basic Mechanisms by Testing the Effects of Script Author and Imagery Content
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 12, 2010
Latest Amendment Date: June 25, 2012
Award Instrument: Continuing grant
Program Manager: Anne Cleary
Start Date: September 15, 2010
Expires: June 30, 2014
Awarded Amount to Date: $353,065
ARRA Amount: $
Investigator(s): Mary Ann Foley mfoley@skidmore.edu (Principal Investigator) 
Organization: Skidmore College
815 North Broadway, Saratoga Springs, NY 12866-1632, (518)580-5177
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 9229
Program Element Code(s): 7252
Abstract: Research confirms that guided imagery (e.g., asking people to visualize a scene in a park in a particular way) may lead people to report aspects of events that never occurred or to report entire episodes that were only experienced in thought. How does guided imagery result in these memory errors? Is the act of generating an image the culprit? Is it the vividness of the images? Or is it possible that the way in which the images are induced is crucial for producing the deleterious effects on memory? An NSF-funded research project conducted by Dr. Mary Ann Foley at Skidmore College will determine the extent to which guided imagery may result in costs (memory errors) or benefits (improved memory accuracy). The research will attempt to disentangle the complex ways in which guided imagery may affect memory. In a series of experiments, after generating images of complex scenes involving plausible life events, participants will be asked to remember whether details included as test items were explicitly mentioned in the scripts that were used to generate the imagined scenarios. These questions will be investigated by examining script content (e.g., whether or not it is relevant to the participant), and individual differences in imagery experiences (e.g., the frequency with which people experience imagery in their daily lives). The script author will also be varied (e.g., the experimenter or the individual participants), because prior research has routinely relied on experimenter-generated descriptions to guide people's imagery experiences. It may well be that memory errors will be reduced or eliminated when people are their own source of the descriptions guiding the imagery. <br/><br/>This research project has a number of implications. The findings will speak to conflicting patterns in the literature on the effects of imagery on memory, refine theories intended to explain the positive or negative effects of imagery, and invite researchers to conceptualize questions about imagery and memory in new ways. The new knowledge may also inform educational, legal, and forensic practices. This research will contribute to the identification and educational development of the next generation of scientists. Undergraduates will be involved in all stages of the investigations, benefitting from meaningful research activities and enriching their education in the psychological sciences. Special attention will be given to attracting undergraduates early in their careers (e.g., recruiting first-year students on the campus where the research is conducted as well as students at nearby community colleges). A postdoctoral fellow will have a unique opportunity to contribute to both undergraduate research and teaching. Finally, through sponsorship of public lectures and panel discussions on campus, this project will showcase the synergistic benefits of drawing together students, scientists, other professionals, and other members of the broader community.

Award Number: 0957072
Title: Interactions Between Visual Working Memory Representations and Mechanisms of Perceptual Selection
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: July 12, 2010
Latest Amendment Date: April 18, 2012
Award Instrument: Continuing grant
Program Manager: Anne Cleary
Start Date: July 15, 2010
Expires: June 30, 2014
Awarded Amount to Date: $500,000
ARRA Amount: $
Investigator(s): Geoffrey Woodman geoffrey.f.woodman@vanderbilt.edu (Principal Investigator) 
Organization: Vanderbilt University
Office of Sponsored Programs, NASHVILLE, TN 37240-7830, (615)875-6070
NSF Directorate: SBE
Program(s): EXP PROG TO STIM COMP RES PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 9150
Program Element Code(s): 9150, 7252
Abstract: Geoffrey Woodman at Vanderbilt University will examine the long-standing theoretical issue of how working memory and attention interact to determine what subset of information we process from complex visual scenes. A growing number of theories propose that during scene processing attention networks are guided to process task-relevant information through top-down control exerted by template representations in visual working memory. In this way, holding a representation in visual working memory is sufficient to guide perceptual attention mechanisms to select incoming information. In addition, some theories propose that representations of attended items need to pass through visual working memory for them to be categorized. But, the evidence for this direct linkage between visual working memory storage and attentional guidance is mixed, perhaps because the theories do not always consider the additional influence of long-term memory on mechanisms of attention. This project will use behavioral and electrophysiological methods to systematically explore the relative contributions of working memory and long-term memory to the top-down control of attention. By addressing these important questions regarding the interactions between memory systems and perceptual attention, the findings of the experiments in this project, will serve to constrain and shape the next generation of models of attention and control of information processing.<br/><br/>Our complex environment overloads our visual systems with a multitude of objects and surfaces, only some of which are relevant for our task at hand. For example, when driving we need to be attending to the cars that surround us and potential hazards entering the roadway, while the trees and houses lining the road are irrelevant for our task and should not be the focus of attention. Models of attention accounting for such complex processing tasks propose that we optimize information processing for task-relevant objects, like other cars on the roadway, by maintaining a representation of these cars in working memory. A better understanding of how our attention is controlled will allow us to predict when the performance of critical tasks, such as driving or controlling air traffic, will face potentially hazardous processing limitations. In addition, a number of cognitive disorders appear to be due to a compromised ability to control attention. Attention deficit disorder, reading disorders, and schizophrenia all appear to have underlying causes in an impaired ability to selectively process inputs and temporarily remember relevant information. The research proposed in this project will allow us to understand how attention is normally controlled and the methods developed here will provide tools for better understanding the deficits that are due to abnormal attentional control and selective memory storage.

Award Number: 1331293
Title: Mechanisms of Verbal Effects on Human Categorization
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: August 31, 2013
Latest Amendment Date: August 31, 2013
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: September 15, 2013
Expires: August 31, 2016
Awarded Amount to Date: $493,592
ARRA Amount: $
Investigator(s): Gary Lupyan lupyan@wisc.edu (Principal Investigator) 
Organization: University of Wisconsin-Madison
21 North Park Street, MADISON, WI 53715-1218, (608)262-3822
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7252
Program Element Code(s): 7252
Abstract: To what extent is human cognition (e.g., visual perception, memory, and categorization) augmented by language? Does language only allow us to communicate with more flexibility or does language transform cognition, allowing humans to represent and manipulate information in novel ways? A series of studies previously conducted by this investigator has shown that language has pervasive and surprising effects on a range of abilities, such as learning new categories, using knowledge about familiar categories, visual memory, and even perception: Hearing a word can literally change what one sees. The proposed studies build on this earlier work by exploring the design features of language that make it an especially useful tool for constructing and manipulating mental representations. The studies use a variety of behavioral paradigms together with noninvasive brain stimulation to explore the behavioral and neural processes involved in the interaction of language, learning new categories, and retrieving information about familiar concepts and categories. <br/><br/>The proposed studies are critical not only for understanding the broad issue of language-augmented cognition, but also for the potential to understand applied issues related to linguistic and cognitive development. Hearing less language in early childhood leads not only to negative outcomes in using language to communicate, but appears to extend more broadly to other cognitive abilities. By understanding the mechanisms by which language augments cognition, we will better understand the broad consequences of language disorders, which may inform the development of effective interventions. The work will also enhance infrastructure for research and education by implementing and disseminating tools for efficient and cost-effective data collection over the web using crowd-sourcing services, which allow for generalizability to broader populations and will allow undergraduates to more easily conduct original research projects.

Award Number: 1329255
Title: Acoustic Foundations of Speech Perception
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: August 29, 2013
Latest Amendment Date: August 29, 2013
Award Instrument: Standard Grant
Program Manager: Akaysha Tang
Start Date: September 01, 2013
Expires: June 30, 2017
Awarded Amount to Date: $475,958
ARRA Amount: $
Investigator(s): Alyssa Brewer aabrewer@uci.edu (Principal Investigator) Gregory Hickok (Co-Principal Investigator) Kourosh Saberi (Co-Principal Investigator) 
Organization: University of California-Irvine
5171 California Avenue, Ste 150, IRVINE, CA 92697-7600, (949)824-4768
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION COGNEURO 
Program Reference Code(s): 1699 7252 7956 8605 8213
Program Element Code(s): 7252, 1699
Abstract: Our brains have evolved a highly-sophisticated auditory system for the analysis of sounds, which underlies more complicated abilities such as speech perception. We currently know very little about the organization of human auditory cortex at the interface between the auditory inputs from the peripheral sensory receptors in the ear and the higher-level language systems of the brain. Understanding the nature of the inputs to higher-level speech perception systems is critical to understanding what kind of information is ultimately used in speech perception and how this information is extracted computationally. With support from the National Science Foundation, Dr. Alyssa Brewer and colleagues Dr. Gregory Hickok and Dr. Kourosh Saberi will use functional magnetic resonance imaging (fMRI) to measure the functional organization of the human auditory cortex with a level of detail that has not previously been achieved. They will then use these measurements to examine how cortical responses to particular types of speech and speech-related stimuli relate to these lower-level cortical regions. This study will thus provide the first systematic measurements of the human speech perception system from the fundamental organization of auditory cortex to cortical speech representations.<br/><br/> Acquired and developmental disorders of hearing, speech and language affect millions of individuals. The knowledge gained from this study will give us a better understanding of the organization and function of these systems, which will have clinical benefits for the treatment of both peripheral auditory diseases and central language disorders. The research team will continue to share the results through "Brain Day" programs in the local K-8 elementary schools to bring the excitement of neuroscience research to the local communities. Furthermore, PI Brewer has developed ongoing "Brilliant Brain" workshops with Girls Inc., which include special presentations and summer workshops on the organization, function, and diseases of the brain. Girls Inc. is a non-profit organization that provides research and STEM-based experiences to girls ages 6-18 across the U.S. and Canada designed to help them navigate gender, economic, and social barriers. Finally, this study incorporates training of new neuroscientists at the undergraduate, graduate, and postdoctoral levels for whom these studies will serve as a foundation of neuroscience research.

Award Number: 1313889
Title: EAGER: Stability and Fluidity in Children's Learning of Physics Concepts
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: July 20, 2013
Latest Amendment Date: July 20, 2013
Award Instrument: Standard Grant
Program Manager: Laura Namy
Start Date: August 01, 2013
Expires: July 31, 2014
Awarded Amount to Date: $138,292
ARRA Amount: $
Investigator(s): Heidi Kloos heidi.kloos@uc.edu (Principal Investigator) 
Organization: University of Cincinnati Main Campus
University Hall, Suite 530, Cincinnati, OH 45221-0222, (513)556-4358
NSF Directorate: SBE
Program(s): DEVELOP& LEARNING SCIENCES/CRI PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7916 1698 7252
Program Element Code(s): 1698, 7252
Abstract: The long-term goal of the current project is to understand the dynamic process that allows young children to learn about the world. It is well established that children formulate ideas about how features relate to one anther, for example how the heaviness of an object affects its buoyancy. This ability of the mind--to form beliefs--is highly beneficial for multiple cognitive activities, including perception, problem solving, and reasoning. However, the process that underlies the development of beliefs is not well understood, a gap that becomes most apparent when an educator tries to replace a child's mistaken belief: corrective feedback does not always lead to conceptual change. In order to understand the underlying process that gives rise to a child's belief and its change, it is first necessary to quantify belief stability. The current project seeks to establish such a quantifying measure, referred to as ascendency, using ideas from thermodynamics that have been applied successfully to understanding eco-systems. To test the validity of this measure for a cognitive system, preschoolers participate in a learning experiment about sinking objects. Two different learning contexts are contrasted. In one of them, children are likely to merely make guesses from one trial to the next. And in the other learning context, children are likely to form mistaken beliefs at first (e.g., that heavy stuff sinks fastest) and then change such mistaken belief as a result of feedback. This experimental context makes it possible to find transition points of belief stability, transition points that can be tested against changes in the measure of ascendency. <br/><br/>Research in cognitive development and education has long established the struggle children face when they are required to change a mistaken belief, for example a belief about a physical phenomenon. This resistance to conceptual change is often surprising, especially given that children sometimes spontaneously change a belief, even without any intervention at all. Quantifying the stability of a belief would make it possible to better understand such conflicting results. More importantly, it would make it possible to better time efforts to change a mistaken belief. Thus, developing a measure of belief stability could prove transformative to science education, making it possible to test the circumstances in which a child is most likely to engage in conceptual change. While only a first step towards understanding the nature of children's beliefs, the results of this project are likely to set the stage for a systematic understanding of the context in which children best learn about the world.

Award Number: 1056744
Title: CAREER: Integrating Perceptual and Linguistic Information in Models of Semantic Representation
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: April 08, 2011
Latest Amendment Date: August 23, 2013
Award Instrument: Continuing grant
Program Manager: Betty H. Tuller
Start Date: April 15, 2011
Expires: March 31, 2016
Awarded Amount to Date: $363,758
ARRA Amount: $
Investigator(s): Michael Jones jonesmn@indiana.edu (Principal Investigator) 
Organization: Indiana University
509 E 3RD ST, Bloomington, IN 47401-3654, (812)855-0516
NSF Directorate: SBE
Program(s): LINGUISTICS PERCEPTION, ACTION & COGNITION ROBUST INTELLIGENCE 
Program Reference Code(s): 1045 1187
Program Element Code(s): 1311, 7252, 7495
Abstract: Humans learn about the meanings of words and larger discourse units from repeated experience with both linguistic and perceptual information. However, current computational models of semantic learning and representation focus only on linguistic structure. This CAREER award explores how humans use multisensory perception and linguistic experience to organize semantic memory. Dr. Michael Jones at Indiana University will use specially designed web-based experiment protocols to collect large amounts of data about the perceptual structure of word referents. For example, a participant presented with a target word will be required to produce a list of verbal properties to describe the word (e.g., given DOG, a participant might produce "has four legs, has fur, barks" etc.). A second participant is then provided with only the list of properties and is required to guess what word is being described. Alternatively, the information to the second participant can be a drawing of what the target word represents to the first participant, who has configured predefined object components into a 2D or 3D display. By designing these experiments as internet based, Dr. Jones will collect very large amounts of data to design and test computer models of linguistic and perceptual integration during word learning. <br/><br/>The models resulting from this research may provide a better understanding of the learning disorders characterized by language deficiencies (e.g. many forms of autism) and age-related disorders characterized by semantic disorganization (e.g., dementias such as Alzheimer's Disease). The integration of perceptual and linguistic information could also lead to better applied algorithms for information search (e.g., Internet search engines) if the computer representation can be made to approximate the semantic representation of the human doing the searching.

Award Number: 1257101
Title: How motor action shapes emotion in the brain
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: August 05, 2013
Latest Amendment Date: August 23, 2013
Award Instrument: Continuing grant
Program Manager: Betty H. Tuller
Start Date: September 15, 2013
Expires: August 31, 2016
Awarded Amount to Date: $295,244
ARRA Amount: $
Investigator(s): Daniel Casasanto casasand@newschool.edu (Principal Investigator) 
Organization: University of Chicago
5801 South Ellis Avenue, Chicago, IL 60637-5418, (773)702-8669
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION COGNEURO 
Program Reference Code(s): 7252 7956 8605 1699
Program Element Code(s): 7252, 1699
Abstract: How is the diversity of the human experience reflected in the brain/mind? What is universal about the brain and mind? What depends on the particulars of people's physical and social experiences? This project explores how people's interaction with the world structures their neural processing of emotion. It asks whether right- and left-handers, who perform many actions differently, use correspondingly different areas of the brain to process two basic components of human emotion: valence (the intrinsic attractiveness or aversiveness of an event, object, or situation) and motivation. The research team, led by Dr. Daniel Casasanto at the University of Chicago, will use a combination of behavioral tasks, brain imaging, and neural stimulation to test the hypothesis that the way people use their hands to perform approach-related actions to attractive stimuli (like petting a cat, if you like cats) and avoidance-related actions to aversive stimuli (like shooing a bee) may determine how approach- and avoidance-related emotions are represented in the brain. <br/><br/>There are important clinical implications of this basic research. FDA-approved neural therapies are currently in use to treat depression and anxiety disorders. These treatments are based on a one-size-fits-all model of the brain and mind. If the way we use our hands influences how emotions are distributed across the cerebral hemispheres, this may signal that treatments that are beneficial for right-handers may be detrimental for left-handers. Tailoring neural therapies to the specifics of patients' interactions with the world may be crucial for developing safe, effective treatments for common mental health issues.

Award Number: 0748684
Title: CAREER: The Time Course of Bottom-up and Top-down Integration in Language Understanding
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: March 11, 2008
Latest Amendment Date: January 24, 2012
Award Instrument: Continuing grant
Program Manager: Betty H. Tuller
Start Date: March 15, 2008
Expires: August 31, 2014
Awarded Amount to Date: $400,005
ARRA Amount: $
Investigator(s): James Magnuson james.magnuson@uconn.edu (Principal Investigator) 
Organization: University of Connecticut
438 Whitney Road Ext., Storrs, CT 06269-1133, (860)486-3622
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 0000 1045 1187 OTHR
Program Element Code(s): 7252
Abstract: Context changes the way we interpret sights and sounds. A shade of color halfway between yellow and green looks more yellow when applied to a picture of a banana, but more green when applied to a lime. An acoustic pattern halfway between "p" and "b" is interpreted as "p" following "sto-" but as "b" following "sta-". But does context actually alter perception of sights and sounds, or only their interpretation? Cognitive scientists have long debated when and how "bottom-up" input signals (such as speech) are integrated with "top-down" information (context, or knowledge in memory). Do early perceptual processes protect a "correct," context-independent record of signals, or do perceptual processes immediately mix bottom-up and top-down information? One view is that accurate perception requires early separation of bottom-up and top-down information and late integration. An alternative is that early mixing of bottom-up and top-down information would make systems more efficient, by allowing context to immediately guide processing. In studies of language comprehension, this timing question is unsettled because of conflicting evidence from two measures of moment-to-moment processing. Studies tracking people's eye movements on objects upon verbal instructions support immediate integration: helpful information appears to be used as soon as it is available. Studies using ERPs (event related potentials, which measure cortical activity via scalp electrodes) suggest delayed integration: early brain responses appear to be affected only by bottom-up information. Results from the two measures have been difficult to compare because they have relied on very different experimental designs. In the proposed research the investigator will study the timing of top-down integration in human sentence processing using experimental designs that allow simultaneous comparisons of eyetracking and ERPs, with the goal of determining when and how top-down context is integrated with bottom-up signal information. <br/><br/>The proposed work has important implications for the design of language technology. In contrast to computer systems, humans efficiently exploit top-down context, and quickly learn to adapt to new contexts. An obstacle to making computer systems as adaptable as humans is that we do not fully understand how humans balance bottom-up signals and top-down context. The proposed research also has implications for understanding and treating language impairments. For example, understanding how normal perceivers balance and integrate signal and context may help identify subtle bottom-up impairments that lead to unusual reliance on context. The investigator is committed to integrating research and training activities in this CAREER project, and will actively involve undergraduate and graduate students in the research. The investigator will also develop courses designed to prepare students for independent research by providing hands-on training in cognitive theories and time course methodologies.

Award Number: 1152916
Title: Collaborative Research: Sensory Integration and Sensorimotor Transformations for Dexterous Manipulation
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: February 13, 2012
Latest Amendment Date: August 02, 2013
Award Instrument: Continuing grant
Program Manager: Betty H. Tuller
Start Date: March 15, 2012
Expires: February 28, 2015
Awarded Amount to Date: $327,061
ARRA Amount: $
Investigator(s): Andrew Gordon ag275@columbia.edu (Principal Investigator) 
Organization: Teachers College, Columbia University
525 West 120th Street, New York, NY 10027-6625, (212)678-3000
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7252 7298
Program Element Code(s): 7252
Abstract: The ability to grasp and manipulate objects is an extremely complex motor behavior. When grasping an object such as a cup, people often vary their finger placements and the forces exerted by the fingers in order to ensure that the goal is attained (in this case, lifting the cup without spilling its contents). How humans do this so efficiently is not known, in part because previous research constrained subjects to place their fingers at pre-specified locations on the objects. This has led to a major gap in our understanding of how these complex motor behaviors are learned, planned, and executed. To address this gap, the proposed studies will investigate how subjects grasp and manipulate objects in tasks that allow them to choose their finger placements and applied forces and, importantly, to adjust the interaction between the two. Another function of grasping is to develop a representation of an object's weight and the distribution of weight within the object. How subjects develop this representation and refine their actions will be examined by studying the interaction between information extracted by grasping and memories of past manipulations of the object. The hypothesis that fingertip placement and forces are learned independently from each other will be tested by manipulating visual feedback of fingertip placement, varying visual shape and density cues, and through object rotation tasks that create a discrepancy between visual cues and memories of the same object from prior manipulations.<br/><br/>The proposed studies represent a major paradigm shift in the research on grasping by opening new and fundamental questions about how the brain learns to control the hand. Thus, the results of this work may inform the design of more dexterous robotic manipulators, brain-machine interfaces, and neuroprosthetic hands.

Award Number: 1152898
Title: Analysis of causal cognition in rats
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: April 30, 2012
Latest Amendment Date: July 30, 2013
Award Instrument: Continuing grant
Program Manager: Betty H. Tuller
Start Date: June 01, 2012
Expires: May 31, 2015
Awarded Amount to Date: $499,996
ARRA Amount: $
Investigator(s): Aaron Blaisdell blaisdell@psych.ucla.edu (Principal Investigator) 
Organization: University of California-Los Angeles
11000 Kinross Avenue, Suite 211, LOS ANGELES, CA 90095-2000, (310)794-0102
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7252 1045
Program Element Code(s): 7252
Abstract: Humans learn about their world through both personal experience and social communication but complete information is rarely available. In fact, we are remarkably adept at drawing conclusions from only partial information (for example, a doctor diagnosing a disease or a detective solving a crime) and imagining alternate scenarios (an especially useful enterprise for scientists). Like humans, many animals are able to learn about the world through personal experience, as Pavlov showed in dogs and Skinner showed in rats and pigeons. In previous NSF-funded research, the principal investigator showed that rats have the capacity to go beyond direct experience and training and make causal inferences, that is, inferences about a cause-effect relationship they had never previously observed. Causal inferences were much like those of a scientist or a human child in that they originated from the rat's interactions with the world. Rats also act as if they expect a cause to be present even if it is not directly observable. Thus, rats exhibit at least some of the hallmarks of human reasoning. The current research project builds on these results by exploring other ways in which animals are able to reason about their world. Can a rat, like a doctor or detective, determine a cause-effect relationship from only partial and incomplete information? If so, what are the psychological mechanisms that support these causal inferences? To what extent can a rat reason about hidden events in order to solve ambiguous problems? <br/><br/>Answers to these questions can determine aspects of cognition that are uniquely human and aspects that humans share with other animals. Such knowledge may impact the development of artificial intelligence systems designed to make adaptive decisions and inferences based on limited prior knowledge. The principal investigator also serves as faculty mentor in two programs for high-school students in the greater Los Angeles area (drawn from diverse backgrounds) and will involve the students in this research.

Award Number: 1152841
Title: Color and language in Somali and US observers
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: February 13, 2012
Latest Amendment Date: July 30, 2013
Award Instrument: Continuing grant
Program Manager: Betty H. Tuller
Start Date: March 01, 2012
Expires: February 28, 2015
Awarded Amount to Date: $450,001
ARRA Amount: $
Investigator(s): Delwin Lindsey lindsey.43@osu.edu (Principal Investigator) Angela Brown (Co-Principal Investigator) 
Organization: Ohio State University
Office of Sponsored Programs, Columbus, OH 43210-1016, (614)292-3805
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7252
Program Element Code(s): 7252
Abstract: This research aims to understand the interaction of language and culture on the perception of color. The Standard Model of color vision asserts that the visual system of all humans with normal color vision is organized around six fundamental sensations in a color-opponent fashion: red vs. green, blue vs. yellow, and black vs. white. Yet most of the world languages do not have a specific term for the colors that English speakers call "blue." Some speakers use a word meaning black, others use the term gray to mean blue, and still others use a single word that means green-or-blue. In what sense, then, is the mental representation of color in these individuals "color-opponent" if identical names are given to two sensations that fall on two presumptively independent color dimensions? How do the language differences affect how people think about color? To answer this, color naming studies will examine in detail the structures of the named color categories in monolingual American English speakers and a group of recent immigrants from Somalia who now live in Columbus, OH. The Somalis, who do not yet speak English, do not use a specific word for blue. Experiments based on color sorting and similarity, which do not require a color name response, will examine the possibility that classical color-opponency is observable in their non-language representation of color. Control studies of spectral sensitivity and discrimination will test for the possible interplay between cultural and sensory factors (including congenital and acquired impairments in color vision) in shaping an individual's mental representation of color. <br/><br/>This research will contribute to understanding the foundational processes by which innate and cultural factors shape an individual's understanding of their world. In addition, it will bring the Somalis, who came to the US as refugees from political upheaval, into a research environment in a university and will give students a better understanding of people from a vastly different culture.

Award Number: 0847653
Title: CAREER: The Role of Prosody in Word Segmentation and Lexical Access
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: August 11, 2009
Latest Amendment Date: April 17, 2012
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: August 15, 2009
Expires: July 31, 2014
Awarded Amount to Date: $464,974
ARRA Amount: $464,974
Investigator(s): Laura Dilley dilleyl@msu.edu (Principal Investigator) 
Organization: Michigan State University
CONTRACT AND GRANT ADMINISTRATIO, EAST LANSING, MI 48824-1046, (517)355-5040
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 0000 1045 1187 6890 OTHR
Program Element Code(s): 7252
Abstract: This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>Understanding how humans comprehend speech is an unsolved and challenging problem, in part because of the many-to-many mapping between the acoustical properties of the speech signal (i.e., frequency, timing, and amplitude) and the words perceived by the listener. The focus of this research is on the contributions of voice pitch, loudness, and speech rate (collectively termed prosody) to understanding spoken words. Previously, these prosodic aspects of the speech signal have been assumed to play a minor role in spoken word recognition, word segmentation, and lexical access. However, recent results suggest that speech prosody can have very significant effects on how words are understood.<br/><br/>This research holds potential for significant advancements in human health, technology, and science. For example, perception or production of voice pitch, loudness, and/or speech timing are often highly disrupted in many disorders affecting speech and language, including dyslexia, autism, stuttering, Parkinsons disease, aphasia, and dysarthria. The proposed research may lead to new insights regarding mechanisms underlying these disorders, which will inform the development of better treatments for those afflicted. In addition, the research has potential to lead to improved speech technology applications, from enhanced automatic speech recognition by computer, to more natural-sounding computer-generated speech. The work provides research experiences for students at the high school, undergraduate and graduate levels.

Award Number: 1029082
Title: The Emergence of Cognitive Flexibility in Neural-Behavioral Systems
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 21, 2010
Latest Amendment Date: August 20, 2012
Award Instrument: Continuing grant
Program Manager: Betty H. Tuller
Start Date: October 01, 2010
Expires: September 30, 2014
Awarded Amount to Date: $1,100,767
ARRA Amount: $
Investigator(s): John Spencer john-spencer@uiowa.edu (Principal Investigator) Gregor Schoner (Co-Principal Investigator) Rodica Curtu (Co-Principal Investigator) Vincent Magnotta (Co-Principal Investigator) Richard Hazeltine (Co-Principal Investigator) 
Organization: University of Iowa
2 GILMORE HALL, IOWA CITY, IA 52242-1320, (319)335-2123
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION DEVELOP& LEARNING SCIENCES/CRI EXP PROG TO STIM COMP RES 
Program Reference Code(s): 7956 7969
Program Element Code(s): 7252, 1698, 9150
Abstract: The goal of this project is to understand how the brain realizes the impressive flexibility that is a hallmark of human cognition. For example, adults can flexibly shift from conversing about current events, to feeding the cats, to cooking dinner, all without getting mixed up and cooking cat food as the main course. This ability is thought to rely on the actions and interactions of multiple brain areas. A central challenge in understanding cognitive flexibility is to understand how these brain networks change with learning and how they organize and re-organize "on-the-fly" depending on the situation. More generally, how does the brain keep track of events as they unfold but at the same time retain flexibility?<br/><br/>In this project, a multidisciplinary team of investigators will test a new theory of cognitive flexibility using computer modeling and functional neuroimaging data. The theory is implemented using dynamic neural fields -a specific type of neural network that can be simulated on a computer and that specifies how different areas of the cortex interact during complex tasks. Some cortical areas actively maintain neural patterns related to the lower-level details of what is happening, for instance, maintaining information about critical visual features of the pan, the items you are cooking for dinner, and so on. Other cortical areas modulate what these systems are up to. Critically, "higher-level" systems do not need to understand all the details of what is going on. They just need to help decide which general patterns of information are important in the context. It is the dialog between these different neural patterns that makes cognition flexible.<br/><br/>The success of this project would have far-reaching effects. The ability to modulate behavior in context-specific ways is a central achievement that impacts language skills, mathematical abilities, school and work performance, and IQ. Moreover, deficits in cognitive flexibility and so-called executive functions underlie different forms of psychopathology, such as schizophrenia, as well as many of the challenges faced by aging adults. The investigators have also established a collaboration with the Iowa Children's Museum where they will work with museum staff to design an interactive display around the theme of "Brain Play," in which children construct simple solar-powered robots that embody how the brain and body realize flexibility.

Award Number: 1219400
Title: The Recovery of Introspection: An ethnography and history of experimental psychology
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: July 30, 2012
Latest Amendment Date: July 01, 2013
Award Instrument: Standard Grant
Program Manager: Jeffrey Mantz
Start Date: September 01, 2012
Expires: August 31, 2014
Awarded Amount to Date: $60,779
ARRA Amount: $
Investigator(s): Emily Martin em81@nyu.edu (Principal Investigator) 
Organization: New York University
70 WASHINGTON SQUARE S, NEW YORK, NY 10012-1019, (212)998-2121
NSF Directorate: SBE
Program(s): CULTURAL ANTHROPOLOGY SCIENCE, TECH & SOCIETY SOCIAL PSYCHOLOGY PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 1332 1390 7252 7603 7554 9179 
Program Element Code(s): 1390, 7603, 1332, 7252
Abstract: Dr. Emily Martin of New York University will conduct research studying how three innovative experimental psychology labs are changing how they study human behavior. In most experimental psychology labs, elicitations of subject's introspections were abandoned after psychology and anthropology split apart at the turn of the 20th century. Recently, however, some labs have begun to use introspections to understand behavior. Dr. Martin will investigate how these psychologists are attempting to do so, and will thereby contribute to the anthropology of science, the anthropology and psychology of subjectivity, and science and technology studies. <br/><br/>The project will employ a range of social science research methods in three laboratory settings in different parts of the U.S., including participant observation, interviews, and close attention to what counts as data for the psychologists, as well as an ongoing survey of media coverage of this science. The researcher will follow experiments in each of the three labs, carefully chronicling how topics such as rumination, insight, attention, or choice develop. <br/><br/>Through the general interest magazine, Anthropology Now, and through a series of public lectures, Dr. Martin will produce and disseminate new knowledge about the process of research in these innovative labs. This anthropological analysis will help give the public a more accurate understanding of how science advances. The research was co-funded by the Cultural Anthropology Program, the Science, Technology, and Society Program, the Social Psychology Program, and the Perception, Action, and Cognition Program.

Award Number: 1028970
Title: RUI Noise, Delays and Development of Expertise
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: August 08, 2010
Latest Amendment Date: August 08, 2010
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: August 15, 2010
Expires: July 31, 2014
Awarded Amount to Date: $335,198
ARRA Amount: $
Investigator(s): John Milton jmilton@jsd.claremont.edu (Principal Investigator) 
Organization: Claremont McKenna College
500 E. Ninth St., Claremont, CA 91711-5929, (909)621-8117
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 9229
Program Element Code(s): 7252
Abstract: How does expertise develop in motor skills? Through research conducted on a challenging task--balancing a stick on the fingertip--some of the secrets of motor expertise have been discovered. Counter-intuitively, random perturbations may be a key component of the solution. Research conducted by John Milton and his students at Claremont McKenna College (a primarily undergraduate institution) suggests that expertise depends on an underlying "drift and act" control strategy, in which unconscious corrective actions are made only when deviations become sufficiently large. Voluntary corrective movements are more likely to be disruptive than effective. The investigators will extend this work to balancing on a wobble board in order to establish the fundamental importance of intermittent control for maintaining balance in an unpredictable, noisy environment.<br/><br/>With support of the National Science Foundation, Dr. Milton brings together an international team of scientists to work with undergraduate students to advance our understanding of expertise development in balancing. The investigators focus on balancing tasks for three main reasons: 1) the tasks are sufficiently difficult to allow identification of levels of expertise; 2) they are sufficiently constrained to permit careful comparisons between observation and prediction; and 3) expertise can be dramatically increased with just a few days of intensive practice. High speed motion capture technologies are used to simultaneously monitor the positions of the stick, body and eye movements (a measure of visual attention) as expertise develops and key elements are varied. <br/><br/>The research project is designed to excite and motivate undergraduate students, while at the same time teach them to work effectively in international teams. Because proper balance control is essential for the expert performance of many motor tasks, it is anticipated that studies of balance control will provide potential solutions for problems ranging from minimizing the risk of falling in the elderly to the development of two-legged robots and novel rehabilitative strategies.

Award Number: 1330937
Title: Workshop: How the Brain Accommodates Variability in Linguistic Representations; July, 2013 - University of Michigan
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: June 19, 2013
Latest Amendment Date: June 19, 2013
Award Instrument: Standard Grant
Program Manager: William J. Badecker
Start Date: July 01, 2013
Expires: December 31, 2014
Awarded Amount to Date: $11,903
ARRA Amount: $
Investigator(s): T. Florian Jaeger fjaeger@bcs.rochester.edu (Principal Investigator) Victor Ferreira (Co-Principal Investigator) 
Organization: University of Rochester
518 HYLAN, RIVER CAMPUSBOX 27014, ROCHESTER, NY 14627-0140, (585)275-4031
NSF Directorate: SBE
Program(s): LINGUISTICS PERCEPTION, ACTION & COGNITION ROBUST INTELLIGENCE 
Program Reference Code(s): 1311 7556 7252 7495
Program Element Code(s): 1311, 7252, 7495
Abstract: When we listen, we rapidly and reliably decode speakers' intentions and we mostly do so independently of whom were are talking to. Yet, anyone who has interacted with an automated speech recognition system (e.g., while booking a flight) is painfully aware that speech recognition is a computationally hard problem: although we hardly ever become aware of it, the physical signal corresponding to, for example, one speaker's "b" can be identical to another speaker's "p", making it hard for computers to distinguish between them. How then does the human brain accomplish this task with such apparent ease? <br/><br/>This NSF funded workshop brings together researchers from computer sciences, linguistics, and the cognitive sciences to discuss and investigate how the brain achieves robust language understanding despite variability. The invited speakers are internationally-known experts. Representatives from both industry and academia will present on the state of the art in automated speech recognition, implicit learning during language understanding, and the neural systems underlying speech perception. The workshop will take place in conjunction with the 2013 Linguistic Society of America's Summer Institute--the largest international linguistics summer school--and will thereby provide training to a large number of young language researchers.

Award Number: 1340784
Title: Women in Cognitive Science: Professional development and building networks
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: May 22, 2013
Latest Amendment Date: May 22, 2013
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: July 01, 2013
Expires: June 30, 2014
Awarded Amount to Date: $18,950
ARRA Amount: $
Investigator(s): Janet van Hell jgv3@psu.edu (Principal Investigator) 
Organization: Pennsylvania State Univ University Park
110 Technology Center Building, UNIVERSITY PARK, PA 16802-7000, (814)865-1372
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7252
Program Element Code(s): 7252
Abstract: Four workshops are designed for women in cognitive science, especially women in the early stages of their academic career. The focus will be on the "imposter phenomenon," the internal emotional experience of not being qualified, both intellectually and professionally, despite objective evidence indicating the opposite. Research suggests that this phenomenon is widespread but is experienced more often by women than by men. Its well-documented aversive effects can be a significant psychological barrier to seeking a career in science. The workshops focus on negotiation techniques to create opportunities and optimize mechanisms to sustain research productivity. A second focus is on predoctoral, postdoctoral, and early career grant application writing. The workshops are to be held in conjunction with four major scientific meetings in Psychology, across a broad geographical area (the Cognitive Science Society, the Psychonomic Society, the Association for Psychological Science, and the Society for Mathematical Psychology). The workshops will take the form of a public forum with invited speaker-panelists to initiate discussion about best practices for the professional advancement of women in cognitive science at the individual and institutional level. By partnering with these established societies, the workshops will maximize the outreach potential to a group that continues to be underrepresented in senior academic positions in the cognitive sciences.

Award Number: 1031903
Title: Levy Distributions in Foraging Games, Scene Perception, and Semantic Memory
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 10, 2010
Latest Amendment Date: September 10, 2010
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: September 15, 2010
Expires: August 31, 2014
Awarded Amount to Date: $316,968
ARRA Amount: $
Investigator(s): Christopher Kello ckello@ucmerced.edu (Principal Investigator) Michael Spivey (Co-Principal Investigator) Teenie Matlock (Co-Principal Investigator) 
Organization: University of California - Merced
5200 North Lake Road, Merced, CA 95343-5001, (209)228-4318
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION ROBUST INTELLIGENCE 
Program Reference Code(s): OTHR
Program Element Code(s): 7252, 7495
Abstract: All mobile organisms spend much of their lives searching for something, be it food, water, shelter, or others of their kind. Searching uses perception, memory, cognition, and action. It can occur over vast landscapes and periods of time, as in whale migration behaviors. It can occur in under a second on the basis of fleeting bits of sensory evidence, as in frogs searching their visual fields for flies. And it occurs on all scales in between, across organisms but also across different behaviors of a given organism. Human search is especially diverse in this regard because human searches range from subatomic to human to cosmic scales of exploration.<br/><br/>With support from the National Science Foundation, Dr. Christopher Kello is leading a team of researchers at the University of California, Merced, in the study of human search behaviors in three very different but complementary domains: Foraging over large virtual spaces, visual searches over scenes and movies, and memory searches over words and concepts. The generality and evolutionary importance of search suggests that search functions may share common principles across scales and domains. Evidence for this conjecture has been found in "Levy distributions" that describe the frequencies with which segments of different lengths occur in search paths. These frequencies are observed to obey a common scaling law across a wide range of different search behaviors but current evidence is not sufficient to determine the meaning of this law.<br/><br/>With mentorship from three faculty members in Cognitive and Information Sciences, undergraduate and graduate students and a postdoctoral fellow will collaborate on experiments and computational models in each of the three domains of interest, in a unique and highly interdisciplinary education and research experience.

Award Number: 1256964
Title: Internal Models and Vocal Imitation
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: February 11, 2013
Latest Amendment Date: February 11, 2013
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: March 01, 2013
Expires: February 29, 2016
Awarded Amount to Date: $361,488
ARRA Amount: $
Investigator(s): Peter Pfordresher pqp@buffalo.edu (Principal Investigator) Andrea Halpern (Co-Principal Investigator) 
Organization: SUNY at Buffalo
402 Crofts Hall, Buffalo, NY 14260-0000, (716)645-2634
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 
Program Element Code(s): 7252
Abstract: The ability to imitate is crucial for human development, for transferring learned abilities from one generation to another, and perhaps even for understanding the intentions and actions of others. Interest in the processes of imitation has a long history but most of the research has focused on the imitation of visually observable actions. This research program focuses instead on vocal imitation, which emerges spontaneously in infants and plays an important communicative role in language acquisition, singing, and other expressive vocalizations. The studies combine behavioral and physiological measures of vocal imitation with a computational model in order to evaluate whether deficits in vocal imitation reflect a failure of auditory imagery or of the complex transformation from auditory perception to coordinated motor action.<br/><br/>A potential application of this research is the development of interventions for individuals who have difficulty with vocal imitation, including pedagogical techniques to train "tone deaf" singers. A second potential application is to the learning of tonal languages such as Chinese, for which pitch information is critical.

Award Number: 1229033
Title: RUI: Phonetic Convergence in Spoken Communication
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: July 26, 2012
Latest Amendment Date: July 26, 2012
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: August 01, 2012
Expires: July 31, 2015
Awarded Amount to Date: $400,098
ARRA Amount: $
Investigator(s): Jennifer Pardo pardoj@mail.montclair.edu (Principal Investigator) 
Organization: Montclair State University
1 Normal Avenue, Montclair, NJ 07043-1624, (973)655-6923
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7252 9229
Program Element Code(s): 7252
Abstract: Spoken communication works so well that its efficacy is taken for granted. Two complete strangers who meet for the first time and speak the same language can converse with relative ease from the moment of introduction. How is this feat accomplished? The fact that talkers share a language with similar vocabulary and grammatical rules is only part of the answer. Because of the enormous variability in phonetic forms used by speakers of the same language, how an individual speaks is as important as what he or she says. When talkers converse, they often adopt some of the phonetic attributes of their conversational partners, a process termed "phonetic convergence." On other occasions, talkers diverge from each other or show little change in their phonetic repertoire. Moreover, some talkers are more adept than others at shedding an accent or adopting the accent of a foreign language. The current project aims to further our understanding of these interactive effects by investigating the relationship between individual perceptual and learning abilities and acoustic-phonetic variability across both nonsocial and conversational settings. Delineating how individual speakers adjust to each other is crucial to understanding communication in interactive, multi-talker settings.<br/><br/>This project has the potential for broad impact across many domains that rely on social interaction of talkers, such as learning a second language, remediation of speech pathologies, and human-computer interaction through the medium of speech. The award is for Research in Undergraduate Institutions and incorporates undergraduate student participation at all levels of the work, from design and data collection to reporting and presentation of results. The recordings and transcription data will also be made available online to other qualified researchers interested in studying the phenomena of conversational interaction via the Linguistics Data Consortium.

Award Number: 1255922
Title: Collaborative Research: Multifrequency Coordination in Dyads and Teams
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: March 22, 2013
Latest Amendment Date: March 22, 2013
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: April 01, 2013
Expires: March 31, 2016
Awarded Amount to Date: $255,997
ARRA Amount: $
Investigator(s): Polemnia Amazeen nia@asu.edu (Principal Investigator) 
Organization: Arizona State University
ORSPA, TEMPE, AZ 85281-6011, (480)965-5479
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7252 7956
Program Element Code(s): 7252
Abstract: People coordinate their actions with one another in many different contexts and in many different ways: Drivers in traffic manage to merge without accident; people walking on crowded streets usually avoid mishap; musicians coordinate a variety of nested rhythms produced by band members. These examples highlight the human capacity to coordinate behavior and exchange information at different rates to accomplish an overall goal, a phenomenon called interperson, multifrequency coordination. The researchers will study interperson, multifrequency coordination by applying the physics of coupled, oscillator systems to the perceptual, cognitive, and social information inherent in this type of coordination. Pairs of participants (dyads) will coordinate their movements while listening to different metronome frequencies through headphones. Both performance pattern and pattern stability will be measured under different experimental conditions in order to test fundamental predictions of the mathematical model. Experimental manipulations specifically designed to increase or decrease coupling between dyads' movements will be tested in the context of perceptual (e.g., visual), cognitive (e.g., counting), and social (e.g., partner familiarity) coupling mechanisms. Field studies are also planned to discover how people coordinate in naturalistic situations. This sequence of studies is designed to advance basic research on multifrequency coordination in dyads and to extend that research to spontaneous coordination in larger teams.<br/><br/>One important aspect of this project is the extension of experimentally induced multifrequency coordination in the laboratory to real-world settings in which spontaneous multifrequency coordination occurs across teams of individuals. The project also has a significant outreach component, including a visit to a nationally organized competitive youth summer camp, where investigators will collect data on spontaneous interperson, multifreqency coordination and teach campers about coordination science and STEM fields.

Award Number: 1246920
Title: INSPIRE: Asynchronous communication, self-organization, and differentiation in human and insect networks
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: July 17, 2012
Latest Amendment Date: July 17, 2012
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: September 01, 2012
Expires: August 31, 2015
Awarded Amount to Date: $999,850
ARRA Amount: $
Investigator(s): Harry Dankowicz danko@illinois.edu (Principal Investigator) Gene Robinson (Co-Principal Investigator) Whitney Tabor (Co-Principal Investigator) 
Organization: University of Illinois at Urbana-Champaign
SUITE A, CHAMPAIGN, IL 61820-7473, (217)333-2187
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION CROSS-EF ACTIVITIES DYNAMICAL SYSTEMS ENG INTERDISC RES (IDR) INSPIRE 
Program Reference Code(s): 8653
Program Element Code(s): 7252, 7275, 7478, 7951, 8078
Abstract: This INSPIRE award is partially funded by the Perception, Action, and Cognition Program in the Division of Behavioral and Cognitive Sciences in the Directorate for Social, Behavioral and Economic Sciences, the Animal Behavior Program in the Division of Integrative Organismal Systems in the Directorate for Biology, and the Dynamical Systems Program in the Division of Civil, Mechanical & Manufacturing Innovation in the Directorate for Engineering.<br/><br/>The project explores the question of how the activities of individuals become integrated into a smoothly functioning society: What are the dominant mechanisms? How resilient are they? How do they depend on the properties of individual society members? To this end, investigators from engineering, biology, psychology and linguistics will work together to study bee colonies and groups of humans to understand how organization and coordination emerges from these multi-agent systems and the factors that influence their robustness and resilience to perturbations. The project relies on quantitative observations of the dynamic emergence of patterns of interaction and coordination using an unprecedented, 24/7 monitoring system of a beehive as well as in groups of humans under controlled conditions designed to distinguish between failed and successful coordination. The investigators will pursue a combined theoretical, experimental, and computational framework for characterizing the resultant parallel and asynchronous communication systems. The work depends crucially on the interdisciplinary framework and the direct involvement of content expertise from the disciplines represented by the investigators. For example, the human transportation network is designed to resemble the coordinated delivery of nectar through a beehive, but with options for varying the number of different materials transported, the size of arena, the flow rates of the materials, and so on.<br/><br/>The investigators are exploring whether a comprehensive computational framework can be discovered to understand, predict and prevent the collapse of very different types of communities (bees and human networks). The research results are expected to provide insight into how to manipulate the behavior of a complex system, for example to address societal challenges associated with the collapse of pollinating bee colonies or the destructive behavior that is often associated with phases of social transition in groups of humans.

Award Number: 1232676
Title: Collaborative Research: Emotional Sophistication - Studies of Facial Expressions in Decision Making
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: August 27, 2012
Latest Amendment Date: August 27, 2012
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: September 01, 2012
Expires: August 31, 2015
Awarded Amount to Date: $332,962
ARRA Amount: $
Investigator(s): Marian Bartlett mbartlett@ucsd.edu (Principal Investigator) 
Organization: University of California-San Diego
Office of Contract & Grant Admin, La Jolla, CA 92093-0934, (858)534-4896
NSF Directorate: SBE
Program(s): ROBUST INTELLIGENCE DECISION RISK & MANAGEMENT SCI PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 6867 7298
Program Element Code(s): 7495, 1321, 7252
Abstract: Social and economic decisions cannot be fully explained by "rational" attempts to maximize monetary gain, even in very simple game-theoretic scenarios. Complex emotional processes such as anger, guilt or generosity act as hidden forces that lead to observable actions. Such "non-rational" motivations can drive our own decisions and they affect our beliefs about what motivates others' decisions as well. The goal of this project is to use automatic measurements of dynamic facial expressions, in combination with other measurements such as functional MRI (fMRI) and eye-tracking, to investigate the role of non-rational motivations in social decision making. The core of the approach is to use state-of-the-art computer vision techniques to extract facial actions from video in real-time while participants interact with a computer or with each other, in some cases viewing live video of each others' faces. The investigators will use powerful statistical machine learning techniques to make inferences about the participants' internal emotional states during the interactions. The goal is to use the inferences concerning emotional state (a) to predict participants' behavior; (b) to explain why a decision is made in terms of the hidden forces driving it; and (c) to build autonomous agents that can use this information to drive their interactions with humans. <br/><br/>This multidisciplinary project contributes to several fields such as psychology, neuroscience, and economics. First, it develops new methodologies to study decision processes. Second, it uses these methods to test hypotheses about social decision-making and to bridge the gap between observable actions and the internal states that generated them. Third, the investigators intend to make available a dataset and toolset that should be an extremely useful for other investigators analyzing facial expression in multiple contexts. Additionally, automatic and on-line decoding of internal motivational states lays the groundwork for "affectively-aware" interactive computers, or artificial systems that can make inferences about the emotions and intentions of their users. Through the development of these systems, this project will make a significant contribution to the growing field of human-machine interaction.<br/><br/>[Supported by Perception, Action and Cognition, Decision, Risk and Management Sciences, and Robust Intelligence]

Award Number: 1246750
Title: INSPIRE: Dynamical Principles of Animal Movement
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: July 17, 2012
Latest Amendment Date: July 17, 2012
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: September 15, 2012
Expires: August 31, 2015
Awarded Amount to Date: $973,963
ARRA Amount: $
Investigator(s): Khalil Iskarous kiskarou@usc.edu (Principal Investigator) Andrew Gracey (Co-Principal Investigator) 
Organization: University of Southern California
University Park, Los Angeles, CA 90089-0701, (213)740-7762
NSF Directorate: SBE
Program(s): LINGUISTICS PERCEPTION, ACTION & COGNITION CROSS-EF ACTIVITIES INSPIRE 
Program Reference Code(s): 8653
Program Element Code(s): 1311, 7252, 7275, 8078
Abstract: This INSPIRE award is partially funded by the Perception, Action, and Cognition Program and the Linguistics Program in the Division of Behavioral and Cognitive Sciences in the Directorate for Social, Behavioral and Economic Sciences, and the Animal Behavior Program in the Division of Integrative Organismal Systems in the Directorate for Biology.<br/><br/>There is enormous diversity in the ways that animals move, from crawling and swimming to courtship displays and talking. Of special interest is whether the context of the movement, such as whether or not it performs a communicative function, fundamentally changes the coordination principles. The proposed research brings together an interdisciplinary team of biologists, linguists, and psychologists to investigate the movements of a diverse set of animals: human, octopus, and a nematode worm. The work allows, for the first time, a comparison of communicative and non-communicative actions both within and between animal species. The cross-species comparisons also allow an exploration of how symbolic and discrete linguistic communication in humans could have evolved from the continuously moving tongue and arms. More generally, movement control systems, in common with the human linguistic system, have a generative capacity to create new functional complex movements (or sentences) from smaller subparts. Does muscular control underlie this generative capacity in any way? <br/><br/>The inclusion of both dysarthric speech movements and signed speech enhances the broader impacts of these experiments. In addition, the worm C. elegans shows promise as a model for the dysarthria that occurs in Parkinson's disease; dopamine synthesis and uptake in C. elegans will be manipulated so as to model the abnormal dopamine signaling in people with Parkinson's disease. The investigators have also partnered with the USC Neighborhood Academic Initiative to bring the research to the attention of students in local high schools, especially those serving minority students under-represented in science.

Award Number: 1257112
Title: Collaborative Research: Multifrequency Coordination in Dyads and Teams
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: March 22, 2013
Latest Amendment Date: March 22, 2013
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: April 01, 2013
Expires: March 31, 2016
Awarded Amount to Date: $144,001
ARRA Amount: $
Investigator(s): Jamie Gorman jamie.gorman@ttu.edu (Principal Investigator) 
Organization: Texas Tech University
349 Administration Bldg, Lubbock, TX 79409-1035, (806)742-3884
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7956 7252
Program Element Code(s): 7252
Abstract: People coordinate their actions with one another in many different contexts and in many different ways: Drivers in traffic manage to merge without accident; people walking on crowded streets usually avoid mishap; musicians coordinate a variety of nested rhythms produced by band members. These examples highlight the human capacity to coordinate behavior and exchange information at different rates to accomplish an overall goal, a phenomenon called interperson, multifrequency coordination. The researchers will study interperson, multifrequency coordination by applying the physics of coupled, oscillator systems to the perceptual, cognitive, and social information inherent in this type of coordination. Pairs of participants (dyads) will coordinate their movements while listening to different metronome frequencies through headphones. Both performance pattern and pattern stability will be measured under different experimental conditions in order to test fundamental predictions of the mathematical model. Experimental manipulations specifically designed to increase or decrease coupling between dyads' movements will be tested in the context of perceptual (e.g., visual), cognitive (e.g., counting), and social (e.g., partner familiarity) coupling mechanisms. Field studies are also planned to discover how people coordinate in naturalistic situations. This sequence of studies is designed to advance basic research on multifrequency coordination in dyads and to extend that research to spontaneous coordination in larger teams.<br/><br/>One important aspect of this project is the extension of experimentally induced multifrequency coordination in the laboratory to real-world settings in which spontaneous multifrequency coordination occurs across teams of individuals. The project also has a significant outreach component, including a visit to a nationally organized competitive youth summer camp, where investigators will collect data on spontaneous interperson, multifreqency coordination and teach campers about coordination science and STEM fields.

Award Number: 1257029
Title: Perspective-Taking in Conversation
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: March 26, 2013
Latest Amendment Date: March 26, 2013
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: August 01, 2013
Expires: July 31, 2016
Awarded Amount to Date: $301,388
ARRA Amount: $
Investigator(s): Sarah Brown-Schmidt brownsch@illinois.edu (Principal Investigator) 
Organization: University of Illinois at Urbana-Champaign
SUITE A, CHAMPAIGN, IL 61820-7473, (217)333-2187
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION LINGUISTICS 
Program Reference Code(s): 9251 7252
Program Element Code(s): 7252, 1311
Abstract: Face-to-face conversation plays a key role in children's language acquisition and adult communication, and in the evolution of languages over time. However, surprisingly little is known about the cognitive mechanisms that affect language use in conversation. One feature of conversation is that what we say and how we say it depends on our insights into the knowledge and beliefs of the people we talk to. Sometimes these insights are correct, sometimes they fail dramatically, and other times the insights are still unformed. This research project will explore how insights into others are formed, how these insights are used, and how they may fail. A series of experiments will use eye-tracking technology to study these processes on a moment-by-moment basis as they occur in natural conversation. The work draws from recent advances in psycholinguistics, developmental psychology, and social psychology.<br/><br/>This project will provide insights into a poorly understood but core component of language, that is, appreciating the perspectives of others. It has potentially diverse implications, including improving education through perspective-taking in teacher-student dyads and in math education through the use of perspective-taking in reasoning about the geometry of multi-dimensional figures.

Award Number: 0847325
Title: CAREER: Dynamic Control of Immediate Locomotor Compensations in the Leg
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: July 19, 2009
Latest Amendment Date: July 19, 2009
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: July 15, 2009
Expires: June 30, 2014
Awarded Amount to Date: $530,000
ARRA Amount: $530,000
Investigator(s): Young-Hui Chang yh.chang@ap.gatech.edu (Principal Investigator) 
Organization: Georgia Tech Research Corporation
Office of Sponsored Programs, Atlanta, GA 30332-0420, (404)894-4819
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 1187 1045 0000 6890 OTHR
Program Element Code(s): 7252
Abstract: "This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5)."<br/><br/>The major objective of this 5-year CAREER Development project is to discover unifying principles that guide locomotor compensation and integrate these scientific principles into prosthetics and orthotics education. The research goal is anchored by a central hypothesis that a common set of joint compensation principles underlie whole leg function during locomotion even when faced with different mechanical constraints. This project will be performed in the Comparative Neuromechanics Laboratory at the Georgia Institute of Technology. Using a well-controlled experimental model of human locomotion the investigators will place mechanical constraints on the locomotor task (e.g., limb movement frequency, amplitude, foot placement precision) and on individual joints during the task (e.g., torque loading, range of motion limitation, mechanical coupling between joints). They will also test whether there is a hierarchical organization to the control parameters of the leg during locomotion. This research will be integrated into prosthetics and orthotics education in three tiers: (1) development of a web-based teaching module, (2) curriculum development in a unique entry-level Master of Science in prosthetics and orthotics, and (3) a website to promote integration of basic research into related programs.<br/><br/>Achieving these project goals will deliver broad impacts to science and science education through better understanding of how nature exploits redundancy in complex systems. The intellectual merit of this work will be to address basic questions about the control and compensatory strategies of legged locomotion that apply across constraint types and across organizational levels. The compensation principles provide a theoretical framework for understanding how normal, healthy human locomotion adapts to different terrains (e.g., asphalt, grass), minor injuries (e.g., ankle sprain) and chronic pathologies (e.g., leg amputation, stroke). Areas of science and engineering can then employ these compensation principles to improve prosthetic and orthotic design, control of biomimetic robots and gait rehabilitation methods. The broader impacts of acting locally through a structured program of outreach and education development are that it will effectively build a bridge for integrating basic science into the first graduate program in prosthetics and orthotics, a historically applied and clinically oriented field.

Award Number: 1123788
Title: Language Production-Comprehension Linkage
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: August 30, 2011
Latest Amendment Date: August 30, 2011
Award Instrument: Standard Grant
Program Manager: William J. Badecker
Start Date: September 01, 2011
Expires: February 28, 2015
Awarded Amount to Date: $394,301
ARRA Amount: $
Investigator(s): Maryellen MacDonald mcmacdonald@wisc.edu (Principal Investigator) 
Organization: University of Wisconsin-Madison
21 North Park Street, MADISON, WI 53715-1218, (608)262-3822
NSF Directorate: SBE
Program(s): LINGUISTICS PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7298 1311
Program Element Code(s): 1311, 7252
Abstract: This project tests specific hypotheses about the dependencies between language comprehension, acquisition, and production, particularly the role of experience in developing language skills. Whereas the role of experience is well known in vocabulary development, there is very little research on how experience affects sentence comprehension. This gap is very unfortunate because comprehension of complex sentences is critical for success in school and reading, and this domain is particularly affected by language delays or impairments. <br/><br/> The Principal Investigator, Maryellen MacDonald, will use her Production-Distribution-Comprehension (PDC) Framework to guide research on the role of experience in language comprehension. This includes investigations of where experience comes from in the form of documenting speakers' production choices in various situations; why speakers make those utterance choices and not others; what comprehenders can learn from the patterns of utterances that they experience in their language environment; and how knowledge of these patterns shapes subsequent comprehension. <br/><br/> To better understand how language production processes create linguistic patterns in the environment, language production is studied in four distinct language environments: English, Japanese, Korean, and Mandarin Chinese. Comprehension processes are studied in these same four languages in a coordinated effort that allows the researchers to relate production choices in each language to comprehension patterns in that language, yielding a much stronger test of claims than with a single language alone. Additional studies investigate how learning from experience shapes language comprehension skill. The research plan will further a number of international collaborations.<br/><br/> This research integrates the study of three typically-distinct fields in language processing (comprehension, production, acquisition), revealing their mutual dependence and how experience shapes processing skill. As a result, this work should provide a foundation for understanding how targeted therapeutic linguistic experiences can best be developed for those with language or reading delays or impairments.

Award Number: 1314162
Title: Action's Effect on Perception
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: January 18, 2013
Latest Amendment Date: January 18, 2013
Award Instrument: Continuing grant
Program Manager: Betty H. Tuller
Start Date: August 11, 2012
Expires: May 31, 2014
Awarded Amount to Date: $143,568
ARRA Amount: $
Investigator(s): Jessica Witt Jessica.Witt@colostate.edu (Principal Investigator) 
Organization: Colorado State University
601 S Howes St, Fort Collins, CO 80523-2002, (970)491-6355
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 0000 OTHR
Program Element Code(s): 7252
Abstract: Athletes playing well describe distortions in spatial perception: basketball hoops appear as large as hula-hoops, golf holes as big as manholes, and baseballs as big as grapefruits. These subjective, anecdotal effects have been confirmed in experiments as psychological reality. However, they are not accounted for by current theories of perception, which consider optical cues and limited cognitive knowledge as the information for perception. In those theories, performance is thought to affect a post-perceptual process that generates the response rather than affecting perception itself. The aim of this proposal is to determine whether the experience of a larger target when playing well reflects a perceptual or a post-perceptual effect and to characterize the conditions under which action is most or least likely to influence perception. <br/><br/>Action-related factors, such as performance, influencing perception has important implications for our understanding of the perceptual process. In particular, it requires a functional approach in which perception provides information that is beneficial for planning actions. Strategies could eventually be developed to exploit or compensate for differences in how individuals experience their environments. Such strategies could elevate skilled performance, enhance rehabilitative strategies, and encourage more active behaviors in sedentary individuals. For example, if a goal is experienced as closer, an elderly or obese adult might be more willing to walk, thereby promoting a healthier lifestyle.

Award Number: 1026023
Title: The role of immediate prior experience in auditory stream segregation
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: August 31, 2010
Latest Amendment Date: August 31, 2010
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: September 01, 2010
Expires: August 31, 2014
Awarded Amount to Date: $201,290
ARRA Amount: $
Investigator(s): Joel Snyder joel.snyder@unlv.edu (Principal Investigator) 
Organization: University of Nevada Las Vegas
4505 MARYLAND PARKWAY, LAS VEGAS, NV 89154-1055, (702)895-1357
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION EXP PROG TO STIM COMP RES 
Program Reference Code(s): 9150
Program Element Code(s): 7252, 9150
Abstract: Human and non-human animals alike rely on a set of abilities that allow them to segregate sounds of interest from noisy background sounds, for example, when we listen to someone talking on a crowded, noisy bus. These abilities, collectively called 'auditory scene analysis,' have been the focus of several decades of laboratory research. However, recent research has pointed out the need for more empirical and theoretical work to explain the diversity of phenomena that occurs during sound segregation. Joel Snyder from the University of Nevada, Las Vegas, focuses on context effects because they show promise for significantly advancing our theoretical understanding of the mechanisms and levels of processing necessary to explain auditory scene analysis. Key issues to be addressed include 1) What features of sound patterns influence context effects? 2) Are sensory or decision levels of processing best suited to explain context effects? 3) Do attention or awareness influence context effects?<br/><br/>The findings from this project may have technical and health applications such as prosthetic design of hearing aids, speech and music recognition devices, and amelioration of auditory impairments that occur in normal hearing, hearing impairments, developmental disorders, and schizophrenia.

Award Number: 1058119
Title: Recreating the world in our minds: Relationships between language, imagery and perception
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: April 06, 2011
Latest Amendment Date: July 30, 2012
Award Instrument: Continuing grant
Program Manager: Betty H. Tuller
Start Date: June 01, 2011
Expires: May 31, 2014
Awarded Amount to Date: $239,432
ARRA Amount: $
Investigator(s): Lera Boroditsky lera@psych.stanford.edu (Principal Investigator) 
Organization: Stanford University
3160 Porter Drive, Palo Alto, CA 94304-1212, (650)723-2300
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7252
Program Element Code(s): 7252
Abstract: One emerging view in cognitive science is that language understanding is fundamentally grounded in perception. In this view, mental simulation or mental imagery serves as the basic substrate through which people represent and process linguistic meaning. However, such theories can only have explanatory power if there is a detailed understanding of the mechanisms underlying mental simulation and mental imagery. If mental imagery proves to be just as mysterious as language understanding, then there is little use in trying to explain one in terms of the other. In the present work, Dr. Lera Boroditsky of Stanford University explores the nature of mental imagery and the relationships between language understanding, mental imagery and perception. The studies examine whether imagined, inferred and real motion show the same motion aftereffect properties in response to changes in perceptual contrast, motion speed, motion type, and frame of reference. Dr. Boroditsky will also explore whether motion imagery is affected by cultural differences in how specific languages represent space. For example, residents of Pormpuraaw, a remote aboriginal community on the west coast of Cape York in Australia, organize space according to cardinal directions (as in "there is an ant on your North-West leg"). This raises the possibility that people who have learned different default ways of organizing space in their language or culture may be imagining motion in different sets of spatial coordinates.<br/><br/><br/>This research has the potential to draw important connections between cognition and perception, two areas that are often viewed as separate. If the characteristics of one's language fundamentally affect perception, this has broad implications for understanding mathematics, which is known to be strongly related to spatial cognition. Effective teaching methods for learners in different cultures should be sensitive to, or exploit, how language structures perception.

Award Number: 1057877
Title: Collaborative Research: Cross-Language Lexical Interaction
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: April 07, 2011
Latest Amendment Date: April 07, 2011
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: July 01, 2011
Expires: June 30, 2014
Awarded Amount to Date: $270,734
ARRA Amount: $
Investigator(s): Ping Li pul8@psu.edu (Principal Investigator) 
Organization: Pennsylvania State Univ University Park
110 Technology Center Building, UNIVERSITY PARK, PA 16802-7000, (814)865-1372
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION LINGUISTICS 
Program Reference Code(s): 7298 1311
Program Element Code(s): 7252, 1311
Abstract: Languages differ in how they carve up the world by their labeling of objects and events. For example, the Chinese word closest to the English word "sofa" includes padded, upholstered seats for one person, while the Chinese word closest to the English word "chair" is limited to unpadded seating made of hard materials, such as wood. How do people learning two languages handle such differences? Do they develop separate ways of connecting words to the world in each language or do they learn a single and unique way that does not fully match monolinguals in either language? In this project, the investigators attempt to understand, within the broader context of language interaction in the bilingual mind, how the pattern of word use in one's first language (L1) can influence that in the second language (L2), how L1 knowledge itself can change as L2 knowledge increases, and how the fluctuating experience and knowledge of one language can create the conditions for language interactions to occur. The project will use both behavioral studies and computational modeling to explore the unique and joint contributions of a set of cognitive variables (age of exposure to each language, proficiency in each, and the type of exposure to each) to bilingual lexical knowledge.<br/><br/>As globalization advances, more peopleare becoming bilingual or multilingual. The study of language interaction in individuals has implications for understanding the bilingual person's verbal communication, social integration, and consequent career opportunities and may yield information useful for designing learning interventions to improve language proficiency. The proposed work will integrate research and education across the two collaborative sites (Pennsylvania State University and Lehigh University). The research also involves international collaborations between scientists in the US, Europe, and China. The cross-disciplinary nature of the project should attract students from psychology, linguistics, and cognitive and computational sciences, providing opportunities particularly to students from bilingual and bi-cultural backgrounds.

Award Number: 0962119
Title: The Perceptual Identification and Representation of Image Contours
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: August 24, 2010
Latest Amendment Date: August 24, 2010
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: September 01, 2010
Expires: August 31, 2014
Awarded Amount to Date: $436,101
ARRA Amount: $
Investigator(s): James Todd todd.44@osu.edu (Principal Investigator) 
Organization: Ohio State University
Office of Sponsored Programs, Columbus, OH 43210-1016, (614)292-3805
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 
Program Element Code(s): 7252
Abstract: It has long been recognized that a convincing pictorial representation of an object can sometimes be achieved by drawing just a few salient contours in an image. This phenomenon is really quite remarkable, given that a line drawing effectively strips away almost all of the variations in color and shading that are ordinarily available in natural scenes. Somehow the artists who create such drawings are able to capture the essential information for perceptual recognition with just a few simple strokes. Although a well structured line drawing is easily interpreted by human observers, the ability to create these drawings can require considerable artistic skill. Indeed, despite almost a half century of research in the field of computer vision, there are no existing algorithms that can duplicate the performance of a competent human artist. In this project, Dr. James Todd and his students at the Ohio State University will investigate how human observers perceptually identify different types of image contours, such as shadows, corners or occlusion. The group will also examine which contours in an image are perceptually most important for creating pictorial representations of objects. The stimuli in these studies will include drawings by artists with varying amounts of training, who will be asked to produce line drawings of objects with known 3D structures. The drawings will be ranked by human observers to assess their relative perceptual effectiveness. The contours in the drawings will also be compared with different aspects of the depicted surface geometry in order to determine which specific aspects of a surface are most important for its pictorial depiction. <br/><br/>A better understanding of how human observers perceptually determine the 3D shapes of surfaces from 2D image data has many possible applications, including the design of more robust and effective algorithms in machine vision, improved techniques for 3D visualization in computer graphics and design, and the potential development of more functional prosthetic devices for the blind. This work may also have a significant impact on how students are taught to draw in art or design courses.

Award Number: 0955090
Title: Language Processing in Bilinguals
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: May 26, 2010
Latest Amendment Date: May 26, 2010
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: June 01, 2010
Expires: May 31, 2014
Awarded Amount to Date: $249,694
ARRA Amount: $
Investigator(s): Judith Kroll jfk7@psu.edu (Principal Investigator) Paola Dussias (Co-Principal Investigator) Janet van Hell (Co-Principal Investigator) 
Organization: Pennsylvania State Univ University Park
110 Technology Center Building, UNIVERSITY PARK, PA 16802-7000, (814)865-1372
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION LINGUISTICS 
Program Reference Code(s): 0000 1311 OTHR
Program Element Code(s): 7252, 1311
Abstract: Language scientists have discovered that bilinguals cannot easily "switch off" one of their languages. If bilinguals cannot easily function as monolinguals, then how do they control the use of the intended language? This research program investigates the conditions that enable bilinguals to select the intended language when words and grammatical structures in both languages are available. The work will explore how some sentence contexts restrict language processing to one language alone but others encourage code switching between the bilingual's two languages. The work uses the experience of bilinguals as a window into the nature of the interactions that characterize language processing and their consequences for cognitive control. <br/><br/>This project has a number of broader implications. It seeks foundational knowledge about multilingualism that can inform educational issues in a society in which many learners are faced with the task of acquiring a second language after the earliest stages of childhood. The research will also contribute to the training of a diverse group of cognitive scientists by including bilingual undergraduate and graduate students and by fostering an international scientific collaboration with scientists in Spain.

Award Number: 1058886
Title: Retrieved Context Models of Episodic Memory
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 26, 2011
Latest Amendment Date: September 26, 2011
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: October 01, 2011
Expires: September 30, 2014
Awarded Amount to Date: $449,283
ARRA Amount: $
Investigator(s): Michael Kahana kahana@psych.upenn.edu (Principal Investigator) 
Organization: University of Pennsylvania
Research Services, Philadelphia, PA 19104-6205, (215)898-7293
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7752 
Program Element Code(s): 7252
Abstract: This research seeks to illuminate the mechanisms underlying human episodic memory through both computational modeling and experimental studies. Episodic memory is the ability to link the information that we experience with its temporal and situational context. The ability to do so places us within our memories, making them autobiographical. Failures of episodic memory are a hallmark of normal aging and neurodegenerative disease. The first aim of the empirical studies is to assess the influence of prior knowledge and memories of past events on people's ability to encode and retrieve newly learned information. A second aim is to examine how repetition influences memory at a mechanistic level and to explain why repetitions are most beneficial for memories that are widely distributed in time. In addressing both aims, the investigators will use and assess the context maintenance and retrieval model, using neural network models of how temporal context is represented in memory, how it evolves through experience, and how it interacts with semantic context and source context in the formation and retrieval of associative information.<br/><br/>Advancing the understanding of human learning and memory has implications for the diagnosis and eventual treatment of disease-related memory impairments such as Alzheimer's disease and other dementias. The work may also impact instructional technology and educational theory and practice.

Award Number: 1023853
Title: Collaborative Research: Integrating Shape, Scaling, and Alignment in a Global Approach to F0 Events in Intonation Systems
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: September 21, 2010
Latest Amendment Date: September 21, 2010
Award Instrument: Standard Grant
Program Manager: William J. Badecker
Start Date: October 01, 2010
Expires: September 30, 2014
Awarded Amount to Date: $252,555
ARRA Amount: $
Investigator(s): Jonathan Barnes jabarnes@bu.edu (Principal Investigator) 
Organization: Trustees of Boston University
881 COMMONWEALTH AVE, BOSTON, MA 02215-1300, (617)353-4365
NSF Directorate: SBE
Program(s): LINGUISTICS PERCEPTION, ACTION & COGNITION ROBUST INTELLIGENCE 
Program Reference Code(s): 0000 1311 6867 7969 OTHR 7495
Program Element Code(s): 1311, 7252, 7495
Abstract: Human languages use pitch to convey meaning in a bewildering variety of ways. In all languages, pitch (as one aspect of speech prosody) can express attitude or emotion. In some languages, like English, pitch patterns, usually called intonation contours, also express distinctions such as that between a question and a statement. In languages like Mandarin Chinese, pitch patterns usually called tones go still further to signal differences between words that are otherwise identical. Despite significant advances in recent decades, a unified theoretical account of such linguistic phenomena remains elusive. What is missing is a common acoustic or articulatory vocabulary for expressing the relevant distinctions---a single measurable dimension within which spoken pitch contours (rises and falls) can be reliably distinguished regardless of the language under investigation. In recent work, the research team developed a new mathematical approach to tone and intonation, based on the notion of Tonal Center of Gravity. TCoG is a gestalt or global approach to tone perception and production that reconciles seemingly contradictory results from different strands of the experimental literature, moving toward a model that incorporates the best aspects of past theories, while avoiding their characteristic weaknesses. That earlier work has established that the TCoG approach accounts well for production and perception data involving two contrasting English intonation contours. This project aims to expand the empirical range of the approach in three crucial ways: First it extends the model to additional English intonation patterns. Second, it moves beyond English to look at other intonation languages (e.g., German), as well as so-called "tone languages" (e.g., Serbian). Lastly, whereas the previous work concentrated primarily on the timing of tonal events in speech, this project goes further, to investigate the interaction of tonal timing patterns with the scaling of tonal events in the pitch domain. The experimental work will be of two primary kinds: automatic classification of pitch contours recorded from native speakers in an experimental setting, and direct manipulation (through speech synthesis) of pitch contours in perception studies designed to determine which aspects of the acoustic signal have the greatest effect on listeners' judgments of utterance meaning.<br/><br/>Given the central role of intonation patterns in speech communication, one major contribution of the Tonal Center of Gravity approach is its potential to transform methods for speech synthesis and speech understanding. Synthetic speech is typically described as repetitive, detached, and often unhelpfully neutral; listeners recognize that they are talking with a machine that 'doesn't get it'. By providing a more detailed understanding of how intonational patterns help to convey a message, TCoG could be used to devise algorithms for the synthesis of more natural and appropriate-sounding speech. Likewise, for automatic understanding of the aspects of meaning that depend on intonational patterns, TCoG could allow an automatic system to detect levels of nuance beyond simply whether a word is emphasized or not, or whether an utterance is a statement or a question. A final application of this work could be in the development of software tools for second language learning, in which automated instruction and feedback on the subtleties of second language intonation patterns could help learners master important aspects of communication that are typically ignored in current approaches to language pedagogy.

Award Number: 1057855
Title: Collaborative Research: Cross-Language Lexical Interaction
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: April 07, 2011
Latest Amendment Date: April 07, 2011
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: July 01, 2011
Expires: June 30, 2014
Awarded Amount to Date: $212,253
ARRA Amount: $
Investigator(s): Barbara Malt bcm0@lehigh.edu (Principal Investigator) 
Organization: Lehigh University
Alumni Building 27, Bethlehem, PA 18015-3005, (610)758-3021
NSF Directorate: SBE
Program(s): LINGUISTICS PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 7298 1311
Program Element Code(s): 1311, 7252
Abstract: Languages differ in how they carve up the world by their labeling of objects and events. For example, the Chinese word closest to the English word "sofa" includes padded, upholstered seats for one person, while the Chinese word closest to the English word "chair" is limited to unpadded seating made of hard materials, such as wood. How do people learning two languages handle such differences? Do they develop separate ways of connecting words to the world in each language or do they learn a single and unique way that does not fully match monolinguals in either language? In this project, the investigators attempt to understand, within the broader context of language interaction in the bilingual mind, how the pattern of word use in one's first language (L1) can influence that in the second language (L2), how L1 knowledge itself can change as L2 knowledge increases, and how the fluctuating experience and knowledge of one language can create the conditions for language interactions to occur. The project will use both behavioral studies and computational modeling to explore the unique and joint contributions of a set of cognitive variables (age of exposure to each language, proficiency in each, and the type of exposure to each) to bilingual lexical knowledge.<br/><br/>As globalization advances, more people are becoming bilingual or multilingual. The study of language interaction in individuals has implications for understanding the bilingual person's verbal communication, social integration, and consequent career opportunities and may yield information useful for designing learning interventions to improve language proficiency. The proposed work will integrate research and education across the two collaborative sites (Pennsylvania State University and Lehigh University). The research also involves international collaborations between scientists in the US, Europe, and China. The cross-disciplinary nature of the project should attract students from psychology, linguistics, and cognitive and computational sciences, providing opportunities particularly to students from bilingual and bicultural backgrounds.

Award Number: 1052855
Title: Collaborative Research: A Bayesian model of phonetic and phonotactic effects in cross-language speech production
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: August 15, 2011
Latest Amendment Date: August 15, 2011
Award Instrument: Standard Grant
Program Manager: William J. Badecker
Start Date: September 01, 2011
Expires: February 28, 2015
Awarded Amount to Date: $174,878
ARRA Amount: $
Investigator(s): Lisa Davidson lisa.davidson@nyu.edu (Principal Investigator) 
Organization: New York University
70 WASHINGTON SQUARE S, NEW YORK, NY 10012-1019, (212)998-2121
NSF Directorate: SBE
Program(s): LINGUISTICS PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 1311 7298
Program Element Code(s): 1311, 7252
Abstract: A fundamental aspect of human cognition is the capacity to perceive and produce language. Studies of non-native speech processing provide some of the most striking evidence bearing on this capacity: when humans attempt to perceive or produce words containing foreign sounds or sound sequences, they show systematic patterns of correct and incorrect performance. Prior research has established that different non-native structures elicit different rates and types of error; it has been hypothesized that these differences can be explained by a combination of grammatical, perceptual, and articulatory factors. The main goals of this project are to provide carefully controlled experimental evaluations of these factors, and to develop an explicit, probabilistic model of how they interact in human performance. Particular experimental issues to be investigated are: (1) what phonetic characteristics humans are most sensitive to when processing non-native sounds; (2) how the quality of the input and ambient acoustics affect non-native perception and production; and (3) whether learning word meaning can modulate sensitivity to detailed properties of non-native sounds. The computational model builds on a growing body of work suggesting that human perception and action reflect optimal Bayesian inference conditioned on prior expectations and noisy sensory measurements. The relevant prior reflects knowledge of the native language; the model predicts that non-native structures that are more similar to those in the native language should be processed with greater accuracy. The model also predicts that non-native sounds with robust perceptual properties should be processed more accurately, even if their prior probabilities are low. The development of this model, which will be made available to other researchers, will promote the role of phonology and phonetics within the broader context of cognitive science research. Because our experiments examine the impact of classroom acoustics and talker variation on non-native sound processing, this project also has ramifications for foreign language pedagogy.

Award Number: 0956993
Title: Metacognition in Comparative Perspective
NSF Org: BCS Division of Behavioral and Cognitive Sciences
Initial Amendment Date: July 10, 2010
Latest Amendment Date: July 10, 2010
Award Instrument: Standard Grant
Program Manager: Betty H. Tuller
Start Date: July 15, 2010
Expires: June 30, 2015
Awarded Amount to Date: $224,640
ARRA Amount: $
Investigator(s): John David Smith psysmith@buffalo.edu (Principal Investigator) Michael Beran (Co-Principal Investigator) 
Organization: SUNY at Buffalo
402 Crofts Hall, Buffalo, NY 14260-0000, (716)645-2634
NSF Directorate: SBE
Program(s): PERCEPTION, ACTION & COGNITION 
Program Reference Code(s): 
Program Element Code(s): 7252
Abstract: Humans feel doubt and uncertainty. We know when we don't know or don't remember. A good example of this is the feeling that someone's name is on the tip of your tongue. This sophisticated cognitive capacity to be aware of our own thinking is called metacognition. It is closely allied to our consciousness and self-awareness and represents a fundamental dimension of our mental experience and intellectual functioning. The goal of the proposed research is to discover the origins of the metacognitive capacity by tracing the evolutionary roots of metacognition, self-awareness, and consciousness in nonhuman animals. Accordingly, this project explores metacognition in humans and rhesus macaques. The research design includes tasks that assess whether humans and nonhuman primates monitor and control their thinking in similar ways. The animals are tested via computer tasks in which they respond by touching icons on their computer screen using a joystick-controlled cursor. In previous work, animals performed these tasks eagerly.<br/><br/>Metacognition is crucial to every aspect of learning and comprehension, and in every educational setting. Understanding the cognitive organization of this capacity in adult humans and tracing its evolutionary emergence will support the development of animal models for metacognition. These models will ground the study of neurological substrates and neurochemical blocks and enhancements to metacognition. The simple, nonverbal tasks developed to suit animals are also ideal for testing very young humans. These paradigms should extend the range of techniques available to child development researchers. These paradigms may also support the study of metacognition in language-delayed or autistic children and promote efforts to train metacognition in educationally challenged populations. The research may open a new window on reflective mind in animals and on human origins, perhaps explaining how or why conscious cognitive regulation came to be such a crucial aspect of human intelligence. Finally, the demonstrations of animal awareness emerging from the research will have important implications regarding respectful, compassionate husbandry in all areas of animal research.

